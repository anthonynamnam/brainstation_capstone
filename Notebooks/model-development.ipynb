{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf855c90-4d76-4b34-95a8-5939266fa0f0",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "\n",
    "<h1> Used Car Listing Price Prediction</h1>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0e54d7-2b98-43e7-8bf8-b521cdbe02f7",
   "metadata": {},
   "source": [
    "<center><img src=\"https://raw.githubusercontent.com/anthonynamnam/anthonynamnam/main/icons/image/car-banner.png\" alt=\"memes\" width=\"600\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a66a4-04c3-4292-afa6-dd4699957835",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d070dc70-c8d9-4361-b779-7312490b4898",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "\n",
    "<h2> Project Overview</h2>\n",
    "    \n",
    "Please kindly refer to the github repo of this project: <a href=\"https://github.com/anthonynamnam/brainstation_capstone#project-overview\">Link</a>\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79217f3-ccad-4024-9740-8d01803abf06",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddfcfdc-5aa3-42f1-a2d6-1d239b0adbe7",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "\n",
    "   <h2> Notebook Overview</h2>\n",
    "    \n",
    "Now that we have a well-prepared dataset in hand, our next steps involve handling categorical variables, scaling features, addressing class imbalances, and ultimately building and evaluating predictive models. Each of these steps plays a pivotal role in the success of our machine learning and deep learning endeavors. In this notebook, we will guide you through the following steps of working with data:\n",
    "    \n",
    "<ol>\n",
    "    <font size=3><li><b>Categorical Data Encoding üé≤</b></li></font>\n",
    "    <p>Many machine learning algorithms require numerical inputs, necessitating the transformation of categorical variables into a format suitable for analysis. In this notebook, we'll explore various encoding techniques to convert categorical data into a numerical representation that our models can comprehend</p>\n",
    "    <font size=3><li><b>Data Scaling üìê</b></li></font>\n",
    "    <p>Ensuring that all features are on a consistent scale is crucial for the performance of many machine learning algorithms. We'll delve into the importance of data scaling and demonstrate methods to standardize or normalize our features.</p>\n",
    "    <font size=3><li><b>Class Imbalance ‚öñÔ∏è</b></li></font>\n",
    "    <p>Real-world datasets often exhibit imbalances in class distribution, where certain outcomes are more prevalent than others. We'll explore techniques to address class imbalances, ensuring that our models are trained to recognize patterns effectively.</p>\n",
    "    <font size=3><li><b>ML/DL Modeling üß†</b></li></font>\n",
    "    <p>The heart of our predictive analytics journey lies in building machine learning and deep learning models. We'll guide you through the process of selecting, training, and fine-tuning models that best suit the nature of our data and the goals of our project.</p>\n",
    "    <font size=3><li><b>Model Evaluation üßÆ</b></li></font>\n",
    "    <p>As we generate predictions, it becomes imperative to assess the performance of our models. We'll introduce metrics and techniques for evaluating model accuracy, precision, recall, and other key indicators to ensure that our models meet the desired standards.</p>\n",
    "\n",
    "</ol>\n",
    "Through this notebook, we aim to equip you with the knowledge and tools needed to navigate the intricacies of turning prepared data into actionable insights. Let's harness the power of machine learning and deep learning to uncover patterns, make predictions, and elevate the impact of our project.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba66112-916f-4c30-bc40-621bf91a6df4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef79b76-49ae-46c2-ad29-c8399568f360",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "\n",
    "<a class=\"anchor\" id=\"4-toc\"> \n",
    "    <h2> Table of Contents </h2>\n",
    "</a>\n",
    "    \n",
    "<ul>    \n",
    "    <li> <a href=\"#4-setup\">Notebook Set Up</a></li>\n",
    "    <li> <a href=\"#4-func\">Functions</a></li>\n",
    "    <li> <a href=\"#4-load\">Data Loading</a></li>\n",
    "    <li> <a href=\"#4-cat-encode\">Categorical Enconding</a></li>\n",
    "    <li> <a href=\"#4-scale\">Data Scaling</a></li>\n",
    "    <li> <a href=\"#4-imbalance\">Class Imbalance</a></li>\n",
    "    <li> <a href=\"#4-models\">Proposed Models</a></li>\n",
    "    <li> <a href=\"#4-pipelines\">Model Pipelines</a></li>\n",
    "    <li> <a href=\"#4-evaluate\">Model Evaluation</a></li>\n",
    "    <li> <a href=\"#4-learn\">Learning/Takeaway</a></li>\n",
    "</ul>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d2e083-9878-41a2-8ff7-0982a4fb4474",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c266769e-8d30-469b-bfc3-c4370861623f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-setup\">\n",
    "    <h2> Set Up </h2>\n",
    "</a>\n",
    "<b>Table of Content:</b>\n",
    "<ul>    \n",
    "    <li> <a href=\"#4-import\">Import Library</a></li>\n",
    "    <li> <a href=\"#4-const\">Global Const</a></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22209042-1912-4268-8023-cde5532d725a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-import\">\n",
    "<h3> Import Library </h3>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a6f9d6e7-093b-45b5-a559-9322d5662f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "# Data Science Package\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce72bea-978e-4b2a-87e8-25a7a4021069",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-const\">\n",
    "<h3> Global Constant </h3>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6b383772-a04f-4b08-b558-2c4a89834c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "ran = random.Random()\n",
    "ran.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "258c92ec-efdb-419d-b445-00bd6c312d5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s >>> %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    handlers=[\n",
    "                        logging.FileHandler(filename='log/modelling.log'),\n",
    "                        logging.StreamHandler(sys.stdout)\n",
    "                    ])\n",
    "logger = logging.getLogger('LOGGER_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469bf518-ab10-45f8-93a7-6330f712e90e",
   "metadata": {},
   "source": [
    "[Back-to-top](#4-toc)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72c40b8-a43c-440d-8806-db04df101abc",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-func\">\n",
    "    <h2> Functions </h2>\n",
    "</a>\n",
    "<b>Table of Content:</b>\n",
    "<ul>    \n",
    "    <li> <a href=\"#4-func-print\">Helper Funcntions (Print Info)</a></li>\n",
    "    <li> <a href=\"#4-func-edit\">Helper Funcntions (Edit Dataframe)</a></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cef8a9-c5c5-497a-b0c3-679ca8e3d3a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-func-print\">\n",
    "<h3> Helper Funcntions (Print Info) </h3>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "73c0e3e2-615b-465a-bf2a-45fb74ba7efb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper Functions to print df info and statement\n",
    "import pandas as pd\n",
    "\n",
    "def print_num_row(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----\n",
    "    Retrieve the number of rows of dataframe and print it as a statement.\n",
    "    \n",
    "    Args\n",
    "    -----\n",
    "    df (pd.DataFrame): the target dataframe\n",
    "    \n",
    "    Returns\n",
    "    -----\n",
    "    None\n",
    "    \n",
    "    Example\n",
    "    -----\n",
    "    df = pd.DataFrame(data = {\"height\":[147,190],\"weight\":[47,72],\"age\":[12,28]},index = [0,1])\n",
    "    print_num_row(df)  =>\n",
    "        |\n",
    "        | \"The dataframe has 2 rows of record now.\"\n",
    "        |\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"The dataframe has {df.shape[0]} rows of record now.\")\n",
    "    return\n",
    "    \n",
    "\n",
    "def print_num_col(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----\n",
    "    Retrieve the number of columns of dataframe and print it as a statement.\n",
    "    \n",
    "    Args\n",
    "    -----\n",
    "    df (pd.DataFrame): the target dataframe\n",
    "    \n",
    "    Returns\n",
    "    -----\n",
    "    None\n",
    "    \n",
    "    Example\n",
    "    -----\n",
    "    df = pd.DataFrame(data = {\"height\":[147,190],\"weight\":[47,72],\"age\":[12,28]},index = [0,1])\n",
    "    print_num_col(df) => \n",
    "        |\n",
    "        | \"The dataframe has 3 columns now.\"\n",
    "        |\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"The dataframe has {df.shape[1]} columns now.\")\n",
    "    return\n",
    "        \n",
    "def print_dim(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----\n",
    "    Retrieve the shape of dataframe and print it as a statement.\n",
    "    \n",
    "    Args\n",
    "    -----\n",
    "    df (pd.DataFrame): the target dataframe\n",
    "    \n",
    "    Returns\n",
    "    -----\n",
    "    None\n",
    "    \n",
    "    Example\n",
    "    -----\n",
    "    abc_df = pd.DataFrame(data = {\"height\":[147,190],\"weight\":[47,72],\"age\":[12,28]},index = [0,1])\n",
    "    print_dim(abc_df) =>\n",
    "        |\n",
    "        | \"There are 2 rows and 3 columns in this dataframe now.\"\n",
    "        |\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"There are {df.shape[0]} rows and {df.shape[1]} columns in this dataframe now.\")\n",
    "    return\n",
    "\n",
    "\n",
    "def print_null_count(df: pd.DataFrame,cols:list = []) -> None:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----\n",
    "    Count the null value in each columns.\n",
    "    If `cols` is provided, only show the null value count for the columns in `cols`. \n",
    "    Otherwise, show null value count for all columns.\n",
    "    \n",
    "    Args\n",
    "    -----\n",
    "    df (pd.DataFrame): target dataframe\n",
    "    cols (list): the column names to show the null value count. Default: []\n",
    "    \n",
    "    Returns\n",
    "    -----\n",
    "    None\n",
    "    \n",
    "    Example\n",
    "    -----\n",
    "    abc_df = pd.DataFrame(data = {\"height\":[147,190],\"weight\":[47,np.nan],\"age\":[np.nan,28]},index = [0,1])\n",
    "    print_null_count(abc_df) => \n",
    "        |\n",
    "        | === Null Count ===\n",
    "        | height    0\n",
    "        | weight    1\n",
    "        | age       2\n",
    "        | dtype: int64\n",
    "        |\n",
    "        \n",
    "    print_null_count(abc_df,cols=[\"age\"]) => \n",
    "        |\n",
    "        | === Null Count ===\n",
    "        | Column `age`: 2\n",
    "        |\n",
    "        \n",
    "    print_null_count(abc_df,cols=[\"age\",\"weight\"]) => \n",
    "        |\n",
    "        | === Null Count ===\n",
    "        | Column `age`: 2\n",
    "        | Column `weight`: 1\n",
    "        |\n",
    "    \n",
    "    \"\"\"\n",
    "    if len(cols) == 0:\n",
    "        null_count = df.isnull().sum()\n",
    "        \n",
    "        print(\"=== Null Count ===\")\n",
    "        print(null_count)\n",
    "    else:\n",
    "        assert set(cols).issubset(df.columns)\n",
    "        null_count = df[cols].isnull().sum()\n",
    "        \n",
    "        print(\"=== Null Count ===\")\n",
    "        for col in cols:\n",
    "            print(f\"Column `{col}`: {null_count[col]}\")\n",
    "    return \n",
    "\n",
    "\n",
    "def print_null_pct(df: pd.DataFrame,cols:list = []) -> None:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----\n",
    "    Count the null percentage in each columns.\n",
    "    If `cols` is provided, only show the null percentage for the columns in `cols`. \n",
    "    Otherwise, show null percentage for all columns.\n",
    "    \n",
    "    Args\n",
    "    -----\n",
    "    df (pd.DataFrame): target dataframe\n",
    "    cols (list): the column names to show the null percentage. Default: []\n",
    "    \n",
    "    Returns\n",
    "    -----\n",
    "    None\n",
    "    \n",
    "    Example\n",
    "    -----\n",
    "    abc_df = pd.DataFrame(data = {\"height\":[147,190],\"weight\":[47,np.nan],\"age\":[np.nan,np.nan]},index = [0,1])\n",
    "    print_null_pct(abc_df) => \n",
    "        |\n",
    "        | === Null Count Precentage ===\n",
    "        | height      0.0%\n",
    "        | weight     50.0%\n",
    "        | age       100.0%\n",
    "        | dtype: object\n",
    "        |\n",
    "        \n",
    "    print_null_pct(abc_df,cols=[\"weight\"]) => \n",
    "        |\n",
    "        | === Null Count Precentage ===\n",
    "        | Column weight: 50.0%\n",
    "        |\n",
    "    \n",
    "    \"\"\"\n",
    "    total_row = df.shape[0]\n",
    "    if len(cols) == 0:\n",
    "        null_count = df.isnull().sum()\n",
    "        null_pct = null_count / total_row * 100\n",
    "        \n",
    "        print(\"=== Null Count Precentage ===\")\n",
    "        print(null_pct.round(2).astype(str)+\"%\")\n",
    "    else:\n",
    "        assert set(cols).issubset(df.columns)\n",
    "        null_count = df[cols].isnull().sum()\n",
    "        null_pct = null_count / total_row * 100\n",
    "\n",
    "        print(\"=== Null Count Precentage ===\")\n",
    "        for col in cols:\n",
    "            print(f\"Column {col}: {round(null_pct[col],4)}%\")\n",
    "    return\n",
    "\n",
    "def print_duplicated_count(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----\n",
    "    Count the number of duplicated rows.\n",
    "    \n",
    "    Args\n",
    "    -----\n",
    "    df (pd.DataFrame): target dataframe\n",
    "    \n",
    "    Returns\n",
    "    -----\n",
    "    None\n",
    "    \n",
    "    Example\n",
    "    -----\n",
    "    abc_df = pd.DataFrame(data = {\"height\":[147,190,147],\"weight\":[47,np.nan,47],\"age\":[13,27,13]},index = [0,1,2])\n",
    "    print_duplicated_count(abc_df) => \n",
    "        |\n",
    "        | There are 1 duplicated rows\n",
    "        | \n",
    "        \n",
    "    \"\"\"\n",
    "    print(f\"There are {df.duplicated().sum()} duplicated rows\")\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6928ce65-5c71-4f9b-a3c2-d5859f35d090",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-func-edit\">\n",
    "<h3> Helper Funcntions (Edit Dataframe) </h3>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "48607615-3cc6-48b6-b7f5-aca3ba432093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_cols_if_exist(df: pd.DataFrame,cols_to_drop:list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----\n",
    "    Drop a column from a dataframe with inplace = True. Only execute the dropping if the cols exist.\n",
    "    \n",
    "    Args\n",
    "    -----\n",
    "    df (pd.DataFrame): the target dataframe\n",
    "    cols_to_drop (list): the list of column to be dropped\n",
    "    \n",
    "    Returns\n",
    "    -----\n",
    "    df (pd.DataFrame): the dataframe with columns dropped\n",
    "    \n",
    "    Example\n",
    "    -----\n",
    "    # Create a DataFrame\n",
    "    abc_df = pd.DataFrame(data = {\"height\":[147,190,147],\"weight\":[47,np.nan,47],\"age\":[13,27,13]},index = [0,1,2])\n",
    "    print(abc_df)  =>\n",
    "        |\n",
    "        |    height  weight  age\n",
    "        | 0     147    47.0   13\n",
    "        | 1     190     NaN   27\n",
    "        | 2     147    47.0   13\n",
    "        |\n",
    "        \n",
    "    # Drop columns if exist\n",
    "    dropped_abc_df = drop_cols_if_exist(abc_df,cols_to_drop=[\"weight\"])\n",
    "    print(dropped_abc_df)   =>\n",
    "        | Successfully dropped columns: {'weight'}\n",
    "        |    height  age\n",
    "        | 0     147   13\n",
    "        | 1     190   27\n",
    "        | 2     147   13\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    intersect_cols = set(cols_to_drop).intersection(df.columns)\n",
    "    df.drop(columns=intersect_cols,inplace=True)\n",
    "    print(f\"Successfully dropped columns: {intersect_cols}\")\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091c145a-b94c-498a-9417-c883aa133ea4",
   "metadata": {},
   "source": [
    "[Back-to-top](#4-toc)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb84599-d133-4e4b-ae4b-16f5fa387d21",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-load\">\n",
    "    <h2> Data Loading </h2>\n",
    "</a>\n",
    "<b>Table of Content:</b>\n",
    "<ul>    \n",
    "    <li> <a href=\"#4-load-process\">Load Processed Data</a></li>\n",
    "    <li> <a href=\"#4-san-check-train\">Sanity Check - Train Data</a></li>\n",
    "    <li> <a href=\"#4-san-check-test\">Sanity Check - Train Data</a></li>\n",
    "    <li> <a href=\"#4-split-xy\">Split Data</a></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eefa5f8-233c-4547-8ebc-e949550de8ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-load-process\">\n",
    "<h3> Load the Split data </h3>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8135ee15-1589-495c-b1d5-ae2d12fc228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dtype for split dataset\n",
    "SPLIT_COL_DTYPE = {\n",
    "    \n",
    "    \"log_miles\":float,\n",
    "    \"year\":int,\n",
    "    \"make\":str,\n",
    "    \"model\":str,\n",
    "    \"trim\":str,\n",
    "    \"body_type\":str,\n",
    "    \"vehicle_type\":str,\n",
    "    \"drivetrain\":str,\n",
    "    \"transmission\":str,\n",
    "    \"engine_size\":float,\n",
    "    \"engine_block\":str,\n",
    "    \"price_range\":int,\n",
    "    \n",
    "    # For encoded fuel type\n",
    "    \"fuel_M85\":int,\n",
    "    \"fuel_Lpg\":int,\n",
    "    \"fuel_Diesel\":int,\n",
    "    \"fuel_Unleaded\":int,\n",
    "    \"fuel_Hydrogen\":int,\n",
    "    \"fuel_PremiumUnleaded\":int,\n",
    "    \"fuel_Biodiesel\":int,\n",
    "    \"fuel_E85\":int,\n",
    "    \"fuel_Electric\":int,\n",
    "    \"fuel_CompressedNaturalGas\":int,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b39b0421-f3a1-44f9-b504-432b481fc61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we read the train dataset\n",
    "train_data = pd.read_parquet(path = \"data/train-data.parquet\",\n",
    "                     columns = SPLIT_COL_DTYPE)\n",
    "train_data.reset_index(drop = True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "be995b54-f12b-4340-b192-e2840e4497b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Then, we read the testt dataset\n",
    "test_data = pd.read_parquet(path = \"data/test-data.parquet\",\n",
    "                     columns = SPLIT_COL_DTYPE)\n",
    "test_data.reset_index(drop = True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46f146e-39d5-44f2-810a-829ac8315a27",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-san-check-train\">\n",
    "<h3> Sanity Check - Train </h3>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0b3e73a4-6cf1-43c0-bc91-a0684398925b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_miles</th>\n",
       "      <th>year</th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>trim</th>\n",
       "      <th>body_type</th>\n",
       "      <th>vehicle_type</th>\n",
       "      <th>drivetrain</th>\n",
       "      <th>transmission</th>\n",
       "      <th>engine_size</th>\n",
       "      <th>engine_block</th>\n",
       "      <th>price_range</th>\n",
       "      <th>fuel_M85</th>\n",
       "      <th>fuel_Lpg</th>\n",
       "      <th>fuel_Diesel</th>\n",
       "      <th>fuel_Unleaded</th>\n",
       "      <th>fuel_Hydrogen</th>\n",
       "      <th>fuel_PremiumUnleaded</th>\n",
       "      <th>fuel_Biodiesel</th>\n",
       "      <th>fuel_E85</th>\n",
       "      <th>fuel_Electric</th>\n",
       "      <th>fuel_CompressedNaturalGas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.353497</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>Tacoma</td>\n",
       "      <td>SR5</td>\n",
       "      <td>Pickup</td>\n",
       "      <td>Truck</td>\n",
       "      <td>4WD</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>3.5</td>\n",
       "      <td>V</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.780189</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>RAM</td>\n",
       "      <td>Ram 1500 Pickup</td>\n",
       "      <td>Big Horn</td>\n",
       "      <td>Pickup</td>\n",
       "      <td>Truck</td>\n",
       "      <td>RWD</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>3.0</td>\n",
       "      <td>V</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.922555</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Lexus</td>\n",
       "      <td>ES</td>\n",
       "      <td>350</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>Car</td>\n",
       "      <td>FWD</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>3.5</td>\n",
       "      <td>V</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.928507</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>BMW</td>\n",
       "      <td>X5</td>\n",
       "      <td>sDrive35i</td>\n",
       "      <td>SUV</td>\n",
       "      <td>Truck</td>\n",
       "      <td>RWD</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>3.0</td>\n",
       "      <td>I</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.268992</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Scion</td>\n",
       "      <td>tC</td>\n",
       "      <td>Release Series 9.0</td>\n",
       "      <td>Coupe</td>\n",
       "      <td>Car</td>\n",
       "      <td>FWD</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>2.5</td>\n",
       "      <td>I</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   log_miles    year    make            model                trim body_type  \\\n",
       "0   8.353497  2020.0  Toyota           Tacoma                 SR5    Pickup   \n",
       "1   9.780189  2018.0     RAM  Ram 1500 Pickup            Big Horn    Pickup   \n",
       "2   9.922555  2018.0   Lexus               ES                 350     Sedan   \n",
       "3  10.928507  2017.0     BMW               X5           sDrive35i       SUV   \n",
       "4  11.268992  2015.0   Scion               tC  Release Series 9.0     Coupe   \n",
       "\n",
       "  vehicle_type drivetrain transmission  engine_size engine_block  price_range  \\\n",
       "0        Truck        4WD    Automatic          3.5            V            3   \n",
       "1        Truck        RWD    Automatic          3.0            V            3   \n",
       "2          Car        FWD    Automatic          3.5            V            3   \n",
       "3        Truck        RWD    Automatic          3.0            I            3   \n",
       "4          Car        FWD    Automatic          2.5            I            1   \n",
       "\n",
       "   fuel_M85  fuel_Lpg  fuel_Diesel  fuel_Unleaded  fuel_Hydrogen  \\\n",
       "0         0         0            0              1              0   \n",
       "1         0         0            1              0              0   \n",
       "2         0         0            0              1              0   \n",
       "3         0         0            0              0              0   \n",
       "4         0         0            0              1              0   \n",
       "\n",
       "   fuel_PremiumUnleaded  fuel_Biodiesel  fuel_E85  fuel_Electric  \\\n",
       "0                     0               0         0              0   \n",
       "1                     0               0         0              0   \n",
       "2                     0               0         0              0   \n",
       "3                     1               0         0              0   \n",
       "4                     0               0         0              0   \n",
       "\n",
       "   fuel_CompressedNaturalGas  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a2471ff3-917f-48e4-b661-0241bf1c4ac0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1764450 rows and 22 columns in this dataframe now.\n"
     ]
    }
   ],
   "source": [
    "print_dim(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9ec4c275-4627-41ca-85eb-6b6161bf92c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Null Count ===\n",
      "log_miles                    0\n",
      "year                         0\n",
      "make                         0\n",
      "model                        0\n",
      "trim                         0\n",
      "body_type                    0\n",
      "vehicle_type                 0\n",
      "drivetrain                   0\n",
      "transmission                 0\n",
      "engine_size                  0\n",
      "engine_block                 0\n",
      "price_range                  0\n",
      "fuel_M85                     0\n",
      "fuel_Lpg                     0\n",
      "fuel_Diesel                  0\n",
      "fuel_Unleaded                0\n",
      "fuel_Hydrogen                0\n",
      "fuel_PremiumUnleaded         0\n",
      "fuel_Biodiesel               0\n",
      "fuel_E85                     0\n",
      "fuel_Electric                0\n",
      "fuel_CompressedNaturalGas    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print_null_count(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "127407cd-7c49-4ce8-9919-4ac2d1371a15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 23968 duplicated rows\n"
     ]
    }
   ],
   "source": [
    "print_duplicated_count(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab521c1b-3c2f-4c91-aba2-44b997446ecd",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-san-check-test\">\n",
    "<h3> Sanity Check - Test </h3>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6bf414fd-ae9c-40ab-9a14-f5b6efd11a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_miles</th>\n",
       "      <th>year</th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>trim</th>\n",
       "      <th>body_type</th>\n",
       "      <th>vehicle_type</th>\n",
       "      <th>drivetrain</th>\n",
       "      <th>transmission</th>\n",
       "      <th>engine_size</th>\n",
       "      <th>engine_block</th>\n",
       "      <th>price_range</th>\n",
       "      <th>fuel_M85</th>\n",
       "      <th>fuel_Lpg</th>\n",
       "      <th>fuel_Diesel</th>\n",
       "      <th>fuel_Unleaded</th>\n",
       "      <th>fuel_Hydrogen</th>\n",
       "      <th>fuel_PremiumUnleaded</th>\n",
       "      <th>fuel_Biodiesel</th>\n",
       "      <th>fuel_E85</th>\n",
       "      <th>fuel_Electric</th>\n",
       "      <th>fuel_CompressedNaturalGas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.532738</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>Jeep</td>\n",
       "      <td>Wrangler Unlimited</td>\n",
       "      <td>Sahara</td>\n",
       "      <td>SUV</td>\n",
       "      <td>Truck</td>\n",
       "      <td>4WD</td>\n",
       "      <td>Manual</td>\n",
       "      <td>3.6</td>\n",
       "      <td>V</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.028894</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Honda</td>\n",
       "      <td>Accord</td>\n",
       "      <td>EX-L V6</td>\n",
       "      <td>Coupe</td>\n",
       "      <td>Car</td>\n",
       "      <td>FWD</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>3.5</td>\n",
       "      <td>V</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.063947</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>Kia</td>\n",
       "      <td>FORTE</td>\n",
       "      <td>LXS</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>Car</td>\n",
       "      <td>FWD</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.526024</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>Jeep</td>\n",
       "      <td>Grand Cherokee</td>\n",
       "      <td>High Altitude</td>\n",
       "      <td>SUV</td>\n",
       "      <td>Truck</td>\n",
       "      <td>4WD</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>3.6</td>\n",
       "      <td>V</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.633834</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>Tiguan</td>\n",
       "      <td>SE</td>\n",
       "      <td>SUV</td>\n",
       "      <td>Truck</td>\n",
       "      <td>4WD</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   log_miles    year        make               model           trim body_type  \\\n",
       "0  11.532738  2014.0        Jeep  Wrangler Unlimited         Sahara       SUV   \n",
       "1  12.028894  2012.0       Honda              Accord        EX-L V6     Coupe   \n",
       "2  10.063947  2020.0         Kia               FORTE            LXS     Sedan   \n",
       "3  10.526024  2019.0        Jeep      Grand Cherokee  High Altitude       SUV   \n",
       "4  10.633834  2018.0  Volkswagen              Tiguan             SE       SUV   \n",
       "\n",
       "  vehicle_type drivetrain transmission  engine_size engine_block  price_range  \\\n",
       "0        Truck        4WD       Manual          3.6            V            2   \n",
       "1          Car        FWD    Automatic          3.5            V            1   \n",
       "2          Car        FWD    Automatic          2.0            I            2   \n",
       "3        Truck        4WD    Automatic          3.6            V            4   \n",
       "4        Truck        4WD    Automatic          2.0            I            2   \n",
       "\n",
       "   fuel_M85  fuel_Lpg  fuel_Diesel  fuel_Unleaded  fuel_Hydrogen  \\\n",
       "0         0         0            0              1              0   \n",
       "1         0         0            0              1              0   \n",
       "2         0         0            0              1              0   \n",
       "3         0         0            0              1              0   \n",
       "4         0         0            0              1              0   \n",
       "\n",
       "   fuel_PremiumUnleaded  fuel_Biodiesel  fuel_E85  fuel_Electric  \\\n",
       "0                     0               0         0              0   \n",
       "1                     0               0         0              0   \n",
       "2                     0               0         0              0   \n",
       "3                     0               0         0              0   \n",
       "4                     0               0         0              0   \n",
       "\n",
       "   fuel_CompressedNaturalGas  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "eeeec794-2922-45d3-8e5a-499b6c4b4e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 588151 rows and 22 columns in this dataframe now.\n"
     ]
    }
   ],
   "source": [
    "print_dim(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b5072efb-a970-4a78-ae3d-48a3c9b9e322",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Null Count ===\n",
      "log_miles                    0\n",
      "year                         0\n",
      "make                         0\n",
      "model                        0\n",
      "trim                         0\n",
      "body_type                    0\n",
      "vehicle_type                 0\n",
      "drivetrain                   0\n",
      "transmission                 0\n",
      "engine_size                  0\n",
      "engine_block                 0\n",
      "price_range                  0\n",
      "fuel_M85                     0\n",
      "fuel_Lpg                     0\n",
      "fuel_Diesel                  0\n",
      "fuel_Unleaded                0\n",
      "fuel_Hydrogen                0\n",
      "fuel_PremiumUnleaded         0\n",
      "fuel_Biodiesel               0\n",
      "fuel_E85                     0\n",
      "fuel_Electric                0\n",
      "fuel_CompressedNaturalGas    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print_null_count(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "546460a4-6a97-4e0f-88c9-c55498230c37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4218 duplicated rows\n"
     ]
    }
   ],
   "source": [
    "print_duplicated_count(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f89346-46f5-4f49-be02-39c706855919",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-split-xy\">\n",
    "<h3> Split Data into X and y </h3>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "97af8a7f-748f-4e96-97f7-be0758c68861",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = train_data.drop(columns=[\"price_range\"])\n",
    "y_train = train_data[\"price_range\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "58f3b9ca-ca51-46ac-99dc-7aa018c60298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test= test_data.drop(columns=[\"price_range\"])\n",
    "y_test = test_data[\"price_range\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8a6db840-9302-41c6-a229-37b6fd79010b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1764450, 21) (588151, 21) (1764450,) (588151,)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2083c0c-a555-4173-a025-a783fad1d979",
   "metadata": {},
   "source": [
    "[Back-to-top](#4-toc)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f086ec80-ffb5-4e75-81bc-2a6a58b953c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-cat-encode\">\n",
    "    <h2> Categorical Encoding </h2>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00e02cd-7506-41f8-a506-228bd6128c46",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üß† Idea:</b></font>\n",
    "<br><br>\n",
    "After some researches, we concluded that these are the available encoding options for our dataset.  \n",
    "\n",
    "- `Dummy Encoding` (keep all categories)\n",
    "- `Dummy Encoding` (with fixed number of category, i.e. top 10 most frequent categories)\n",
    "- `Dummy Encoding` (with value counts percentage threshold, i.e. only keep categories with more than X% of total records)\n",
    "- `Ordinal Encoding` (for category with ordinal meaning)\n",
    "- `Count Encoding` (Data Leakage if no split data) `->` Use `sklearn`.`Pipeline`\n",
    "- `Target Encoding`  (Data Leakage if no split data) `->` Use `sklearn`.`Pipeline`\n",
    "\n",
    "**However**, some of them are not suitable for our columns.\n",
    "\n",
    "`Count Enconding`: It is not useful as we have > 7M records and some values may have over millions count.  \n",
    "`Ordinal Encoding`: Our categorical columns generally do not consist of any order or level, so ordinal enconding may not be useful at all in this scenario.  \n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f3c0c6-9390-4d72-a569-03dfa9676e12",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment:</b></font>\n",
    "\n",
    "- From the above table, as we do not want to expand our feature spaces too much. We will apply target encoding to columns `make`, `model`, `trim` and `body_type`.\n",
    "\n",
    "- Advantages of `target encoding`:\n",
    "    1. `Target encoding` will not expand our feature spaces.\n",
    "    1. `Target encoding` is encoded by averaging target variable (`price_range`) within each feature group (`model`), which mean the encoded value is the average `price_range` of that `model`. \n",
    "    \n",
    "- Disadvantage of `target encoding`:\n",
    "    1. As the averaging process will gather the information across different rows, this may lead to **data leakage**. So, we will not perform encoding transformation here. Instead, we will embed the transformation in modelling pipeline.\n",
    "\n",
    "- For other columns, we will apply `dummy variable encoding`, which may not consist of data leakage problem.\n",
    "    \n",
    "- In order to integrate encoder with cross validation, we will embed the encoder into pipeline in modelling section.\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76413640-d7b6-46a6-8aad-0b9a5f71bf98",
   "metadata": {},
   "source": [
    "[Back-to-top](#4-toc)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120b72b3-2eda-4554-b5b3-5ff742033e47",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-scale\">\n",
    "    <h2> Data Scaling </h2>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c8e696-d707-4399-8b5c-752cdc28561c",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üß† Idea:</b></font>\n",
    "<br><br>\n",
    "There are three available encoding options for our dataset.  \n",
    "    \n",
    "---\n",
    "- `Standard Scaling`\n",
    "    - Output space: `[-inf,inf]` with `mean` of 0 and `variance` of 1\n",
    "    - Characteristics:\n",
    "        - Useful when features have different units\n",
    "        - Sensitive to outliers\n",
    "---\n",
    "- `Min-Max Scaling`\n",
    "    - Output space: `[0,1]`\n",
    "    - Characteristics:\n",
    "        - Maintains the shape of the original distribution\n",
    "        - Sensitive to outliers\n",
    "---\n",
    "- `Robust Scaling`\n",
    "    - Output space: `[-inf,inf]`\n",
    "    - Characteristics:\n",
    "        - Effective in the presence of outliers, as it uses the median and IQR.\n",
    "        - Maintains the central tendency (Data tends to stay at the center)\n",
    "    \n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4a26da-83e9-4ff9-a4b8-e6958f7f4a2d",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment:</b></font>\n",
    "\n",
    "In our dataset, there are several feature types:\n",
    "- `Categorical Features` (e.g. `make`, `drivetrain`)\n",
    "- `Numerical Features` (e.g. `year`, `log_miles`)\n",
    "\n",
    "---\n",
    "    \n",
    "- For `Categorical Features`, as we will apply `one-hot encoding` and `target encoding`. The output space of both method are in `[0,1]` too. If we apply Min-Max Scaling, it does not change anything. If we apply `standard scaling`, it would change the scale from `[0,1]` to `[-inf, inf]`. Therefore, we should not apply any scaling on it.\n",
    "    \n",
    "- For `Numerical Features`, as they have different magnitude scales, we should apply `standard scaling` on it.\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abd5dde-721f-4e48-aaca-bf36a8b48cae",
   "metadata": {},
   "source": [
    "[Back-to-top](#4-toc)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1849b3cf-fb79-483a-81cb-ff671649084b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-imbalance\">\n",
    "    <h2> Class Imbalance</h2>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdc8a50-54cc-474c-abbf-d87b95783417",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>‚ùì What is Class Imbalance?</b></font>\n",
    "<br><br>\n",
    "    <p><b>Class imbalance</b> refers to a situation in a classification problem where the distribution of the classes is not uniform, meaning that one or more classes have significantly fewer instances than the others. In other words, there is an unequal distribution of target labels in the dataset, and one or more classes are underrepresented compared to the others.</p>\n",
    "<br>\n",
    "    <p>For our classification task, which is <b>multi-class classification</b>, class imbalance can refer to <b>unequal distribution</b> across multiple classes. </p>\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e331e6fb-3dfc-4fdc-b712-4459c3e77f51",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üß† How to tackle Class Imbalance?</b></font>\n",
    "<br><br>\n",
    "\n",
    "There are several techniques for **Class Imbalance**.  \n",
    "1. Resampling\n",
    "    - **Over Sampling**\n",
    "    - **Under Sampling**\n",
    "    - **Hybrid Sampling** (Over Sampling on Minority Class + Under Sampling on Majority Class) (To be tested)\n",
    "2. Synthetic Data Generation\n",
    "    - <b>S</b>ynthetic <b>M</b>inority <b>O</b>ver sampling <b>TE</b>chnique <b>(SMOTE)</b>\n",
    "    - <b>S</b>ynthetic <b>M</b>inority <b>O</b>ver sampling <b>TE</b>chnique for <b>N</b>ominal & <b>C</b>ontinuous <b>(SMOTENC)</b>\n",
    "\n",
    "    \n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d121de-938b-4f41-bfb7-71a9fe7b0800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f02163-1123-4b3e-a488-71f9d8c9e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a81b29f-efec-42c8-8f28-873aa8bb1a77",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment:</b></font>\n",
    "TODO: Edit base on the chart  \n",
    "    \n",
    "- According to above chart, we can see that the minority class only consist of **5%** of the total data, if we apply **undersampling**, we will result in **25%** of data and lose **75%** of the information.\n",
    "    \n",
    "- According to above chart, we can see that the ratio of majority class and minority class is **20:1**. If we apply **oversampling**, we will duplicate **20** times of the minority class.\n",
    "    \n",
    "**[After first-round of experiment]**  \n",
    "- We find that oversampling on minority class does not improve our most of our model.\n",
    "- Perhaps we can also consider hybrid sampling in next sprint, which combines both the oversampling and undersampling\n",
    "    \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac152d5-64cd-465f-a3e8-89358b80fabc",
   "metadata": {},
   "source": [
    "[Back-to-top](#4-toc)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa74660b-8590-4490-9754-819374b9debc",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-models\">\n",
    "    <h2> Proposed Models </h2>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a081eab-8d8e-4403-8236-979c89388672",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üß† Ideas:</b></font>\n",
    "\n",
    "\n",
    "- [x] Logistic Regression\n",
    "- [x] Decision Tree\n",
    "- [x] Stochastic Gradient Boosting\n",
    "- [x] AdaBoost\n",
    "- [x] XGBoost\n",
    "- [ ] Random Forest\n",
    "- [ ] CatBoost\n",
    "- [ ] Naive Bayes\n",
    "- [ ] K-Nearest Neighbor\n",
    "- [ ] Neural Network (Less Deep)\n",
    "- [ ] Neural Network (Medium Deep)\n",
    "- [ ] Neural Network (More Deep)\n",
    "\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e8cc94-5696-42c1-b67b-bf7a04ae6b20",
   "metadata": {},
   "source": [
    "[Back-to-top](#4-toc)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc2a15c-df99-418f-b322-1cf9fea2e554",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-pipelines\">\n",
    "    <h2> Model Pipelines </h2>\n",
    "</a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4da34e7-493c-41ce-86b8-4e450c81c8b0",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>‚ö†Ô∏è Note:</b></font>\n",
    "\n",
    "- For faster training process, we will use 20% of our training data (Around 340K Rows) here by using `train_test_split` and setting `stratify` = `y`.\n",
    "- We will apply Oversampling technique to deal with class imbalance.\n",
    "- We will apply Grid Search CV with 5-fold for getting the optimized parameters.\n",
    "    \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91b2f43-be82-4116-8f5a-545e0b8b5ef8",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-model\">\n",
    "    <h3> Classes for Model Pipelines </h3>\n",
    "</a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "44a49b73-088e-4185-9c2e-772d8c5aa143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold,train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, TargetEncoder, StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "model_performance_dict = {\n",
    "    \"model_name\": None,\n",
    "    \"model\": None,\n",
    "    \n",
    "    \"sub_sampling\":False,\n",
    "    \"sub_sampling_time\":0,\n",
    "    \"sub_sampling_pct_X_train\":None,\n",
    "    \"sub_sampling_pct_X_test\":None,\n",
    "    \n",
    "    \"over_sampling\":False,\n",
    "    \"over_sampling_time\":0,\n",
    "    \n",
    "    \"X_train_shape\":None,\n",
    "    \"X_test_shape\":None,\n",
    "    \n",
    "    \"target_encoding\": False,\n",
    "    \"target_encoding_col\": [],\n",
    "    \"one_hot_encoding\": False,\n",
    "    \"one_hot_encoding_col\": [],\n",
    "    \n",
    "    \"standard_scaling\": False,\n",
    "    \"standard_scaling_col\": [],\n",
    "    \"min_max_scaling\": False,\n",
    "    \"min_max_scaling_col\": [],\n",
    "    \n",
    "    \"pca\": False,\n",
    "    \"pca_n_components\": None,\n",
    "    \n",
    "    \"scoring\":None,\n",
    "    \n",
    "    \"grid_search\": False,\n",
    "    \"best_params\": None,\n",
    "    \"best_train_score\": None,\n",
    "    \n",
    "    \"params\": None,\n",
    "    \"train_score\": None,\n",
    "    \n",
    "    \n",
    "    \"test_score\": None,    \n",
    "    \"confusion_matrix\": None,\n",
    "}\n",
    "\n",
    "class ModelPerf:\n",
    "    \n",
    "    _data = None\n",
    "    _path = None\n",
    "\n",
    "    def __init__(self,path,read = True):\n",
    "        self._path = path\n",
    "        # try to load the previous file\n",
    "        if read:\n",
    "            self.read_csv()\n",
    "        \n",
    "    def _is_path_valid(self):\n",
    "        os.path.isfile(self._path)\n",
    "        \n",
    "    def read_csv(self):\n",
    "        try:\n",
    "            self._data = pd.read_csv(self._path)\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Creating new model performance file...\")\n",
    "            self._data = pd.DataFrame()\n",
    "            \n",
    "    def export_csv(self):\n",
    "        if self._data.shape[0] > 0:\n",
    "            self._data.to_csv(self._path,index = False)\n",
    "            logger.info(f\"Model Performance CSV is exported\")\n",
    "        else:\n",
    "            logger.warning(f\"Model Performance is empty\")\n",
    "            \n",
    "    def add_data(self,new_data:dict,export = True):\n",
    "        new_df = pd.DataFrame([new_data])\n",
    "        self._data = pd.concat([self._data,new_df])\n",
    "        self._data.reset_index(drop = True, inplace=True)\n",
    "        if export:\n",
    "            self.export_csv()\n",
    "        \n",
    "    def get_data(self):\n",
    "        return self._data\n",
    "    \n",
    "    def print_data(self):\n",
    "        print(self.get_data())\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "class MyModel:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 X_train,X_test,\n",
    "                 y_train,y_test,\n",
    "                 train_subsample_size = None,\n",
    "                 test_subsample_size = None):\n",
    "        # Random State\n",
    "        self._random_state = 42\n",
    "        self._train_subsample_size = train_subsample_size\n",
    "        self._test_subsample_size = test_subsample_size\n",
    "        \n",
    "        # Store the data\n",
    "        self.X_train = X_train\n",
    "        le = LabelEncoder()\n",
    "        self.y_train = pd.Series(le.fit_transform(y_train),index= y_train)\n",
    "            \n",
    "        self.X_test = X_test\n",
    "        self.y_test = pd.Series(le.transform(y_test),index= y_test)\n",
    "        \n",
    "        self.print_data_size()\n",
    "                \n",
    "        self._classes = sorted(list(set(y_train)))\n",
    "        \n",
    "        # Default is 5-fold\n",
    "        self.cv = None\n",
    "        \n",
    "        # Subsampling\n",
    "        self._subsampled = False\n",
    "        \n",
    "        # OverSampling\n",
    "        self._oversampled = False\n",
    "        \n",
    "        # Custom Encoder\n",
    "        self._one_hot_transformer = None\n",
    "        self._one_hot_col = []\n",
    "        self._tar_end_transformer = None\n",
    "        self._tar_end_col = []\n",
    "        self.encoder = None\n",
    "        self._encoder_steps = []\n",
    "        self._encoder_name = \"\"\n",
    "        \n",
    "        # Custom Scaler\n",
    "        self._standard_scaler = None\n",
    "        self._ss_col = []\n",
    "        self._min_max_scaler = None\n",
    "        self._mm_col = []\n",
    "        self.scaler = None\n",
    "        self._scaler_steps = []\n",
    "        self._scaler_name = \"\"\n",
    "        \n",
    "        # Custom PCA\n",
    "        self.pca = None\n",
    "        self._pca_name = None\n",
    "        self._pca_n_components = None\n",
    "        \n",
    "        # Grid Search\n",
    "        self._use_grid_search= False\n",
    "        self.grid_search = None\n",
    "        self.gs_params = {}\n",
    "        \n",
    "        # Custom Model\n",
    "        self.model = None\n",
    "        self._model_name = \"\"\n",
    "        \n",
    "        # Custom Score\n",
    "        self._scoring = \"f1_weighted\"\n",
    "        self._scoring_func = lambda y_true,y_pred: f1_score(y_true,y_pred,average = \"weighted\")\n",
    "        \n",
    "        # Custom Pipeline\n",
    "        self.pipeline = None\n",
    "        self._pipeline_steps = []\n",
    "        \n",
    "        # Prediction\n",
    "        self.train_y_pred = None\n",
    "        self.y_pred = None\n",
    "        self.y_prob = None\n",
    "        \n",
    "        # Timer\n",
    "        self._subsample_time = None\n",
    "        self._oversampling_time = None\n",
    "        self._fit_time = None\n",
    "        self._predict_time = None\n",
    "        \n",
    "        \n",
    "        # Model Performance\n",
    "        self.model_perf = None\n",
    "        self._raw_cm = None\n",
    "        self._cm = None\n",
    "        self._cv_results = None\n",
    "        \n",
    "    def init_subsampling(self):\n",
    "        if self._train_subsample_size is not None or self._test_subsample_size is not None:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            if self._train_subsample_size is not None:\n",
    "                # Sample the train dataset with train_test_split function\n",
    "                self.X_train, _, self.y_train, _ = train_test_split(self.X_train, self.y_train,\n",
    "                                                                    stratify = self.y_train, \n",
    "                                                                    train_size = self._train_subsample_size,\n",
    "                                                                    random_state = self._random_state)\n",
    "            if self._test_subsample_size is not None:\n",
    "                # Sample the test dataset with train_test_split function\n",
    "                _, self.X_test, _ ,self.y_test= train_test_split(self.X_test, self.y_test,\n",
    "                                                                 stratify = self.y_test, \n",
    "                                                                 test_size = self._test_subsample_size, \n",
    "                                                                 random_state = self._random_state)\n",
    "\n",
    "            end_time = time.time()\n",
    "            \n",
    "            self._subsample_time = end_time - start_time\n",
    "            logger.info(f\"Subsampling Completed | Time elapsed: {self.time_to_str(self._subsample_time)}\")\n",
    "            self.print_data_size(title = \"After Subsampling\")\n",
    "            self._subsampled = True\n",
    "            \n",
    "        else: \n",
    "            logger.error(\"'train_subsample_size' & 'test_subsample_size' cannot be None.\")\n",
    "        \n",
    "    def init_over_sampling(self):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        logger.info(\"Oversampling on Train Data...\")\n",
    "        self.X_train, self.y_train = self.random_over_sampling(self.X_train,self.y_train)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        self._oversampling_time = end_time - start_time\n",
    "        logger.info(f\"Oversampling Completed | Time elapsed: {self.time_to_str(self._oversampling_time)}\")\n",
    "        self.print_data_size(title = \"After Oversampling\")\n",
    "        self._oversampled = True\n",
    "        \n",
    "        \n",
    "    def init_one_hot_encoding(self,encoder = None, columns=[]):\n",
    "        if len(columns) > 0:\n",
    "            if encoder is None:\n",
    "                self._one_hot_transformer = Pipeline(\n",
    "                    steps=[(\"one_hot\",OneHotEncoder(sparse_output = False,drop=\"first\"))]\n",
    "                )\n",
    "            else:\n",
    "                self._one_hot_transformer = Pipeline(\n",
    "                    steps=[(\"one_hot\",encoder)]\n",
    "                )\n",
    "                \n",
    "            self._set_one_hot_col(columns)\n",
    "            \n",
    "            # Add to encoder steps\n",
    "            self._encoder_steps.append((\"one_hot\",self._one_hot_transformer,self._one_hot_col))\n",
    "            \n",
    "            logger.info(\"One-hot Encoder Initialization Completed\")\n",
    "            self.print_empty()\n",
    "        \n",
    "    def init_target_encoding(self,encoder = None, columns=[]):\n",
    "        if len(columns) > 0:\n",
    "            if encoder is None:\n",
    "                self._tar_end_transformer = Pipeline(\n",
    "                    steps=[(\"tar_end\",TargetEncoder(target_type=\"continuous\",\n",
    "                                                    random_state=self._random_state))]\n",
    "                )\n",
    "            else:\n",
    "                self._tar_end_transformer = Pipeline(\n",
    "                    steps=[(\"tar_end\",encoder)]\n",
    "                )\n",
    "                \n",
    "            self._set_target_encode_col(columns)\n",
    "            \n",
    "            # Add to encoder steps\n",
    "            self._encoder_steps.append((\"tar_end\",self._tar_end_transformer,self._tar_end_col))\n",
    "            \n",
    "            logger.info(\"Target Encoder Initialization Completed\")\n",
    "            self.print_empty()\n",
    "\n",
    "    def init_encoder(self,name = \"encoders\"):        \n",
    "        if len(self._encoder_steps) == 0:\n",
    "            self.encoder = None\n",
    "        else:\n",
    "            self.encoder = ColumnTransformer(\n",
    "                transformers=self._encoder_steps,\n",
    "                remainder = \"passthrough\",\n",
    "                verbose_feature_names_out = False\n",
    "            )\n",
    "            self._encoder_name = name\n",
    "            self._pipeline_steps.append((self._encoder_name,self.encoder))\n",
    "        \n",
    "    def init_standard_scaler(self,scaler = None, columns=[]):\n",
    "        if len(columns) > 0:\n",
    "            if scaler is None:\n",
    "                self._standard_scaler = Pipeline(\n",
    "                    steps=[(\"standard_scaler\",StandardScaler()),]\n",
    "                )\n",
    "            else:\n",
    "                self._standard_scaler = Pipeline(\n",
    "                    steps=[(\"standard_scaler\",scaler),]\n",
    "                )\n",
    "                \n",
    "            self._set_standard_scaling_col(columns)\n",
    "                \n",
    "            # Add to encoder steps\n",
    "            self._scaler_steps.append((\"standard_scaler\",self._standard_scaler,self._ss_col))\n",
    "            \n",
    "            logger.info(\"Standard Scaler Initialization Completed\")\n",
    "            self.print_empty()\n",
    "        \n",
    "    def init_min_max_scaler(self,scaler = None,columns=[]):\n",
    "        if len(columns) > 0:\n",
    "            if scaler is None:\n",
    "                self._min_max_scaler = Pipeline(\n",
    "                    steps=[(\"min_max_scaler\",MinMaxScaler()),]\n",
    "                )\n",
    "            else:\n",
    "                self._min_max_scaler = Pipeline(\n",
    "                    steps=[(\"min_max_scaler\",scaler),]\n",
    "                )\n",
    "            \n",
    "            self._set_min_max_scaling_col(columns)\n",
    "                \n",
    "            # Add to encoder steps\n",
    "            self._scaler_steps.append((\"min_max_scaler\",self._min_max_scaler,self._mm_col))\n",
    "            \n",
    "            logger.info(\"Min-Max Scaler Initialization Completed\")\n",
    "            self.print_empty()\n",
    "            \n",
    "    def init_scaler(self,name = \"scalers\"):\n",
    "        if len(self._scaler_steps) == 0:\n",
    "            self.scaler = None\n",
    "        else:        \n",
    "            self.scaler = ColumnTransformer(\n",
    "                transformers=self._scaler_steps,\n",
    "                remainder = \"passthrough\",\n",
    "                verbose_feature_names_out = False\n",
    "            )\n",
    "            self._scaler_name = name\n",
    "            self._pipeline_steps.append((self._scaler_name,self.scaler))\n",
    "            \n",
    "    def init_pca(self,name=\"pca\",n_components = None):\n",
    "        \n",
    "        if n_components is None:\n",
    "            self.pca = PCA()\n",
    "        else:\n",
    "            self._pca_n_components = n_components\n",
    "            self.pca = PCA(n_components = n_components)\n",
    "            \n",
    "        self._pca_name = name\n",
    "        self._pipeline_steps.append((self._pca_name,self.pca))\n",
    "        logger.info(\"PCA Initialization Completed\")\n",
    "        self.print_empty()\n",
    "        \n",
    "    def init_model(self,name,model):\n",
    "        self.model = model\n",
    "        self._model_name = name\n",
    "        self._pipeline_steps.append((self._model_name,self.model))\n",
    "        logger.info(f\"{self.model.__class__.__name__} (name: {self._model_name}) Initialization Completed\")\n",
    "        self.print_empty()\n",
    "                \n",
    "        \n",
    "    def init_grid_search(self,params,n_job = 1,no_cv = False):\n",
    "        if self.pipeline is None:\n",
    "            logger.error(\"Please initialize the model pipeline first\")\n",
    "            return\n",
    "        if len(params) == 0:\n",
    "            logger.error(f\"Grid Search params cannot be empty.\")\n",
    "        else:\n",
    "            self._set_grid_search_params(params)\n",
    "            self.grid_search = GridSearchCV(self.pipeline,\n",
    "                                            self.gs_params,\n",
    "                                            n_jobs = n_job,\n",
    "                                            cv = self.cv if not no_cv else None,\n",
    "                                            scoring = self._scoring,\n",
    "                                            verbose= 0)\n",
    "            self._use_grid_search = True\n",
    "            if self.cv is not None:\n",
    "                logger.info(f\"Grid Search with {self.cv if isinstance(self.cv, int) else self.cv.get_n_splits()}-folds cross-validation Initialized Completed\")\n",
    "            else:\n",
    "                logger.info(f\"Grid Search without cross-validation Initialized Completed\")\n",
    "            \n",
    "            self.print_empty()\n",
    "     \n",
    "    def init_pipeline(self):\n",
    "        if self.model is None:\n",
    "            logger.error(f\"No Model is initiated\")\n",
    "            return\n",
    "        if len(self._pipeline_steps) == 0:\n",
    "            logger.error(f\"No steps in pipeline\")\n",
    "            return\n",
    "        else:           \n",
    "            self.pipeline = Pipeline(self._pipeline_steps,verbose=False)\n",
    "            self.pipeline.set_output(transform=\"pandas\")\n",
    "            \n",
    "            \n",
    "    # ===================== Set Function ============================== \n",
    "        \n",
    "    def set_kfold_cv(self,cv = 5):\n",
    "        if cv is None:\n",
    "            self.cv = None\n",
    "        assert isinstance(cv,int), \"'cv' must be integer\"\n",
    "        assert cv >= 2, \"'cv' must be greater than 1\"\n",
    "        self.cv = StratifiedKFold(n_splits = cv)\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    def _set_one_hot_col(self,columns=[]):\n",
    "        if len(columns) == 0:\n",
    "            return\n",
    "        else:\n",
    "            for col in columns:\n",
    "                assert col in self.X_train.columns, f\"{col} not found in X_train\"\n",
    "                assert col in self.X_test.columns, f\"{col} not found in X_test\"\n",
    "            self._one_hot_col = columns\n",
    "            \n",
    "            \n",
    "    def _set_target_encode_col(self,columns=[]):\n",
    "        if len(columns) == 0:\n",
    "            return\n",
    "        else:\n",
    "            for col in columns:\n",
    "                assert col in self.X_train.columns, f\"{col} not found in X_train\"\n",
    "                assert col in self.X_test.columns, f\"{col} not found in X_test\"\n",
    "            self._tar_end_col = columns\n",
    "            \n",
    "            \n",
    "    def _set_standard_scaling_col(self,columns=[]):\n",
    "        if len(columns) == 0:\n",
    "            return\n",
    "        else:\n",
    "            for col in columns:\n",
    "                assert col in self.X_train.columns, f\"{col} not found in X_train\"\n",
    "                assert col in self.X_test.columns, f\"{col} not found in X_test\"\n",
    "            self._ss_col = columns\n",
    "        \n",
    "    def _set_min_max_scaling_col(self,columns=[]):\n",
    "        if len(columns) == 0:\n",
    "            return\n",
    "        else:\n",
    "            for col in columns:\n",
    "                assert col in self.X_train.columns, f\"{col} not found in X_train\"\n",
    "                assert col in self.X_test.columns, f\"{col} not found in X_test\"\n",
    "            self._mm_col = columns\n",
    "            \n",
    "    def _set_grid_search_params(self,params):\n",
    "        available_prefix = [step[0] for step in self.pipeline.steps]\n",
    "        for param in params:\n",
    "            assert param.split(\"__\")[0] in available_prefix, f\"Grid Search Params {param} not found. Only found: {available_prefix}\"\n",
    "        self.gs_params = params    \n",
    "    \n",
    "    # ================== Model Fitting =================\n",
    "    def fit(self):\n",
    "        if self._use_grid_search:\n",
    "            start_time = time.time()\n",
    "            logger.info(f\"Model Fitting with Grid Search...\")\n",
    "            self.grid_search.fit(self.X_train,self.y_train)\n",
    "            self.cv_results = pd.DataFrame(self.grid_search.cv_results_)\n",
    "            self.cv_results.to_csv(f\"log/gs-{self._model_name}-{datetime.datetime.now().strftime('%Y-%m-%d--%H-%M-%S')}.csv\")\n",
    "            logger.info(\"Grid Search Cross Validationn Results Exported.\")\n",
    "        else:\n",
    "            start_time = time.time()\n",
    "            logger.info(f\"Model Fitting...\")\n",
    "            self.pipeline.fit(self.X_train,self.y_train)\n",
    "            self.train_y_pred = self.pipeline.predict(self.X_train)\n",
    "            \n",
    "        end_time = time.time()\n",
    "        self._fit_time = end_time - start_time\n",
    "        logger.info(f\"Total Fitting Time:{self.time_to_str(self._fit_time)}\")\n",
    "        self.print_empty()\n",
    "        \n",
    "    \n",
    "    # ================== Model Prediction =================\n",
    "    def predict(self):\n",
    "        logger.info(\"Predicting on Test Data...\")\n",
    "        start_time = time.time()\n",
    "        if self._use_grid_search:\n",
    "            try:\n",
    "                self.y_prob = self.grid_search.predict_proba(self.X_test)\n",
    "            except AttributeError as e:\n",
    "                logger.error(f\"{e}\")\n",
    "            self.y_pred = self.grid_search.predict(self.X_test)\n",
    "        else:\n",
    "            try:\n",
    "                self.y_prob = self.pipeline.predict_proba(self.X_test)\n",
    "            except AttributeError as e:\n",
    "                logger.error(f\"{e}\")\n",
    "            self.y_pred = self.pipeline.predict(self.X_test)\n",
    "\n",
    "        end_time = time.time()\n",
    "        self._predict_time = end_time - start_time\n",
    "        logger.info(f\"Total Predicting Time:{self.time_to_str(self._predict_time)}\")\n",
    "        self.print_empty()\n",
    "        \n",
    "        \n",
    "    # ================== Model Evaluation =================\n",
    "    def get_best_train_score(self):\n",
    "        if self._use_grid_search:\n",
    "            return self.grid_search.best_score_\n",
    "        else:\n",
    "            return self._scoring_func(self.y_train,self.train_y_pred)\n",
    "        \n",
    "    def print_best_train_score(self):\n",
    "        logger.info(f\"Best Train Score: {self.get_best_train_score()}\")\n",
    "        \n",
    "    def get_test_score(self):\n",
    "        if self.y_pred is not None:\n",
    "            return self._scoring_func(self.y_test,self.y_pred)\n",
    "        else:\n",
    "            logger.error(f\"No y_pred found for testing score.\")\n",
    "    \n",
    "    def print_test_score(self):\n",
    "        if self.y_pred is not None:\n",
    "            logger.info(f\"Test Score: {self.get_test_score()}\")\n",
    "        else:\n",
    "            logger.error(f\"No y_pred found for testing score.\")\n",
    "        \n",
    "    def get_best_params(self):\n",
    "        if self._use_grid_search:\n",
    "            return self.grid_search.best_params_\n",
    "        else:\n",
    "            return self.pipeline.steps[-1][1].get_params()\n",
    "        \n",
    "    def print_best_params(self):\n",
    "        self.print_title(title = \"Parameters\")\n",
    "        pprint.pprint(self.get_best_params())\n",
    "        \n",
    "    def compute_raw_confusion_matrix(self):\n",
    "        if self.y_pred is not None:\n",
    "            logger.info(\"Computing Confusion Matrix...\")\n",
    "            self._raw_cm = confusion_matrix(self.y_test,self.y_pred)\n",
    "        else:\n",
    "            logger.error(\"Please make prediction first.\")\n",
    "    \n",
    "    def compute_confusion_matrix(self):\n",
    "        if self._raw_cm is None:\n",
    "            self.compute_raw_confusion_matrix()\n",
    "        self._cm = pd.DataFrame(self._raw_cm,\n",
    "                                index = [f\"actual_{i}\" for i in self._classes],\n",
    "                                columns = [f\"predict_{i}\" for i in self._classes])\n",
    "        \n",
    "    def print_confusion_matrix(self):\n",
    "        if self._cm is None:\n",
    "            self.compute_confusion_matrix()\n",
    "                    \n",
    "        self.print_title(title = \"Confusion Matrix on Test Data\")\n",
    "        print(self._cm)\n",
    "    \n",
    "    # ================== Helper Function =================\n",
    "    @staticmethod\n",
    "    def time_to_str(t):\n",
    "        time_list = str(datetime.timedelta(seconds=t)).split(\".\")[0].split(\":\")\n",
    "        return f\"\\t{time_list[0]} hours {time_list[1]} minutes {time_list[2]} seconds\"\n",
    "    \n",
    "    def print_data_size(self, title = \"Data Shape\"):\n",
    "        self.print_title(title)\n",
    "        logger.info(f\"X_train: {self.X_train.shape} | y_train: {self.y_train.shape}\")\n",
    "        logger.info(f\"X_test : {self.X_test.shape} | y_test : {self.y_test.shape}\")\n",
    "        self.print_empty()\n",
    "        \n",
    "    @staticmethod\n",
    "    def print_title(title):\n",
    "        num_of_equal = 15\n",
    "        logger.info(f\"{'='*num_of_equal} {title} {'='*num_of_equal}\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def print_empty():\n",
    "        logger.info(\"\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def random_over_sampling(X,y):\n",
    "        assert X.shape[0] == y.shape[0]\n",
    "\n",
    "        X.reset_index(drop = True,inplace = True)\n",
    "        y.reset_index(drop = True,inplace = True)\n",
    "\n",
    "        # Join both table\n",
    "        target_col = \"price_range\"\n",
    "        X[target_col] = y\n",
    "\n",
    "        # Get the class in target variable\n",
    "        _classes = sorted(list(set(y)))\n",
    "\n",
    "        # Value Count for each class\n",
    "        val_count = y.value_counts()\n",
    "        # print(val_count)\n",
    "\n",
    "        # Get the maximum count\n",
    "        max_count = max(val_count)\n",
    "\n",
    "        # list to store sampled data\n",
    "        new_data_list = []\n",
    "\n",
    "        for i in _classes:\n",
    "            diff = max_count - val_count[i]\n",
    "\n",
    "            # set seed to for reproducibilty\n",
    "            rand_seed = 0\n",
    "\n",
    "            # cache the filter dataframe\n",
    "            this_X = X[X[target_col]==i]\n",
    "\n",
    "            sample_batch_size = min(diff,min(val_count[i],5000)) # Round off to nearest thousand, to avoid duplicate the whole sample set => more variance\n",
    "\n",
    "            while diff > 0:\n",
    "                # For eac\n",
    "                sampled_data = this_X.sample(min(diff,sample_batch_size), # for last iteration, use diff to make sure all classes have same same sample size\n",
    "                                  replace = False, # Ensure no duplicates in this batch\n",
    "                                  random_state = rand_seed)\n",
    "                new_data_list.append(sampled_data)\n",
    "                diff -= sample_batch_size\n",
    "                rand_seed += 1\n",
    "\n",
    "        new_data_list.append(X)\n",
    "        new_data = pd.concat(new_data_list)\n",
    "        new_data.reset_index(drop = True, inplace = True)\n",
    "        new_X = new_data.drop(columns = [target_col])\n",
    "        new_y = new_data[target_col]\n",
    "\n",
    "        return new_X, new_y\n",
    "    \n",
    "\n",
    "    def add_perf(self,model_res: ModelPerf):\n",
    "        this_dict = model_performance_dict.copy()\n",
    "        \n",
    "        this_dict[\"model_name\"] = self._model_name\n",
    "        this_dict[\"model\"] = self.model.__class__.__name__\n",
    "        \n",
    "        this_dict[\"sub_sampling\"] = self._subsampled\n",
    "        this_dict[\"sub_sampling_time\"] = self._subsample_time\n",
    "        this_dict[\"sub_sampling_pct_X_train\"] = self._train_subsample_size\n",
    "        this_dict[\"sub_sampling_pct_X_test\"] = self._train_subsample_size\n",
    "        \n",
    "        this_dict[\"over_sampling\"] = self._oversampled\n",
    "        this_dict[\"over_sampling_time\"] = self._oversampling_time\n",
    "        \n",
    "        this_dict[\"X_train_shape\"] = self.X_train.shape\n",
    "        this_dict[\"X_test_shape\"] = self.X_test.shape\n",
    "        \n",
    "        if self._tar_end_transformer is not None:\n",
    "            this_dict[\"target_encoding\"] = True\n",
    "            this_dict[\"target_encoding_col\"] = self._tar_end_col\n",
    "            \n",
    "        \n",
    "        if self._one_hot_transformer is not None:\n",
    "            this_dict[\"one_hot_encoding\"] = True\n",
    "            this_dict[\"one_hot_encoding_col\"] = self._one_hot_col\n",
    "            \n",
    "        if self._standard_scaler is not None:\n",
    "            this_dict[\"standard_scaling\"] = True\n",
    "            this_dict[\"standard_scaling_col\"] = self._ss_col\n",
    "            \n",
    "        if self._min_max_scaler is not None:\n",
    "            this_dict[\"min_max_scaling\"] = True\n",
    "            this_dict[\"min_max_scaling_col\"] = self._mm_col\n",
    "            \n",
    "        if self.pca is not None:\n",
    "            this_dict[\"pca\"] = True\n",
    "            this_dict[\"pca_n_components\"] = self._pca_n_components\n",
    "            \n",
    "        this_dict[\"scoring\"] = self._scoring\n",
    "        if self._use_grid_search:\n",
    "            this_dict[\"grid_search\"] = self._use_grid_search \n",
    "            this_dict[\"best_params\"] = self.get_best_params()\n",
    "            this_dict[\"best_train_score\"] = self.get_best_train_score()\n",
    "        else:\n",
    "            this_dict[\"params\"] = self.get_best_params()\n",
    "            this_dict[\"train_score\"] = self.get_best_train_score()\n",
    "            \n",
    "        \n",
    "        this_dict[\"test_score\"] = self.get_test_score()\n",
    "        if self._cm is not None:\n",
    "            this_dict[\"confusion_matrix\"] = self._raw_cm\n",
    "            \n",
    "            \n",
    "        model_res.add_data(this_dict)\n",
    "        return model_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "661cde7f-8547-4624-ab7c-974155dc7b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log the model performance\n",
    "perf = ModelPerf(\"log/model_performance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7a41d9a2-fc1d-4967-8777-6ef649323442",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use 20% of the data to train the model and make prediction on the whole test data \n",
    "global_subsample_pct = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d5f29b-440b-4750-a700-3087ceaa6382",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-model\">\n",
    "    <h3> Model - Logistic Regression (Baseline Model) </h3>\n",
    "</a>\n",
    "<br>\n",
    "<b>Why Logistic Regression Model is the baseline model?</b>\n",
    "<ol>\n",
    "    <li><b>Simplicity</b></li>\n",
    "    <p>For classification task, <b>Logistic Regression</b> is a simple and interpretable linear model. It models the relationship between the input features and the binary outcome by applying the logistic function to a linear combination of the input features. </p>       \n",
    "    <li><b>Interpretability</b></li>\n",
    "    <p>The coefficients in Logistic Regression provide a direct interpretation of the impact of each feature on the log-odds of the outcome. This interpretability is valuable for gaining insights into the relationships between predictors and the target variable. </p>         \n",
    "    <li><b>Robustness to Irrelevant Features</b></li>\n",
    "    <p>Logistic Regression can be robust to irrelevant features, as its regularization techniques (e.g., L1 or L2 regularization) help prevent overfitting and suppress the impact of less informative features. </p>     \n",
    "\n",
    "</ol>\n",
    "\n",
    "Therefore, any model tested later on should be compared with Logistic Regression baseline model.    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc141f4-6bd4-4cb0-8415-dcdf5f1c2a2d",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>‚ö†Ô∏è Note:</b></font>\n",
    "\n",
    "For baseline model, we just use Logistic Regression to get our first and simple modelling result.\n",
    "    \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca9297-1c87-4abe-bc58-6fcf7a413f3f",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e7bc2aa2-b40d-418b-8cd1-193738465d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-09 20:09:07.f - INFO >>> =============== Data Shape ===============\n",
      "2023-11-09 20:09:07.f - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-09 20:09:07.f - INFO >>> X_test : (588151, 22) | y_test : (588151,)\n",
      "2023-11-09 20:09:07.f - INFO >>> \n",
      "2023-11-09 20:09:09.f - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 01 seconds\n",
      "2023-11-09 20:09:09.f - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-09 20:09:09.f - INFO >>> X_train: (352890, 21) | y_train: (352890,)\n",
      "2023-11-09 20:09:09.f - INFO >>> X_test : (588151, 22) | y_test : (588151,)\n",
      "2023-11-09 20:09:09.f - INFO >>> \n",
      "2023-11-09 20:09:09.f - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-09 20:09:09.f - INFO >>> \n",
      "2023-11-09 20:09:09.f - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-09 20:09:09.f - INFO >>> \n",
      "2023-11-09 20:09:09.f - INFO >>> Standard Scaler Initialization Completed\n",
      "2023-11-09 20:09:09.f - INFO >>> \n",
      "2023-11-09 20:09:09.f - INFO >>> LogisticRegression (name: logit_v1) Initialization Completed\n",
      "2023-11-09 20:09:09.f - INFO >>> \n",
      "2023-11-09 20:09:09.f - INFO >>> Model Fitting...\n",
      "2023-11-09 20:10:25.f - INFO >>> Total Fitting Time:\t0 hours 01 minutes 15 seconds\n",
      "2023-11-09 20:10:25.f - INFO >>> \n",
      "2023-11-09 20:10:25.f - INFO >>> Predicting on Test Data...\n",
      "2023-11-09 20:10:30.f - INFO >>> Total Predicting Time:\t0 hours 00 minutes 04 seconds\n",
      "2023-11-09 20:10:30.f - INFO >>> \n",
      "2023-11-09 20:10:30.f - INFO >>> Best Train Score: 0.7198456065410235\n",
      "2023-11-09 20:10:30.f - INFO >>> Test Score: 0.7193415430356718\n",
      "2023-11-09 20:10:30.f - INFO >>> =============== Parameters ===============\n",
      "{'C': 1,\n",
      " 'class_weight': None,\n",
      " 'dual': False,\n",
      " 'fit_intercept': True,\n",
      " 'intercept_scaling': 1,\n",
      " 'l1_ratio': None,\n",
      " 'max_iter': 10000,\n",
      " 'multi_class': 'auto',\n",
      " 'n_jobs': None,\n",
      " 'penalty': 'l2',\n",
      " 'random_state': None,\n",
      " 'solver': 'lbfgs',\n",
      " 'tol': 0.0001,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n",
      "2023-11-09 20:10:30.f - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-09 20:10:30.f - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     125502      29226       3412        537        154\n",
      "actual_2      27144     209015      21969       1397        363\n",
      "actual_3        420      36774      73389       4154       1140\n",
      "actual_4         65       1754      21656       9380       3062\n",
      "actual_5         37        286       2905       5530       8880\n",
      "2023-11-09 20:10:31.f - INFO >>> Model Performance CSV is exported\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def model(test = False):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsample_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    model.init_standard_scaler(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_scaler() # Activate all scalers and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    model_name = \"logit_v1\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = LogisticRegression(penalty = \"l2\", # Avoid Overfitting\n",
    "                                                max_iter=10000,\n",
    "                                                C = 1))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "logit_v1 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ca8006-58af-4381-9f85-f9392baa4c7a",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment:</b></font>\n",
    "    \n",
    "- From the confusion matrix, we might see a bit of class imbalance issue. Let's try to apply oversampling.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1357c5c6-155c-4cdb-821b-814cff66a7ec",
   "metadata": {},
   "source": [
    "#### Logistic Regression + Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f91f0abb-9be6-4cb3-8863-cdae9546c460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-09 20:10:31.f - INFO >>> =============== Data Shape ===============\n",
      "2023-11-09 20:10:31.f - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-09 20:10:31.f - INFO >>> X_test : (588151, 22) | y_test : (588151,)\n",
      "2023-11-09 20:10:31.f - INFO >>> \n",
      "2023-11-09 20:10:33.f - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 01 seconds\n",
      "2023-11-09 20:10:33.f - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-09 20:10:33.f - INFO >>> X_train: (352890, 21) | y_train: (352890,)\n",
      "2023-11-09 20:10:33.f - INFO >>> X_test : (588151, 22) | y_test : (588151,)\n",
      "2023-11-09 20:10:33.f - INFO >>> \n",
      "2023-11-09 20:10:33.f - INFO >>> Oversampling on Train Data...\n",
      "2023-11-09 20:10:34.f - INFO >>> Oversampling Completed | Time elapsed: \t0 hours 00 minutes 00 seconds\n",
      "2023-11-09 20:10:34.f - INFO >>> =============== After Oversampling ===============\n",
      "2023-11-09 20:10:34.f - INFO >>> X_train: (779665, 21) | y_train: (779665,)\n",
      "2023-11-09 20:10:34.f - INFO >>> X_test : (588151, 22) | y_test : (588151,)\n",
      "2023-11-09 20:10:34.f - INFO >>> \n",
      "2023-11-09 20:10:34.f - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-09 20:10:34.f - INFO >>> \n",
      "2023-11-09 20:10:34.f - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-09 20:10:34.f - INFO >>> \n",
      "2023-11-09 20:10:34.f - INFO >>> Standard Scaler Initialization Completed\n",
      "2023-11-09 20:10:34.f - INFO >>> \n",
      "2023-11-09 20:10:34.f - INFO >>> LogisticRegression (name: logit_v2) Initialization Completed\n",
      "2023-11-09 20:10:34.f - INFO >>> \n",
      "2023-11-09 20:10:34.f - INFO >>> Model Fitting...\n",
      "2023-11-09 20:14:51.f - INFO >>> Total Fitting Time:\t0 hours 04 minutes 17 seconds\n",
      "2023-11-09 20:14:51.f - INFO >>> \n",
      "2023-11-09 20:14:51.f - INFO >>> Predicting on Test Data...\n",
      "2023-11-09 20:14:55.f - INFO >>> Total Predicting Time:\t0 hours 00 minutes 03 seconds\n",
      "2023-11-09 20:14:55.f - INFO >>> \n",
      "2023-11-09 20:14:55.f - INFO >>> Best Train Score: 0.689168831537984\n",
      "2023-11-09 20:14:55.f - INFO >>> Test Score: 0.6941063405317159\n",
      "2023-11-09 20:14:55.f - INFO >>> =============== Parameters ===============\n",
      "{'C': 1,\n",
      " 'class_weight': None,\n",
      " 'dual': False,\n",
      " 'fit_intercept': True,\n",
      " 'intercept_scaling': 1,\n",
      " 'l1_ratio': None,\n",
      " 'max_iter': 10000,\n",
      " 'multi_class': 'auto',\n",
      " 'n_jobs': None,\n",
      " 'penalty': 'l2',\n",
      " 'random_state': None,\n",
      " 'solver': 'lbfgs',\n",
      " 'tol': 0.0001,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n",
      "2023-11-09 20:14:55.f - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-09 20:14:55.f - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     136482      16123       3691       1878        657\n",
      "actual_2      48831     162207      42069       5158       1623\n",
      "actual_3        847      16196      73765      21484       3585\n",
      "actual_4         82        370       7262      19836       8367\n",
      "actual_5         39         64        384       3360      13791\n",
      "2023-11-09 20:14:56.f - INFO >>> Model Performance CSV is exported\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def model(test = False):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsample_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "    model.init_over_sampling() # Oversampling for class imabalance\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "    \n",
    "    # Instantiate scaler steps\n",
    "    model.init_standard_scaler(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_scaler() # Activate all scalers and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    model_name = \"logit_v2\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = LogisticRegression(penalty = \"l2\", # Avoid Overfitting\n",
    "                                                max_iter=10000,\n",
    "                                                C = 1))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "logit_v2 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e4c76a-d1c2-4c71-8248-aa89998b1b1a",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment:</b></font>\n",
    "\n",
    "- From the above confusion matrix result, it seems that `Oversampling` does not help much with Logistic Regression.\n",
    "- Let's apply `grid search` for the best parameters.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76efddbc-9699-4451-980a-fd3a90d0c3ac",
   "metadata": {},
   "source": [
    "#### Logistic Regression + Grid Search 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ae31efab-ae1a-4857-a114-3b5b608ab15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-09 20:14:56.f - INFO >>> =============== Data Shape ===============\n",
      "2023-11-09 20:14:56.f - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-09 20:14:56.f - INFO >>> X_test : (588151, 22) | y_test : (588151,)\n",
      "2023-11-09 20:14:56.f - INFO >>> \n",
      "2023-11-09 20:14:58.f - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 01 seconds\n",
      "2023-11-09 20:14:58.f - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-09 20:14:58.f - INFO >>> X_train: (352890, 21) | y_train: (352890,)\n",
      "2023-11-09 20:14:58.f - INFO >>> X_test : (588151, 22) | y_test : (588151,)\n",
      "2023-11-09 20:14:58.f - INFO >>> \n",
      "2023-11-09 20:14:58.f - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-09 20:14:58.f - INFO >>> \n",
      "2023-11-09 20:14:58.f - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-09 20:14:58.f - INFO >>> \n",
      "2023-11-09 20:14:58.f - INFO >>> Standard Scaler Initialization Completed\n",
      "2023-11-09 20:14:58.f - INFO >>> \n",
      "2023-11-09 20:14:58.f - INFO >>> LogisticRegression (name: logit_v3) Initialization Completed\n",
      "2023-11-09 20:14:58.f - INFO >>> \n",
      "2023-11-09 20:14:58.f - INFO >>> Grid Search with 5-folds cross-validation Initialized Completed\n",
      "2023-11-09 20:14:58.f - INFO >>> \n",
      "2023-11-09 20:14:58.f - INFO >>> Model Fitting with Grid Search...\n",
      "2023-11-09 20:40:10.f - INFO >>> Grid Search Cross Validationn Results Exported.\n",
      "2023-11-09 20:40:10.f - INFO >>> Total Fitting Time:\t0 hours 25 minutes 12 seconds\n",
      "2023-11-09 20:40:10.f - INFO >>> \n",
      "2023-11-09 20:40:10.f - INFO >>> Predicting on Test Data...\n",
      "2023-11-09 20:40:13.f - INFO >>> Total Predicting Time:\t0 hours 00 minutes 03 seconds\n",
      "2023-11-09 20:40:13.f - INFO >>> \n",
      "2023-11-09 20:40:13.f - INFO >>> Best Train Score: 0.7178077166770628\n",
      "2023-11-09 20:40:14.f - INFO >>> Test Score: 0.71763150089859\n",
      "2023-11-09 20:40:14.f - INFO >>> =============== Parameters ===============\n",
      "{'logit_v3__C': 1000}\n",
      "2023-11-09 20:40:14.f - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-09 20:40:14.f - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     125402      29284       3471        528        146\n",
      "actual_2      26959     209489      21785       1318        337\n",
      "actual_3        392      37219      73521       3639       1106\n",
      "actual_4         62       1722      22613       8337       3183\n",
      "actual_5         35        277       3162       5219       8945\n",
      "2023-11-09 20:40:14.f - INFO >>> Model Performance CSV is exported\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def model(test = False):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsample_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    model.init_standard_scaler(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_scaler() # Activate all scalers and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    model_name = \"logit_v3\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = LogisticRegression(penalty = \"l2\", # Avoid Overfitting\n",
    "                                                max_iter=10000))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Set K-Fold CV\n",
    "    model.set_kfold_cv(5)\n",
    "    \n",
    "    # Instantiate the grid search\n",
    "    model.init_grid_search(params={f\"{model_name}__C\":[10 ** i for i in [-3,-1,0,1,3]],\n",
    "                                  },\n",
    "                           n_job = 1\n",
    "                          )\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "logit_v3 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536e4ada-7833-4126-a420-85364f1a9a6c",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment - Logistic Regression</b></font>\n",
    "\n",
    "    \n",
    "- From the above results for logistic regression, we obtained:\n",
    "|Model|Best Params / Params|training F1|testing F1|\n",
    "|---|---|:---:|:---:|\n",
    "|`Logistic Regression`|`{'C': 1}`|`71.93%`|`71.98%`|\n",
    "|`Logistic Regression` (with Oversampling)|`{'C': 1}`|`68.91%`|`69.41%`|\n",
    "|`Logistic Regression` (with 5-fold Grid Search CV)|`{'C': 1000}`|`71.78%`|`71.76%`|\n",
    "\n",
    "\n",
    "- Seems that OverSampling method does not work well in our case.    \n",
    "- While the first model give us a general insight on how well a logistic model can perform on our task. We should alway use cross validation to obtain a more accurate score for model evaluation.\n",
    "- For the following model, we will always apply 5-fold cross validation.\n",
    "- Therefore, we will use the first model (`Logistic Regression with 5-fold Grid Search CV`) as our baseline model.\n",
    "\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b98b511-c179-4bdd-abe2-aa7671fca6d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Interpret Model coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c77615d-54e3-4618-9344-12125b83aa55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c028979-b100-4ac8-9d65-0eb91c6417ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-model\">\n",
    "    <h3> Model - Decision Tree </h3>\n",
    "</a>\n",
    "    \n",
    "Other than Logistics Regression, we can also try to use Decision Tree.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcaeb59-87ef-4209-bca2-3092f98c6aa5",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>‚ö†Ô∏è Note:</b></font>\n",
    "\n",
    "- For Decision Tree, we can skip the scaling the data as decision tree optimization is not based on distance of value.\n",
    "    \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf23501-5b4b-487d-a919-61cc96c82315",
   "metadata": {},
   "source": [
    "#### Decision Tree + Grid Search 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6190c67a-fdaa-456a-aa00-15eb28dd3f98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-09 20:40:14.f - INFO >>> =============== Data Shape ===============\n",
      "2023-11-09 20:40:14.f - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-09 20:40:14.f - INFO >>> X_test : (588151, 22) | y_test : (588151,)\n",
      "2023-11-09 20:40:14.f - INFO >>> \n",
      "2023-11-09 20:40:16.f - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 01 seconds\n",
      "2023-11-09 20:40:16.f - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-09 20:40:16.f - INFO >>> X_train: (352890, 21) | y_train: (352890,)\n",
      "2023-11-09 20:40:16.f - INFO >>> X_test : (588151, 22) | y_test : (588151,)\n",
      "2023-11-09 20:40:16.f - INFO >>> \n",
      "2023-11-09 20:40:16.f - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-09 20:40:16.f - INFO >>> \n",
      "2023-11-09 20:40:16.f - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-09 20:40:16.f - INFO >>> \n",
      "2023-11-09 20:40:16.f - INFO >>> DecisionTreeClassifier (name: dtc_v1) Initialization Completed\n",
      "2023-11-09 20:40:16.f - INFO >>> \n",
      "2023-11-09 20:40:16.f - INFO >>> Grid Search with 5-folds cross-validation Initialized Completed\n",
      "2023-11-09 20:40:16.f - INFO >>> \n",
      "2023-11-09 20:40:16.f - INFO >>> Model Fitting with Grid Search...\n",
      "2023-11-09 20:43:10.f - INFO >>> Grid Search Cross Validationn Results Exported.\n",
      "2023-11-09 20:43:10.f - INFO >>> Total Fitting Time:\t0 hours 02 minutes 54 seconds\n",
      "2023-11-09 20:43:10.f - INFO >>> \n",
      "2023-11-09 20:43:10.f - INFO >>> Predicting on Test Data...\n",
      "2023-11-09 20:43:13.f - INFO >>> Total Predicting Time:\t0 hours 00 minutes 03 seconds\n",
      "2023-11-09 20:43:13.f - INFO >>> \n",
      "2023-11-09 20:43:13.f - INFO >>> Best Train Score: 0.8357624277719635\n",
      "2023-11-09 20:43:14.f - INFO >>> Test Score: 0.8404230649630143\n",
      "2023-11-09 20:43:14.f - INFO >>> =============== Parameters ===============\n",
      "{'dtc_v1__max_depth': 25, 'dtc_v1__min_samples_split': 50}\n",
      "2023-11-09 20:43:14.f - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-09 20:43:14.f - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     140045      18575        183         17         11\n",
      "actual_2      18921     226591      14180        144         52\n",
      "actual_3        304      17630      91556       6201        186\n",
      "actual_4         81        461       9641      23400       2334\n",
      "actual_5         54        155        522       3904      13003\n",
      "2023-11-09 20:43:14.f - INFO >>> Model Performance CSV is exported\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def model(test = False):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsample_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    this_model_name = \"dtc_v1\"\n",
    "    model.init_model(name = this_model_name,\n",
    "                     model = DecisionTreeClassifier())\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "    \n",
    "    # Set K-Fold CV\n",
    "    model.set_kfold_cv(5)\n",
    "    \n",
    "    # Instantiate the grid search\n",
    "    model.init_grid_search(params={f\"{this_model_name}__max_depth\":[5,10,25,50],\n",
    "                                   f\"{this_model_name}__min_samples_split\":[10,25,50,100]\n",
    "                                  },\n",
    "                           n_job = 1\n",
    "                          )\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "dtc_v1 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5572cb51-f407-4d26-9609-ffb9c3a618ac",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment:</b></font>\n",
    "\n",
    "- Let's try to apply oversampling to see if it works with Decision Tree.\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca4864a-00f3-4149-b519-fdd2abb92202",
   "metadata": {},
   "source": [
    "#### Decision Tree + Oversampling + Grid Search 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2bb995cf-05e2-4162-aa6f-e3e7c008dabb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-09 20:43:14.f - INFO >>> =============== Data Shape ===============\n",
      "2023-11-09 20:43:14.f - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-09 20:43:14.f - INFO >>> X_test : (588151, 22) | y_test : (588151,)\n",
      "2023-11-09 20:43:14.f - INFO >>> \n",
      "2023-11-09 20:43:16.f - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 01 seconds\n",
      "2023-11-09 20:43:16.f - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-09 20:43:16.f - INFO >>> X_train: (352890, 21) | y_train: (352890,)\n",
      "2023-11-09 20:43:16.f - INFO >>> X_test : (588151, 22) | y_test : (588151,)\n",
      "2023-11-09 20:43:16.f - INFO >>> \n",
      "2023-11-09 20:43:16.f - INFO >>> Oversampling on Train Data...\n",
      "2023-11-09 20:43:16.f - INFO >>> Oversampling Completed | Time elapsed: \t0 hours 00 minutes 00 seconds\n",
      "2023-11-09 20:43:16.f - INFO >>> =============== After Oversampling ===============\n",
      "2023-11-09 20:43:16.f - INFO >>> X_train: (779665, 21) | y_train: (779665,)\n",
      "2023-11-09 20:43:16.f - INFO >>> X_test : (588151, 22) | y_test : (588151,)\n",
      "2023-11-09 20:43:16.f - INFO >>> \n",
      "2023-11-09 20:43:16.f - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-09 20:43:16.f - INFO >>> \n",
      "2023-11-09 20:43:16.f - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-09 20:43:16.f - INFO >>> \n",
      "2023-11-09 20:43:16.f - INFO >>> DecisionTreeClassifier (name: dtc_v2) Initialization Completed\n",
      "2023-11-09 20:43:16.f - INFO >>> \n",
      "2023-11-09 20:43:16.f - INFO >>> Grid Search with 5-folds cross-validation Initialized Completed\n",
      "2023-11-09 20:43:16.f - INFO >>> \n",
      "2023-11-09 20:43:16.f - INFO >>> Model Fitting with Grid Search...\n",
      "2023-11-09 20:49:38.f - INFO >>> Grid Search Cross Validationn Results Exported.\n",
      "2023-11-09 20:49:38.f - INFO >>> Total Fitting Time:\t0 hours 06 minutes 21 seconds\n",
      "2023-11-09 20:49:38.f - INFO >>> \n",
      "2023-11-09 20:49:38.f - INFO >>> Predicting on Test Data...\n",
      "2023-11-09 20:49:41.f - INFO >>> Total Predicting Time:\t0 hours 00 minutes 03 seconds\n",
      "2023-11-09 20:49:41.f - INFO >>> \n",
      "2023-11-09 20:49:41.f - INFO >>> Best Train Score: 0.91709857689494\n",
      "2023-11-09 20:49:41.f - INFO >>> Test Score: 0.8176719023899912\n",
      "2023-11-09 20:49:41.f - INFO >>> =============== Parameters ===============\n",
      "{'dtc_v2__max_depth': 50, 'dtc_v2__min_samples_split': 10}\n",
      "2023-11-09 20:49:41.f - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-09 20:49:41.f - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     140404      17920        344        105         58\n",
      "actual_2      24413     214045      20618        712        100\n",
      "actual_3        298      15488      88653      11007        431\n",
      "actual_4         62        336       8341      23810       3368\n",
      "actual_5         51        106        411       3425      13645\n",
      "2023-11-09 20:49:41.f - INFO >>> Model Performance CSV is exported\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def model(test = False):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsample_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "    model.init_over_sampling() # Oversampling for class imabalance\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    this_model_name = \"dtc_v2\"\n",
    "    model.init_model(name = this_model_name,\n",
    "                     model = DecisionTreeClassifier())\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "    \n",
    "    # Set K-Fold CV\n",
    "    model.set_kfold_cv(5)\n",
    "    \n",
    "    # Instantiate the grid search\n",
    "    model.init_grid_search(params={f\"{this_model_name}__max_depth\":[5,10,25,50],\n",
    "                                   f\"{this_model_name}__min_samples_split\":[10,25,50,100]\n",
    "                                  },\n",
    "                           n_job = 1\n",
    "                          )\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "dtc_v2 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f831e5-8566-41f0-9fbd-e39921019fac",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment:</b></font>\n",
    "\n",
    "- It seems oversampling does not improve the performance when working with Decision Tree.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04133db0-98a1-489e-8192-6da2c54c703c",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üß† Idea:</b></font>\n",
    "\n",
    "What about if we try to use PCA to selct the perform feature selection?\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225579b2-a7e9-4c58-a91a-fd6726940f1f",
   "metadata": {},
   "source": [
    "#### Decision Tree + PCA + Grid Search 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1bc6e54d-35ac-4201-8e33-eb8d31027536",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-09 20:49:41.f - INFO >>> =============== Data Shape ===============\n",
      "2023-11-09 20:49:41.f - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-09 20:49:41.f - INFO >>> X_test : (588151, 22) | y_test : (588151,)\n",
      "2023-11-09 20:49:41.f - INFO >>> \n",
      "2023-11-09 20:49:43.f - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 01 seconds\n",
      "2023-11-09 20:49:43.f - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-09 20:49:43.f - INFO >>> X_train: (352890, 21) | y_train: (352890,)\n",
      "2023-11-09 20:49:43.f - INFO >>> X_test : (588151, 22) | y_test : (588151,)\n",
      "2023-11-09 20:49:43.f - INFO >>> \n",
      "2023-11-09 20:49:43.f - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-09 20:49:43.f - INFO >>> \n",
      "2023-11-09 20:49:43.f - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-09 20:49:43.f - INFO >>> \n",
      "2023-11-09 20:49:43.f - INFO >>> PCA Initialization Completed\n",
      "2023-11-09 20:49:43.f - INFO >>> \n",
      "2023-11-09 20:49:43.f - INFO >>> DecisionTreeClassifier (name: dtc_v3) Initialization Completed\n",
      "2023-11-09 20:49:43.f - INFO >>> \n",
      "2023-11-09 20:49:43.f - INFO >>> Grid Search with 5-folds cross-validation Initialized Completed\n",
      "2023-11-09 20:49:43.f - INFO >>> \n",
      "2023-11-09 20:49:43.f - INFO >>> Model Fitting with Grid Search...\n",
      "2023-11-09 20:57:40.f - INFO >>> Grid Search Cross Validationn Results Exported.\n",
      "2023-11-09 20:57:40.f - INFO >>> Total Fitting Time:\t0 hours 07 minutes 56 seconds\n",
      "2023-11-09 20:57:40.f - INFO >>> \n",
      "2023-11-09 20:57:40.f - INFO >>> Predicting on Test Data...\n",
      "2023-11-09 20:57:44.f - INFO >>> Total Predicting Time:\t0 hours 00 minutes 03 seconds\n",
      "2023-11-09 20:57:44.f - INFO >>> \n",
      "2023-11-09 20:57:44.f - INFO >>> Best Train Score: 0.8155643651788885\n",
      "2023-11-09 20:57:44.f - INFO >>> Test Score: 0.8205435603893648\n",
      "2023-11-09 20:57:44.f - INFO >>> =============== Parameters ===============\n",
      "{'dtc_v3__max_depth': 25,\n",
      " 'dtc_v3__min_samples_split': 50,\n",
      " 'pca__n_components': 0.95}\n",
      "2023-11-09 20:57:44.f - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-09 20:57:44.f - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     137421      21139        228         20         23\n",
      "actual_2      20346     222045      17180        236         81\n",
      "actual_3        378      19094      89013       6927        465\n",
      "actual_4         73        662      10608      21762       2812\n",
      "actual_5         67        222       1030       3625      12694\n",
      "2023-11-09 20:57:44.f - INFO >>> Model Performance CSV is exported\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def model(test = False):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsample_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # # Instantiate PCA step\n",
    "    pca_name = \"pca\"\n",
    "    model.init_pca(name=pca_name)\n",
    "\n",
    "    # Instantiate model\n",
    "    model_name = \"dtc_v3\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = DecisionTreeClassifier())\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Set K-Fold CV\n",
    "    model.set_kfold_cv(5)\n",
    "    \n",
    "    # Instantiate the grid search\n",
    "    model.init_grid_search(params={f\"{model_name}__max_depth\":[5,10,25,50],\n",
    "                                   f\"{model_name}__min_samples_split\":[5,10,25,50],\n",
    "                                   f\"{pca_name}__n_components\":[0.9,0.95],\n",
    "                                  },\n",
    "                           n_job = 1\n",
    "                          )\n",
    "    \n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "dtc_v3 = model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ad2c6f-eb51-46aa-845b-45b7a0885aab",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment - Decision Tree</b></font>\n",
    "    \n",
    "- From the above results for Decision Tree, we obtained:\n",
    "|Model|Best Params / Params|training F1|testing F1|\n",
    "|---|---|:---:|:---:|\n",
    "|`Decision Tree` (with 5-fold Grid Search CV)|`{'max_depth': 25, 'min_samples_split': 50}`|`83.57%`|`84.04%`|\n",
    "|`Decision Tree` (with Oversampling & 5-fold Grid Search CV)|`{'max_depth': 50, 'min_samples_split': 10}`|`91.70%`|`81.97%`|\n",
    "|`Decision Tree` (with PCA & 5-fold Grid Search CV)|`{'max_depth': 25, 'min_samples_split': 50, 'n_components': 0.95}`|`81.55%`|`82.05%`|\n",
    "\n",
    "\n",
    "- Recall that our baseline model (Logistic Regression) return weighted f1 score of around 70%, decision tree seems works better with our data.\n",
    "- Although oversampling improve a lot in terms of training score, it does not improve the testing score.\n",
    "- For PCA, it does not helping us to improve our model. Maybe our data is already cleaned that any PCA will only remove useful informaiton from our data.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a90c5a-1f50-42a9-a398-7a0b315099ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-model\">\n",
    "    <h3> Model - Adaptive Boosting</h3>\n",
    "</a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c7407-4510-4f23-9c96-65b468673ce6",
   "metadata": {},
   "source": [
    "#### Adaboost + Grid Search 5-fold CV (Based on the Best Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b3a25a-59da-4b53-a5a3-c518685bcb4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-09 23:34:20.f - INFO >>> =============== Data Shape ===============\n",
      "2023-11-09 23:34:20.f - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-09 23:34:20.f - INFO >>> X_test : (588151, 22) | y_test : (588151,)\n",
      "2023-11-09 23:34:20.f - INFO >>> \n",
      "2023-11-09 23:34:22.f - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 01 seconds\n",
      "2023-11-09 23:34:22.f - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-09 23:34:22.f - INFO >>> X_train: (352890, 21) | y_train: (352890,)\n",
      "2023-11-09 23:34:22.f - INFO >>> X_test : (588151, 22) | y_test : (588151,)\n",
      "2023-11-09 23:34:22.f - INFO >>> \n",
      "2023-11-09 23:34:22.f - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-09 23:34:22.f - INFO >>> \n",
      "2023-11-09 23:34:22.f - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-09 23:34:22.f - INFO >>> \n",
      "2023-11-09 23:34:22.f - INFO >>> AdaBoostClassifier (name: abc_v1) Initialization Completed\n",
      "2023-11-09 23:34:22.f - INFO >>> \n",
      "2023-11-09 23:34:22.f - INFO >>> Grid Search with 5-folds cross-validation Initialized Completed\n",
      "2023-11-09 23:34:22.f - INFO >>> \n",
      "2023-11-09 23:34:22.f - INFO >>> Model Fitting with Grid Search...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def model(test = False):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    # train_subsample_size=global_subsample_pct,\n",
    "                    train_subsample_size=global_subsample_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "    # model.init_over_sampling() # Oversampling for class imabalance\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model\n",
    "    model_name = \"abc_v1\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = AdaBoostClassifier(estimator = DecisionTreeClassifier(max_depth = 25,\n",
    "                                                                                   min_samples_split = 50)\n",
    "                                                )\n",
    "                    )\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Set K-Fold CV\n",
    "    model.set_kfold_cv(5)\n",
    "    \n",
    "    # Instantiate the grid search\n",
    "    model.init_grid_search(params={f\"{model_name}__n_estimators\":[100,150], \n",
    "                                   f\"{model_name}__learning_rate\":[0.95,1,1.05],\n",
    "                                  },\n",
    "                           n_job = 1)\n",
    "    \n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "abc_v1 = model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b740c72-58ae-40f8-bf6b-89c9bf013a46",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment - AdabBoost</b></font>\n",
    "\n",
    "TODO: Edit Score  \n",
    "TODO:Comment\n",
    "    \n",
    "- From the above results for Adaptive Boosting, we obtained:\n",
    "|Model|Best Params / Params|training F1|testing F1|\n",
    "|---|---|:---:|:---:|\n",
    "|`Adaboost` (with 5-fold Grid Search CV)|`__`|`__%`|`__%`|\n",
    "\n",
    "    \n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7059b8aa-383e-46ee-b695-4732082129d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-model\">\n",
    "    <h3> Model - Stochastic Gradient Boosting</h3>\n",
    "</a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4332bd3-e113-42a3-9336-dc0a2127412d",
   "metadata": {},
   "source": [
    "#### Gradient Boosting + Grid Search 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4ec1ca-ee77-4396-8aa8-f81cc6b1fb4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def model(test = False):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsample_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model\n",
    "    model_name = \"gbc_v1\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = GradientBoostingClassifier(max_depth = 25,\n",
    "                                                        min_samples_split = 50,\n",
    "                                                        n_iter_no_change = 3,\n",
    "                                                        subsample = 0.5, # Apply Stochastic Gradient Boosting\n",
    "                                                        verbose= 0))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Set K-Fold CV\n",
    "    model.set_kfold_cv(5)\n",
    "    \n",
    "    # Instantiate the grid search\n",
    "    model.init_grid_search(params={f\"{model_name}__learning_rate\":[0.05,0.1],\n",
    "                                   f\"{model_name}__subsample\":[0.5,0.75,0.9],\n",
    "                                   f\"{model_name}__n_estimators\":[50,100,200],\n",
    "                                  },\n",
    "                           n_job = 1)\n",
    "    \n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "gbc_v1 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30ac495-ee85-4f52-9a3c-15dd8b824ba7",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment - Stochastic Gradient Boosting</b></font>\n",
    "\n",
    "TODO: Edit Score  \n",
    "TODO: Comment\n",
    "    \n",
    "- From the above results for Stochastic Gradient Boosting, we obtained:\n",
    "|Model|Best Params / Params|training F1|testing F1|\n",
    "|---|---|:---:|:---:|\n",
    "|`Stochastic Gradient Boosting` (with 5-fold Grid Search CV)|`__`|`__%`|`__%`|\n",
    "\n",
    "    \n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6347de93-c0d2-4443-9921-4186a5c479ba",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-model\">\n",
    "    <h3> Model - XGBoost </h3>\n",
    "</a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ec0ad-43ad-4333-9bea-98a4ca685522",
   "metadata": {},
   "source": [
    "#### XG Boosting + Grid Search 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f442ec-76b1-49a3-aac5-45a574cdbd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def model(test = False):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsample_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    model_name = \"xgb_v1\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = xgb.XGBClassifier(n_estimators = 100))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Set K-Fold CV\n",
    "    # model.set_kfold_cv(5)\n",
    "    \n",
    "    # Instantiate the grid search\n",
    "    model.init_grid_search(params={f\"{this_model_name}__max_depth\":[5,10,25,50],\n",
    "                                   f\"{this_model_name}__min_samples_split\":[10,25,50,100]\n",
    "                                  },\n",
    "                           n_job = 1)\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "xgb_v1 = model(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2205a58-7e91-4b1d-a953-afe701dfd132",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment - XG Boost</b></font>\n",
    "\n",
    "TODO: Edit Score  \n",
    "TODO: Comment\n",
    "    \n",
    "- From the above results for XG Boost, we obtained:\n",
    "|Model|Best Params / Params|training F1|testing F1|\n",
    "|---|---|:---:|:---:|\n",
    "|`XG Boost` (with 5-fold Grid Search CV)|`__`|`__%`|`__%`|\n",
    "\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec629eb-764b-4ee2-99bb-e5ec7e11ef3c",
   "metadata": {},
   "source": [
    "[Back-to-top](#4-toc)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f11ea57-9ed1-47d5-9e1b-b3395ac4cbc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-evaluate\">\n",
    "    <h2> Model Evaluation </h2>\n",
    "</a>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995515b5-56ea-44df-8fe2-1813e75c8852",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-model\">\n",
    "    <h3> Model Comparison </h3>\n",
    "</a>\n",
    "    \n",
    "TODO: Summarise    \n",
    "\n",
    "- By comparing the best model from different types of model, we obtained:\n",
    "|Model|Best Params|training F1|testing F1|\n",
    "|---|---|:---:|:---:|\n",
    "|`Logistic Regression` (with 5-fold Grid Search CV)|`{'C': 1000}`|`71.78%`|`71.76%`|\n",
    "|`Decision Tree` (with 5-fold Grid Search CV)|`{'max_depth': 25, 'min_samples_split': 50}`|`83.57%`|`84.04%`|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1f8848-b376-4d80-9a8f-e10bae859462",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-model\">\n",
    "    <h3> Model Training Result </h3>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8cfce0eb-78e9-47fc-919f-a8733821f53c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model</th>\n",
       "      <th>sub_sampling</th>\n",
       "      <th>sub_sampling_time</th>\n",
       "      <th>sub_sampling_pct_X_train</th>\n",
       "      <th>sub_sampling_pct_X_test</th>\n",
       "      <th>over_sampling</th>\n",
       "      <th>over_sampling_time</th>\n",
       "      <th>X_train_shape</th>\n",
       "      <th>X_test_shape</th>\n",
       "      <th>target_encoding</th>\n",
       "      <th>target_encoding_col</th>\n",
       "      <th>one_hot_encoding</th>\n",
       "      <th>one_hot_encoding_col</th>\n",
       "      <th>standard_scaling</th>\n",
       "      <th>standard_scaling_col</th>\n",
       "      <th>min_max_scaling</th>\n",
       "      <th>min_max_scaling_col</th>\n",
       "      <th>pca</th>\n",
       "      <th>pca_n_components</th>\n",
       "      <th>scoring</th>\n",
       "      <th>grid_search</th>\n",
       "      <th>best_params</th>\n",
       "      <th>best_train_score</th>\n",
       "      <th>params</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>confusion_matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>abc_v1</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>True</td>\n",
       "      <td>2.548039</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(17644, 21)</td>\n",
       "      <td>(5882, 21)</td>\n",
       "      <td>True</td>\n",
       "      <td>['make', 'model', 'trim', 'body_type']</td>\n",
       "      <td>True</td>\n",
       "      <td>['vehicle_type', 'drivetrain', 'transmission',...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1_weighted</td>\n",
       "      <td>True</td>\n",
       "      <td>{'abc_v1__learning_rate': 0.9, 'abc_v1__n_esti...</td>\n",
       "      <td>0.804023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.801451</td>\n",
       "      <td>[[1377  212    0    0    0]\\n [ 286 2166  147 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dtc_v1</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>True</td>\n",
       "      <td>4.236204</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(17644, 21)</td>\n",
       "      <td>(5882, 21)</td>\n",
       "      <td>True</td>\n",
       "      <td>['make', 'model', 'trim', 'body_type']</td>\n",
       "      <td>True</td>\n",
       "      <td>['vehicle_type', 'drivetrain', 'transmission',...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1_weighted</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'class_weight': None, 'crit...</td>\n",
       "      <td>0.826225</td>\n",
       "      <td>0.778531</td>\n",
       "      <td>[[1335  250    4    0    0]\\n [ 253 2145  196 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dtc_v1</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>True</td>\n",
       "      <td>4.204373</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(17644, 21)</td>\n",
       "      <td>(5882, 21)</td>\n",
       "      <td>True</td>\n",
       "      <td>['make', 'model', 'trim', 'body_type']</td>\n",
       "      <td>True</td>\n",
       "      <td>['vehicle_type', 'drivetrain', 'transmission',...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1_weighted</td>\n",
       "      <td>True</td>\n",
       "      <td>{'dtc_v1__max_depth': 10, 'dtc_v1__min_samples...</td>\n",
       "      <td>0.771995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.776918</td>\n",
       "      <td>[[1341  246    2    0    0]\\n [ 230 2148  217 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dtc_v1</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>True</td>\n",
       "      <td>4.139838</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(17644, 21)</td>\n",
       "      <td>(5882, 21)</td>\n",
       "      <td>True</td>\n",
       "      <td>['make', 'model', 'trim', 'body_type']</td>\n",
       "      <td>True</td>\n",
       "      <td>['vehicle_type', 'drivetrain', 'transmission',...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1_weighted</td>\n",
       "      <td>True</td>\n",
       "      <td>{'dtc_v1__max_depth': 10, 'dtc_v1__min_samples...</td>\n",
       "      <td>0.772048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.776860</td>\n",
       "      <td>[[1331  256    2    0    0]\\n [ 224 2163  205 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dtc_v1</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>True</td>\n",
       "      <td>3.881756</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(17644, 21)</td>\n",
       "      <td>(5882, 21)</td>\n",
       "      <td>True</td>\n",
       "      <td>['make', 'model', 'trim', 'body_type']</td>\n",
       "      <td>True</td>\n",
       "      <td>['vehicle_type', 'drivetrain', 'transmission',...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1_weighted</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'class_weight': None, 'crit...</td>\n",
       "      <td>0.811460</td>\n",
       "      <td>0.773672</td>\n",
       "      <td>[[1337  249    3    0    0]\\n [ 239 2148  208 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dtc_v3</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>True</td>\n",
       "      <td>4.296172</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(17644, 21)</td>\n",
       "      <td>(5882, 21)</td>\n",
       "      <td>True</td>\n",
       "      <td>['make', 'model', 'trim', 'body_type']</td>\n",
       "      <td>True</td>\n",
       "      <td>['vehicle_type', 'drivetrain', 'transmission',...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1_weighted</td>\n",
       "      <td>True</td>\n",
       "      <td>{'dtc_v3__max_depth': 10, 'dtc_v3__min_samples...</td>\n",
       "      <td>0.765591</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.770113</td>\n",
       "      <td>[[1290  295    3    0    1]\\n [ 212 2180  202 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dtc_v4</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>True</td>\n",
       "      <td>4.219699</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(17644, 21)</td>\n",
       "      <td>(5882, 21)</td>\n",
       "      <td>True</td>\n",
       "      <td>['make', 'model', 'trim', 'body_type']</td>\n",
       "      <td>True</td>\n",
       "      <td>['vehicle_type', 'drivetrain', 'transmission',...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1_weighted</td>\n",
       "      <td>True</td>\n",
       "      <td>{'dtc_v4__max_depth': 15, 'dtc_v4__min_samples...</td>\n",
       "      <td>0.758576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.765461</td>\n",
       "      <td>[[1312  275    2    0    0]\\n [ 267 2128  198 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dtc_v2</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>True</td>\n",
       "      <td>4.146628</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.169100</td>\n",
       "      <td>(38980, 21)</td>\n",
       "      <td>(12995, 21)</td>\n",
       "      <td>True</td>\n",
       "      <td>['make', 'model', 'trim', 'body_type']</td>\n",
       "      <td>True</td>\n",
       "      <td>['vehicle_type', 'drivetrain', 'transmission',...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1_weighted</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'class_weight': None, 'crit...</td>\n",
       "      <td>0.821929</td>\n",
       "      <td>0.730786</td>\n",
       "      <td>[[2266  319    2   10    2]\\n [ 326 1928  306 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dtc_v2</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>True</td>\n",
       "      <td>4.236193</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.221858</td>\n",
       "      <td>(38980, 21)</td>\n",
       "      <td>(12995, 21)</td>\n",
       "      <td>True</td>\n",
       "      <td>['make', 'model', 'trim', 'body_type']</td>\n",
       "      <td>True</td>\n",
       "      <td>['vehicle_type', 'drivetrain', 'transmission',...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1_weighted</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'class_weight': None, 'crit...</td>\n",
       "      <td>0.879233</td>\n",
       "      <td>0.716833</td>\n",
       "      <td>[[2254  326    9   10    0]\\n [ 328 1904  319 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logit_v1</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>True</td>\n",
       "      <td>4.579603</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(17644, 21)</td>\n",
       "      <td>(5882, 21)</td>\n",
       "      <td>True</td>\n",
       "      <td>['make', 'model', 'trim', 'body_type']</td>\n",
       "      <td>True</td>\n",
       "      <td>['vehicle_type', 'drivetrain', 'transmission',...</td>\n",
       "      <td>True</td>\n",
       "      <td>['make', 'model', 'trim', 'body_type']</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1_weighted</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'C': 1, 'class_weight': None, 'dual': False, ...</td>\n",
       "      <td>0.723986</td>\n",
       "      <td>0.715027</td>\n",
       "      <td>[[1255  286   34    7    7]\\n [ 274 2085  210 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dtc_v2</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>True</td>\n",
       "      <td>4.706741</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.193771</td>\n",
       "      <td>(38980, 21)</td>\n",
       "      <td>(12995, 21)</td>\n",
       "      <td>True</td>\n",
       "      <td>['make', 'model', 'trim', 'body_type']</td>\n",
       "      <td>True</td>\n",
       "      <td>['vehicle_type', 'drivetrain', 'transmission',...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1_weighted</td>\n",
       "      <td>True</td>\n",
       "      <td>{'dtc_v2__max_depth': 50, 'dtc_v2__min_samples...</td>\n",
       "      <td>0.896392</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.710412</td>\n",
       "      <td>[[2222  367    4    5    1]\\n [ 333 1980  265 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logit_v3</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>True</td>\n",
       "      <td>4.420159</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(17644, 21)</td>\n",
       "      <td>(5882, 21)</td>\n",
       "      <td>True</td>\n",
       "      <td>['make', 'model', 'trim', 'body_type']</td>\n",
       "      <td>True</td>\n",
       "      <td>['vehicle_type', 'drivetrain', 'transmission',...</td>\n",
       "      <td>True</td>\n",
       "      <td>['make', 'model', 'trim', 'body_type']</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1_weighted</td>\n",
       "      <td>True</td>\n",
       "      <td>{'logit_v3__C': 10}</td>\n",
       "      <td>0.708054</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.710200</td>\n",
       "      <td>[[1248  291   37    6    7]\\n [ 278 2074  217 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dtc_v3</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>True</td>\n",
       "      <td>4.435941</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.174061</td>\n",
       "      <td>(38980, 21)</td>\n",
       "      <td>(12995, 21)</td>\n",
       "      <td>True</td>\n",
       "      <td>['make', 'model', 'trim', 'body_type']</td>\n",
       "      <td>True</td>\n",
       "      <td>['vehicle_type', 'drivetrain', 'transmission',...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1_weighted</td>\n",
       "      <td>True</td>\n",
       "      <td>{'dtc_v3__max_depth': 50, 'dtc_v3__min_samples...</td>\n",
       "      <td>0.868331</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.706340</td>\n",
       "      <td>[[2240  342    9    5    3]\\n [ 334 1950  279 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dtc_v2</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>True</td>\n",
       "      <td>4.573992</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.148800</td>\n",
       "      <td>(38980, 21)</td>\n",
       "      <td>(12995, 21)</td>\n",
       "      <td>True</td>\n",
       "      <td>['make', 'model', 'trim', 'body_type']</td>\n",
       "      <td>True</td>\n",
       "      <td>['vehicle_type', 'drivetrain', 'transmission',...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1_weighted</td>\n",
       "      <td>True</td>\n",
       "      <td>{'dtc_v2__max_depth': 50, 'dtc_v2__min_samples...</td>\n",
       "      <td>0.896867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.704925</td>\n",
       "      <td>[[2216  371    8    3    1]\\n [ 344 1968  265 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit_v2</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>True</td>\n",
       "      <td>4.148163</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.167082</td>\n",
       "      <td>(38980, 21)</td>\n",
       "      <td>(12995, 21)</td>\n",
       "      <td>True</td>\n",
       "      <td>['make', 'model', 'trim', 'body_type']</td>\n",
       "      <td>True</td>\n",
       "      <td>['vehicle_type', 'drivetrain', 'transmission',...</td>\n",
       "      <td>True</td>\n",
       "      <td>['make', 'model', 'trim', 'body_type']</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1_weighted</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'C': 1, 'class_weight': None, 'dual': False, ...</td>\n",
       "      <td>0.701592</td>\n",
       "      <td>0.670253</td>\n",
       "      <td>[[2192  286   70   30   21]\\n [ 506 1619  386 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dtc_v4</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>True</td>\n",
       "      <td>4.597316</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>0.161828</td>\n",
       "      <td>(38980, 21)</td>\n",
       "      <td>(12995, 21)</td>\n",
       "      <td>True</td>\n",
       "      <td>['make', 'model', 'trim', 'body_type']</td>\n",
       "      <td>True</td>\n",
       "      <td>['vehicle_type', 'drivetrain', 'transmission',...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f1_weighted</td>\n",
       "      <td>True</td>\n",
       "      <td>{'dtc_v4__max_depth': 25, 'dtc_v4__min_samples...</td>\n",
       "      <td>0.861559</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.669727</td>\n",
       "      <td>[[2185  392   13    4    5]\\n [ 362 1933  277 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_name                   model  sub_sampling  sub_sampling_time  \\\n",
       "15     abc_v1      AdaBoostClassifier          True           2.548039   \n",
       "3      dtc_v1  DecisionTreeClassifier          True           4.236204   \n",
       "11     dtc_v1  DecisionTreeClassifier          True           4.204373   \n",
       "9      dtc_v1  DecisionTreeClassifier          True           4.139838   \n",
       "7      dtc_v1  DecisionTreeClassifier          True           3.881756   \n",
       "14     dtc_v3  DecisionTreeClassifier          True           4.296172   \n",
       "6      dtc_v4  DecisionTreeClassifier          True           4.219699   \n",
       "8      dtc_v2  DecisionTreeClassifier          True           4.146628   \n",
       "4      dtc_v2  DecisionTreeClassifier          True           4.236193   \n",
       "0    logit_v1      LogisticRegression          True           4.579603   \n",
       "12     dtc_v2  DecisionTreeClassifier          True           4.706741   \n",
       "2    logit_v3      LogisticRegression          True           4.420159   \n",
       "5      dtc_v3  DecisionTreeClassifier          True           4.435941   \n",
       "10     dtc_v2  DecisionTreeClassifier          True           4.573992   \n",
       "1    logit_v2      LogisticRegression          True           4.148163   \n",
       "13     dtc_v4  DecisionTreeClassifier          True           4.597316   \n",
       "\n",
       "    sub_sampling_pct_X_train  sub_sampling_pct_X_test  over_sampling  \\\n",
       "15                      0.01                     0.01          False   \n",
       "3                       0.01                     0.01          False   \n",
       "11                      0.01                     0.01          False   \n",
       "9                       0.01                     0.01          False   \n",
       "7                       0.01                     0.01          False   \n",
       "14                      0.01                     0.01          False   \n",
       "6                       0.01                     0.01          False   \n",
       "8                       0.01                     0.01           True   \n",
       "4                       0.01                     0.01           True   \n",
       "0                       0.01                     0.01          False   \n",
       "12                      0.01                     0.01           True   \n",
       "2                       0.01                     0.01          False   \n",
       "5                       0.01                     0.01           True   \n",
       "10                      0.01                     0.01           True   \n",
       "1                       0.01                     0.01           True   \n",
       "13                      0.01                     0.01           True   \n",
       "\n",
       "    over_sampling_time X_train_shape X_test_shape  target_encoding  \\\n",
       "15                 NaN   (17644, 21)   (5882, 21)             True   \n",
       "3                  NaN   (17644, 21)   (5882, 21)             True   \n",
       "11                 NaN   (17644, 21)   (5882, 21)             True   \n",
       "9                  NaN   (17644, 21)   (5882, 21)             True   \n",
       "7                  NaN   (17644, 21)   (5882, 21)             True   \n",
       "14                 NaN   (17644, 21)   (5882, 21)             True   \n",
       "6                  NaN   (17644, 21)   (5882, 21)             True   \n",
       "8             0.169100   (38980, 21)  (12995, 21)             True   \n",
       "4             0.221858   (38980, 21)  (12995, 21)             True   \n",
       "0                  NaN   (17644, 21)   (5882, 21)             True   \n",
       "12            0.193771   (38980, 21)  (12995, 21)             True   \n",
       "2                  NaN   (17644, 21)   (5882, 21)             True   \n",
       "5             0.174061   (38980, 21)  (12995, 21)             True   \n",
       "10            0.148800   (38980, 21)  (12995, 21)             True   \n",
       "1             0.167082   (38980, 21)  (12995, 21)             True   \n",
       "13            0.161828   (38980, 21)  (12995, 21)             True   \n",
       "\n",
       "                       target_encoding_col  one_hot_encoding  \\\n",
       "15  ['make', 'model', 'trim', 'body_type']              True   \n",
       "3   ['make', 'model', 'trim', 'body_type']              True   \n",
       "11  ['make', 'model', 'trim', 'body_type']              True   \n",
       "9   ['make', 'model', 'trim', 'body_type']              True   \n",
       "7   ['make', 'model', 'trim', 'body_type']              True   \n",
       "14  ['make', 'model', 'trim', 'body_type']              True   \n",
       "6   ['make', 'model', 'trim', 'body_type']              True   \n",
       "8   ['make', 'model', 'trim', 'body_type']              True   \n",
       "4   ['make', 'model', 'trim', 'body_type']              True   \n",
       "0   ['make', 'model', 'trim', 'body_type']              True   \n",
       "12  ['make', 'model', 'trim', 'body_type']              True   \n",
       "2   ['make', 'model', 'trim', 'body_type']              True   \n",
       "5   ['make', 'model', 'trim', 'body_type']              True   \n",
       "10  ['make', 'model', 'trim', 'body_type']              True   \n",
       "1   ['make', 'model', 'trim', 'body_type']              True   \n",
       "13  ['make', 'model', 'trim', 'body_type']              True   \n",
       "\n",
       "                                 one_hot_encoding_col  standard_scaling  \\\n",
       "15  ['vehicle_type', 'drivetrain', 'transmission',...             False   \n",
       "3   ['vehicle_type', 'drivetrain', 'transmission',...             False   \n",
       "11  ['vehicle_type', 'drivetrain', 'transmission',...             False   \n",
       "9   ['vehicle_type', 'drivetrain', 'transmission',...             False   \n",
       "7   ['vehicle_type', 'drivetrain', 'transmission',...             False   \n",
       "14  ['vehicle_type', 'drivetrain', 'transmission',...             False   \n",
       "6   ['vehicle_type', 'drivetrain', 'transmission',...             False   \n",
       "8   ['vehicle_type', 'drivetrain', 'transmission',...             False   \n",
       "4   ['vehicle_type', 'drivetrain', 'transmission',...             False   \n",
       "0   ['vehicle_type', 'drivetrain', 'transmission',...              True   \n",
       "12  ['vehicle_type', 'drivetrain', 'transmission',...             False   \n",
       "2   ['vehicle_type', 'drivetrain', 'transmission',...              True   \n",
       "5   ['vehicle_type', 'drivetrain', 'transmission',...             False   \n",
       "10  ['vehicle_type', 'drivetrain', 'transmission',...             False   \n",
       "1   ['vehicle_type', 'drivetrain', 'transmission',...              True   \n",
       "13  ['vehicle_type', 'drivetrain', 'transmission',...             False   \n",
       "\n",
       "                      standard_scaling_col  min_max_scaling  \\\n",
       "15                                      []            False   \n",
       "3                                       []            False   \n",
       "11                                      []            False   \n",
       "9                                       []            False   \n",
       "7                                       []            False   \n",
       "14                                      []            False   \n",
       "6                                       []            False   \n",
       "8                                       []            False   \n",
       "4                                       []            False   \n",
       "0   ['make', 'model', 'trim', 'body_type']            False   \n",
       "12                                      []            False   \n",
       "2   ['make', 'model', 'trim', 'body_type']            False   \n",
       "5                                       []            False   \n",
       "10                                      []            False   \n",
       "1   ['make', 'model', 'trim', 'body_type']            False   \n",
       "13                                      []            False   \n",
       "\n",
       "   min_max_scaling_col    pca  pca_n_components      scoring  grid_search  \\\n",
       "15                  []  False               NaN  f1_weighted         True   \n",
       "3                   []  False               NaN  f1_weighted        False   \n",
       "11                  []  False               NaN  f1_weighted         True   \n",
       "9                   []  False               NaN  f1_weighted         True   \n",
       "7                   []  False               NaN  f1_weighted        False   \n",
       "14                  []   True               NaN  f1_weighted         True   \n",
       "6                   []   True               NaN  f1_weighted         True   \n",
       "8                   []  False               NaN  f1_weighted        False   \n",
       "4                   []  False               NaN  f1_weighted        False   \n",
       "0                   []  False               NaN  f1_weighted        False   \n",
       "12                  []  False               NaN  f1_weighted         True   \n",
       "2                   []  False               NaN  f1_weighted         True   \n",
       "5                   []  False               NaN  f1_weighted         True   \n",
       "10                  []  False               NaN  f1_weighted         True   \n",
       "1                   []  False               NaN  f1_weighted        False   \n",
       "13                  []   True               NaN  f1_weighted         True   \n",
       "\n",
       "                                          best_params  best_train_score  \\\n",
       "15  {'abc_v1__learning_rate': 0.9, 'abc_v1__n_esti...          0.804023   \n",
       "3                                                 NaN               NaN   \n",
       "11  {'dtc_v1__max_depth': 10, 'dtc_v1__min_samples...          0.771995   \n",
       "9   {'dtc_v1__max_depth': 10, 'dtc_v1__min_samples...          0.772048   \n",
       "7                                                 NaN               NaN   \n",
       "14  {'dtc_v3__max_depth': 10, 'dtc_v3__min_samples...          0.765591   \n",
       "6   {'dtc_v4__max_depth': 15, 'dtc_v4__min_samples...          0.758576   \n",
       "8                                                 NaN               NaN   \n",
       "4                                                 NaN               NaN   \n",
       "0                                                 NaN               NaN   \n",
       "12  {'dtc_v2__max_depth': 50, 'dtc_v2__min_samples...          0.896392   \n",
       "2                                 {'logit_v3__C': 10}          0.708054   \n",
       "5   {'dtc_v3__max_depth': 50, 'dtc_v3__min_samples...          0.868331   \n",
       "10  {'dtc_v2__max_depth': 50, 'dtc_v2__min_samples...          0.896867   \n",
       "1                                                 NaN               NaN   \n",
       "13  {'dtc_v4__max_depth': 25, 'dtc_v4__min_samples...          0.861559   \n",
       "\n",
       "                                               params  train_score  \\\n",
       "15                                                NaN          NaN   \n",
       "3   {'ccp_alpha': 0.0, 'class_weight': None, 'crit...     0.826225   \n",
       "11                                                NaN          NaN   \n",
       "9                                                 NaN          NaN   \n",
       "7   {'ccp_alpha': 0.0, 'class_weight': None, 'crit...     0.811460   \n",
       "14                                                NaN          NaN   \n",
       "6                                                 NaN          NaN   \n",
       "8   {'ccp_alpha': 0.0, 'class_weight': None, 'crit...     0.821929   \n",
       "4   {'ccp_alpha': 0.0, 'class_weight': None, 'crit...     0.879233   \n",
       "0   {'C': 1, 'class_weight': None, 'dual': False, ...     0.723986   \n",
       "12                                                NaN          NaN   \n",
       "2                                                 NaN          NaN   \n",
       "5                                                 NaN          NaN   \n",
       "10                                                NaN          NaN   \n",
       "1   {'C': 1, 'class_weight': None, 'dual': False, ...     0.701592   \n",
       "13                                                NaN          NaN   \n",
       "\n",
       "    test_score                                   confusion_matrix  \n",
       "15    0.801451  [[1377  212    0    0    0]\\n [ 286 2166  147 ...  \n",
       "3     0.778531  [[1335  250    4    0    0]\\n [ 253 2145  196 ...  \n",
       "11    0.776918  [[1341  246    2    0    0]\\n [ 230 2148  217 ...  \n",
       "9     0.776860  [[1331  256    2    0    0]\\n [ 224 2163  205 ...  \n",
       "7     0.773672  [[1337  249    3    0    0]\\n [ 239 2148  208 ...  \n",
       "14    0.770113  [[1290  295    3    0    1]\\n [ 212 2180  202 ...  \n",
       "6     0.765461  [[1312  275    2    0    0]\\n [ 267 2128  198 ...  \n",
       "8     0.730786  [[2266  319    2   10    2]\\n [ 326 1928  306 ...  \n",
       "4     0.716833  [[2254  326    9   10    0]\\n [ 328 1904  319 ...  \n",
       "0     0.715027  [[1255  286   34    7    7]\\n [ 274 2085  210 ...  \n",
       "12    0.710412  [[2222  367    4    5    1]\\n [ 333 1980  265 ...  \n",
       "2     0.710200  [[1248  291   37    6    7]\\n [ 278 2074  217 ...  \n",
       "5     0.706340  [[2240  342    9    5    3]\\n [ 334 1950  279 ...  \n",
       "10    0.704925  [[2216  371    8    3    1]\\n [ 344 1968  265 ...  \n",
       "1     0.670253  [[2192  286   70   30   21]\\n [ 506 1619  386 ...  \n",
       "13    0.669727  [[2185  392   13    4    5]\\n [ 362 1933  277 ...  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = ModelPerf(\"log/model_performance.csv\")\n",
    "temp = p.get_data()#[[\"model\",\"test_score\",\"standard_scaling\",\"pca\",\"grid_search\",\"best_params\",\"best_train_score\",\"params\",\"train_score\"]]\n",
    "temp.sort_values(by=[\"test_score\"],ascending = [False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d027e629-128b-42ef-93d7-819b0a1dfa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: COmment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48867037-dac2-42ad-b7c6-4a573214b7e3",
   "metadata": {},
   "source": [
    "[Back-to-top](#4-toc)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998801db-8731-4623-8eb1-342b5a0a5ccf",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-learn\">\n",
    "    <h2> Learning / Takeaway </h2>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35b1ec0-1dfa-4565-b760-335ac4d9549f",
   "metadata": {},
   "source": [
    "[Back-to-top](#4-toc)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
