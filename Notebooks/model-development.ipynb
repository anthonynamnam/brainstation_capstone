{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf855c90-4d76-4b34-95a8-5939266fa0f0",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "\n",
    "<h1> Used Car Listing Price Prediction</h1>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0e54d7-2b98-43e7-8bf8-b521cdbe02f7",
   "metadata": {},
   "source": [
    "<center><img src=\"https://raw.githubusercontent.com/anthonynamnam/anthonynamnam/main/icons/image/car-banner.png\" alt=\"memes\" width=\"600\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a66a4-04c3-4292-afa6-dd4699957835",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d070dc70-c8d9-4361-b779-7312490b4898",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "\n",
    "<h2> Project Overview</h2>\n",
    "    \n",
    "Please kindly refer to the github repo of this project: <a href=\"https://github.com/anthonynamnam/brainstation_capstone#project-overview\">Link</a>\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79217f3-ccad-4024-9740-8d01803abf06",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddfcfdc-5aa3-42f1-a2d6-1d239b0adbe7",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "\n",
    "   <h2> Notebook Overview</h2>\n",
    "    \n",
    "Now that we have a well-prepared dataset in hand, our next steps involve handling categorical variables, scaling features, addressing class imbalances, and ultimately building and evaluating predictive models. Each of these steps plays a pivotal role in the success of our machine learning and deep learning endeavors. In this notebook, we will guide you through the following steps of working with data:\n",
    "    \n",
    "<ol>\n",
    "    <font size=3><li><b>Categorical Data Encoding üé≤</b></li></font>\n",
    "    <p>Many machine learning algorithms require numerical inputs, necessitating the transformation of categorical variables into a format suitable for analysis. In this notebook, we'll explore various encoding techniques to convert categorical data into a numerical representation that our models can comprehend</p>\n",
    "    <font size=3><li><b>Data Scaling üìê</b></li></font>\n",
    "    <p>Ensuring that all features are on a consistent scale is crucial for the performance of many machine learning algorithms. We'll delve into the importance of data scaling and demonstrate methods to standardize or normalize our features.</p>\n",
    "    <font size=3><li><b>Class Imbalance ‚öñÔ∏è</b></li></font>\n",
    "    <p>Real-world datasets often exhibit imbalances in class distribution, where certain outcomes are more prevalent than others. We'll explore techniques to address class imbalances, ensuring that our models are trained to recognize patterns effectively.</p>\n",
    "    <font size=3><li><b>ML/DL Modeling üß†</b></li></font>\n",
    "    <p>The heart of our predictive analytics journey lies in building machine learning and deep learning models. We'll guide you through the process of selecting, training, and fine-tuning models that best suit the nature of our data and the goals of our project.</p>\n",
    "    <font size=3><li><b>Model Evaluation üßÆ</b></li></font>\n",
    "    <p>As we generate predictions, it becomes imperative to assess the performance of our models. We'll introduce metrics and techniques for evaluating model accuracy, precision, recall, and other key indicators to ensure that our models meet the desired standards.</p>\n",
    "\n",
    "</ol>\n",
    "Through this notebook, we aim to equip you with the knowledge and tools needed to navigate the intricacies of turning prepared data into actionable insights. Let's harness the power of machine learning and deep learning to uncover patterns, make predictions, and elevate the impact of our project.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba66112-916f-4c30-bc40-621bf91a6df4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef79b76-49ae-46c2-ad29-c8399568f360",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "\n",
    "<a class=\"anchor\" id=\"4-toc\"> \n",
    "    <h2> Table of Contents </h2>\n",
    "</a>\n",
    "    \n",
    "<ul>    \n",
    "    <li> <a href=\"#4-setup\">Notebook Set Up</a></li>\n",
    "    <li> <a href=\"#4-func\">Functions</a></li>\n",
    "    <li> <a href=\"#4-load\">Data Loading</a></li>\n",
    "    <li> <a href=\"#4-cat-encode\">Categorical Enconding</a></li>\n",
    "    <li> <a href=\"#4-scale\">Data Scaling</a></li>\n",
    "    <li> <a href=\"#4-imbalance\">Class Imbalance</a></li>\n",
    "    <li> <a href=\"#4-models\">Proposed Models</a></li>\n",
    "    <li> <a href=\"#4-pipelines\">Model Pipelines</a></li>\n",
    "    <li> <a href=\"#4-evaluate\">Model Evaluation</a></li>\n",
    "<!--     <li> <a href=\"#4-learn\">Learning/Takeaway</a></li> -->\n",
    "</ul>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d2e083-9878-41a2-8ff7-0982a4fb4474",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c266769e-8d30-469b-bfc3-c4370861623f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-setup\">\n",
    "    <h2> Set Up </h2>\n",
    "</a>\n",
    "<b>Table of Content:</b>\n",
    "<ul>    \n",
    "    <li> <a href=\"#4-import\">Import Library</a></li>\n",
    "    <li> <a href=\"#4-const\">Global Const</a></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22209042-1912-4268-8023-cde5532d725a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-import\">\n",
    "<h3> Import Library </h3>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a6f9d6e7-093b-45b5-a559-9322d5662f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "# Data Science Package\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce72bea-978e-4b2a-87e8-25a7a4021069",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-const\">\n",
    "<h3> Global Constant </h3>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6b383772-a04f-4b08-b558-2c4a89834c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "ran = random.Random()\n",
    "ran.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "258c92ec-efdb-419d-b445-00bd6c312d5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s >>> %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    handlers=[\n",
    "                        logging.FileHandler(filename='log/modelling.log'),\n",
    "                        logging.StreamHandler(sys.stdout)\n",
    "                    ])\n",
    "logger = logging.getLogger('LOGGER_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469bf518-ab10-45f8-93a7-6330f712e90e",
   "metadata": {},
   "source": [
    "[Back-to-top](#4-toc)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72c40b8-a43c-440d-8806-db04df101abc",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-func\">\n",
    "    <h2> Functions </h2>\n",
    "</a>\n",
    "<b>Table of Content:</b>\n",
    "<ul>    \n",
    "    <li> <a href=\"#4-func-print\">Helper Funcntions (Print Info)</a></li>\n",
    "    <li> <a href=\"#4-func-edit\">Helper Funcntions (Edit Dataframe)</a></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cef8a9-c5c5-497a-b0c3-679ca8e3d3a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-func-print\">\n",
    "<h3> Helper Funcntions (Print Info) </h3>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "73c0e3e2-615b-465a-bf2a-45fb74ba7efb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper Functions to print df info and statement\n",
    "import pandas as pd\n",
    "\n",
    "def print_num_row(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----\n",
    "    Retrieve the number of rows of dataframe and print it as a statement.\n",
    "    \n",
    "    Args\n",
    "    -----\n",
    "    df (pd.DataFrame): the target dataframe\n",
    "    \n",
    "    Returns\n",
    "    -----\n",
    "    None\n",
    "    \n",
    "    Example\n",
    "    -----\n",
    "    df = pd.DataFrame(data = {\"height\":[147,190],\"weight\":[47,72],\"age\":[12,28]},index = [0,1])\n",
    "    print_num_row(df)  =>\n",
    "        |\n",
    "        | \"The dataframe has 2 rows of record now.\"\n",
    "        |\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"The dataframe has {df.shape[0]} rows of record now.\")\n",
    "    return\n",
    "    \n",
    "\n",
    "def print_num_col(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----\n",
    "    Retrieve the number of columns of dataframe and print it as a statement.\n",
    "    \n",
    "    Args\n",
    "    -----\n",
    "    df (pd.DataFrame): the target dataframe\n",
    "    \n",
    "    Returns\n",
    "    -----\n",
    "    None\n",
    "    \n",
    "    Example\n",
    "    -----\n",
    "    df = pd.DataFrame(data = {\"height\":[147,190],\"weight\":[47,72],\"age\":[12,28]},index = [0,1])\n",
    "    print_num_col(df) => \n",
    "        |\n",
    "        | \"The dataframe has 3 columns now.\"\n",
    "        |\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"The dataframe has {df.shape[1]} columns now.\")\n",
    "    return\n",
    "        \n",
    "def print_dim(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----\n",
    "    Retrieve the shape of dataframe and print it as a statement.\n",
    "    \n",
    "    Args\n",
    "    -----\n",
    "    df (pd.DataFrame): the target dataframe\n",
    "    \n",
    "    Returns\n",
    "    -----\n",
    "    None\n",
    "    \n",
    "    Example\n",
    "    -----\n",
    "    abc_df = pd.DataFrame(data = {\"height\":[147,190],\"weight\":[47,72],\"age\":[12,28]},index = [0,1])\n",
    "    print_dim(abc_df) =>\n",
    "        |\n",
    "        | \"There are 2 rows and 3 columns in this dataframe now.\"\n",
    "        |\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"There are {df.shape[0]} rows and {df.shape[1]} columns in this dataframe now.\")\n",
    "    return\n",
    "\n",
    "\n",
    "def print_null_count(df: pd.DataFrame,cols:list = []) -> None:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----\n",
    "    Count the null value in each columns.\n",
    "    If `cols` is provided, only show the null value count for the columns in `cols`. \n",
    "    Otherwise, show null value count for all columns.\n",
    "    \n",
    "    Args\n",
    "    -----\n",
    "    df (pd.DataFrame): target dataframe\n",
    "    cols (list): the column names to show the null value count. Default: []\n",
    "    \n",
    "    Returns\n",
    "    -----\n",
    "    None\n",
    "    \n",
    "    Example\n",
    "    -----\n",
    "    abc_df = pd.DataFrame(data = {\"height\":[147,190],\"weight\":[47,np.nan],\"age\":[np.nan,28]},index = [0,1])\n",
    "    print_null_count(abc_df) => \n",
    "        |\n",
    "        | === Null Count ===\n",
    "        | height    0\n",
    "        | weight    1\n",
    "        | age       2\n",
    "        | dtype: int64\n",
    "        |\n",
    "        \n",
    "    print_null_count(abc_df,cols=[\"age\"]) => \n",
    "        |\n",
    "        | === Null Count ===\n",
    "        | Column `age`: 2\n",
    "        |\n",
    "        \n",
    "    print_null_count(abc_df,cols=[\"age\",\"weight\"]) => \n",
    "        |\n",
    "        | === Null Count ===\n",
    "        | Column `age`: 2\n",
    "        | Column `weight`: 1\n",
    "        |\n",
    "    \n",
    "    \"\"\"\n",
    "    if len(cols) == 0:\n",
    "        null_count = df.isnull().sum()\n",
    "        \n",
    "        print(\"=== Null Count ===\")\n",
    "        print(null_count)\n",
    "    else:\n",
    "        assert set(cols).issubset(df.columns)\n",
    "        null_count = df[cols].isnull().sum()\n",
    "        \n",
    "        print(\"=== Null Count ===\")\n",
    "        for col in cols:\n",
    "            print(f\"Column `{col}`: {null_count[col]}\")\n",
    "    return \n",
    "\n",
    "\n",
    "def print_null_pct(df: pd.DataFrame,cols:list = []) -> None:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----\n",
    "    Count the null percentage in each columns.\n",
    "    If `cols` is provided, only show the null percentage for the columns in `cols`. \n",
    "    Otherwise, show null percentage for all columns.\n",
    "    \n",
    "    Args\n",
    "    -----\n",
    "    df (pd.DataFrame): target dataframe\n",
    "    cols (list): the column names to show the null percentage. Default: []\n",
    "    \n",
    "    Returns\n",
    "    -----\n",
    "    None\n",
    "    \n",
    "    Example\n",
    "    -----\n",
    "    abc_df = pd.DataFrame(data = {\"height\":[147,190],\"weight\":[47,np.nan],\"age\":[np.nan,np.nan]},index = [0,1])\n",
    "    print_null_pct(abc_df) => \n",
    "        |\n",
    "        | === Null Count Precentage ===\n",
    "        | height      0.0%\n",
    "        | weight     50.0%\n",
    "        | age       100.0%\n",
    "        | dtype: object\n",
    "        |\n",
    "        \n",
    "    print_null_pct(abc_df,cols=[\"weight\"]) => \n",
    "        |\n",
    "        | === Null Count Precentage ===\n",
    "        | Column weight: 50.0%\n",
    "        |\n",
    "    \n",
    "    \"\"\"\n",
    "    total_row = df.shape[0]\n",
    "    if len(cols) == 0:\n",
    "        null_count = df.isnull().sum()\n",
    "        null_pct = null_count / total_row * 100\n",
    "        \n",
    "        print(\"=== Null Count Precentage ===\")\n",
    "        print(null_pct.round(2).astype(str)+\"%\")\n",
    "    else:\n",
    "        assert set(cols).issubset(df.columns)\n",
    "        null_count = df[cols].isnull().sum()\n",
    "        null_pct = null_count / total_row * 100\n",
    "\n",
    "        print(\"=== Null Count Precentage ===\")\n",
    "        for col in cols:\n",
    "            print(f\"Column {col}: {round(null_pct[col],4)}%\")\n",
    "    return\n",
    "\n",
    "def print_duplicated_count(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----\n",
    "    Count the number of duplicated rows.\n",
    "    \n",
    "    Args\n",
    "    -----\n",
    "    df (pd.DataFrame): target dataframe\n",
    "    \n",
    "    Returns\n",
    "    -----\n",
    "    None\n",
    "    \n",
    "    Example\n",
    "    -----\n",
    "    abc_df = pd.DataFrame(data = {\"height\":[147,190,147],\"weight\":[47,np.nan,47],\"age\":[13,27,13]},index = [0,1,2])\n",
    "    print_duplicated_count(abc_df) => \n",
    "        |\n",
    "        | There are 1 duplicated rows\n",
    "        | \n",
    "        \n",
    "    \"\"\"\n",
    "    print(f\"There are {df.duplicated().sum()} duplicated rows\")\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6928ce65-5c71-4f9b-a3c2-d5859f35d090",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-func-edit\">\n",
    "<h3> Helper Funcntions (Edit Dataframe) </h3>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "48607615-3cc6-48b6-b7f5-aca3ba432093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_cols_if_exist(df: pd.DataFrame,cols_to_drop:list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----\n",
    "    Drop a column from a dataframe with inplace = True. Only execute the dropping if the cols exist.\n",
    "    \n",
    "    Args\n",
    "    -----\n",
    "    df (pd.DataFrame): the target dataframe\n",
    "    cols_to_drop (list): the list of column to be dropped\n",
    "    \n",
    "    Returns\n",
    "    -----\n",
    "    df (pd.DataFrame): the dataframe with columns dropped\n",
    "    \n",
    "    Example\n",
    "    -----\n",
    "    # Create a DataFrame\n",
    "    abc_df = pd.DataFrame(data = {\"height\":[147,190,147],\"weight\":[47,np.nan,47],\"age\":[13,27,13]},index = [0,1,2])\n",
    "    print(abc_df)  =>\n",
    "        |\n",
    "        |    height  weight  age\n",
    "        | 0     147    47.0   13\n",
    "        | 1     190     NaN   27\n",
    "        | 2     147    47.0   13\n",
    "        |\n",
    "        \n",
    "    # Drop columns if exist\n",
    "    dropped_abc_df = drop_cols_if_exist(abc_df,cols_to_drop=[\"weight\"])\n",
    "    print(dropped_abc_df)   =>\n",
    "        | Successfully dropped columns: {'weight'}\n",
    "        |    height  age\n",
    "        | 0     147   13\n",
    "        | 1     190   27\n",
    "        | 2     147   13\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    intersect_cols = set(cols_to_drop).intersection(df.columns)\n",
    "    df.drop(columns=intersect_cols,inplace=True)\n",
    "    print(f\"Successfully dropped columns: {intersect_cols}\")\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091c145a-b94c-498a-9417-c883aa133ea4",
   "metadata": {},
   "source": [
    "[Back-to-top](#4-toc)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb84599-d133-4e4b-ae4b-16f5fa387d21",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-load\">\n",
    "    <h2> Data Loading </h2>\n",
    "</a>\n",
    "<b>Table of Content:</b>\n",
    "<ul>    \n",
    "    <li> <a href=\"#4-load-process\">Load Processed Data</a></li>\n",
    "    <li> <a href=\"#4-san-check-train\">Sanity Check - Train Data</a></li>\n",
    "    <li> <a href=\"#4-san-check-test\">Sanity Check - Train Data</a></li>\n",
    "    <li> <a href=\"#4-split-xy\">Split Data</a></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eefa5f8-233c-4547-8ebc-e949550de8ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-load-process\">\n",
    "<h3> Load the Split data </h3>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "8135ee15-1589-495c-b1d5-ae2d12fc228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dtype for split dataset\n",
    "SPLIT_COL_DTYPE = {\n",
    "    \n",
    "    \"log_miles\":float,\n",
    "    \"year\":int,\n",
    "    \"make\":str,\n",
    "    \"model\":str,\n",
    "    \"trim\":str,\n",
    "    \"body_type\":str,\n",
    "    \"vehicle_type\":str,\n",
    "    \"drivetrain\":str,\n",
    "    \"transmission\":str,\n",
    "    \"engine_size\":float,\n",
    "    \"engine_block\":str,\n",
    "    \"price_range\":int,\n",
    "    \n",
    "    # For encoded fuel type\n",
    "    \"fuel_M85\":int,\n",
    "    \"fuel_Lpg\":int,\n",
    "    \"fuel_Diesel\":int,\n",
    "    \"fuel_Unleaded\":int,\n",
    "    \"fuel_Hydrogen\":int,\n",
    "    \"fuel_PremiumUnleaded\":int,\n",
    "    \"fuel_Biodiesel\":int,\n",
    "    \"fuel_E85\":int,\n",
    "    \"fuel_Electric\":int,\n",
    "    \"fuel_CompressedNaturalGas\":int,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b39b0421-f3a1-44f9-b504-432b481fc61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we read the train dataset\n",
    "train_data = pd.read_parquet(path = \"data/train-data.parquet\",\n",
    "                     columns = SPLIT_COL_DTYPE)\n",
    "train_data.reset_index(drop = True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "be995b54-f12b-4340-b192-e2840e4497b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Then, we read the testt dataset\n",
    "test_data = pd.read_parquet(path = \"data/test-data.parquet\",\n",
    "                     columns = SPLIT_COL_DTYPE)\n",
    "test_data.reset_index(drop = True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "04cd5517-f055-48d8-aa28-5a2f527b4f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b154af64-70d2-4817-ab5e-b9adf8820298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_miles</th>\n",
       "      <th>year</th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>trim</th>\n",
       "      <th>body_type</th>\n",
       "      <th>vehicle_type</th>\n",
       "      <th>drivetrain</th>\n",
       "      <th>transmission</th>\n",
       "      <th>engine_size</th>\n",
       "      <th>engine_block</th>\n",
       "      <th>price_range</th>\n",
       "      <th>fuel_M85</th>\n",
       "      <th>fuel_Lpg</th>\n",
       "      <th>fuel_Diesel</th>\n",
       "      <th>fuel_Unleaded</th>\n",
       "      <th>fuel_Hydrogen</th>\n",
       "      <th>fuel_PremiumUnleaded</th>\n",
       "      <th>fuel_Biodiesel</th>\n",
       "      <th>fuel_E85</th>\n",
       "      <th>fuel_Electric</th>\n",
       "      <th>fuel_CompressedNaturalGas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.353497</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>Tacoma</td>\n",
       "      <td>SR5</td>\n",
       "      <td>Pickup</td>\n",
       "      <td>Truck</td>\n",
       "      <td>4WD</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>3.5</td>\n",
       "      <td>V</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.780189</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>RAM</td>\n",
       "      <td>Ram 1500 Pickup</td>\n",
       "      <td>Big Horn</td>\n",
       "      <td>Pickup</td>\n",
       "      <td>Truck</td>\n",
       "      <td>RWD</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>3.0</td>\n",
       "      <td>V</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.922555</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Lexus</td>\n",
       "      <td>ES</td>\n",
       "      <td>350</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>Car</td>\n",
       "      <td>FWD</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>3.5</td>\n",
       "      <td>V</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   log_miles    year    make            model      trim body_type  \\\n",
       "0   8.353497  2020.0  Toyota           Tacoma       SR5    Pickup   \n",
       "1   9.780189  2018.0     RAM  Ram 1500 Pickup  Big Horn    Pickup   \n",
       "2   9.922555  2018.0   Lexus               ES       350     Sedan   \n",
       "\n",
       "  vehicle_type drivetrain transmission  engine_size engine_block  price_range  \\\n",
       "0        Truck        4WD    Automatic          3.5            V            3   \n",
       "1        Truck        RWD    Automatic          3.0            V            3   \n",
       "2          Car        FWD    Automatic          3.5            V            3   \n",
       "\n",
       "   fuel_M85  fuel_Lpg  fuel_Diesel  fuel_Unleaded  fuel_Hydrogen  \\\n",
       "0         0         0            0              1              0   \n",
       "1         0         0            1              0              0   \n",
       "2         0         0            0              1              0   \n",
       "\n",
       "   fuel_PremiumUnleaded  fuel_Biodiesel  fuel_E85  fuel_Electric  \\\n",
       "0                     0               0         0              0   \n",
       "1                     0               0         0              0   \n",
       "2                     0               0         0              0   \n",
       "\n",
       "   fuel_CompressedNaturalGas  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.drop(columns=col_to_drop,inplace = True)\n",
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0ab7714d-4d25-4eaf-8e2c-45de444c85d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_miles</th>\n",
       "      <th>year</th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>trim</th>\n",
       "      <th>body_type</th>\n",
       "      <th>vehicle_type</th>\n",
       "      <th>drivetrain</th>\n",
       "      <th>transmission</th>\n",
       "      <th>engine_size</th>\n",
       "      <th>engine_block</th>\n",
       "      <th>price_range</th>\n",
       "      <th>fuel_M85</th>\n",
       "      <th>fuel_Lpg</th>\n",
       "      <th>fuel_Diesel</th>\n",
       "      <th>fuel_Unleaded</th>\n",
       "      <th>fuel_Hydrogen</th>\n",
       "      <th>fuel_PremiumUnleaded</th>\n",
       "      <th>fuel_Biodiesel</th>\n",
       "      <th>fuel_E85</th>\n",
       "      <th>fuel_Electric</th>\n",
       "      <th>fuel_CompressedNaturalGas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.532738</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>Jeep</td>\n",
       "      <td>Wrangler Unlimited</td>\n",
       "      <td>Sahara</td>\n",
       "      <td>SUV</td>\n",
       "      <td>Truck</td>\n",
       "      <td>4WD</td>\n",
       "      <td>Manual</td>\n",
       "      <td>3.6</td>\n",
       "      <td>V</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.028894</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Honda</td>\n",
       "      <td>Accord</td>\n",
       "      <td>EX-L V6</td>\n",
       "      <td>Coupe</td>\n",
       "      <td>Car</td>\n",
       "      <td>FWD</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>3.5</td>\n",
       "      <td>V</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.063947</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>Kia</td>\n",
       "      <td>FORTE</td>\n",
       "      <td>LXS</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>Car</td>\n",
       "      <td>FWD</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   log_miles    year   make               model     trim body_type  \\\n",
       "0  11.532738  2014.0   Jeep  Wrangler Unlimited   Sahara       SUV   \n",
       "1  12.028894  2012.0  Honda              Accord  EX-L V6     Coupe   \n",
       "2  10.063947  2020.0    Kia               FORTE      LXS     Sedan   \n",
       "\n",
       "  vehicle_type drivetrain transmission  engine_size engine_block  price_range  \\\n",
       "0        Truck        4WD       Manual          3.6            V            2   \n",
       "1          Car        FWD    Automatic          3.5            V            1   \n",
       "2          Car        FWD    Automatic          2.0            I            2   \n",
       "\n",
       "   fuel_M85  fuel_Lpg  fuel_Diesel  fuel_Unleaded  fuel_Hydrogen  \\\n",
       "0         0         0            0              1              0   \n",
       "1         0         0            0              1              0   \n",
       "2         0         0            0              1              0   \n",
       "\n",
       "   fuel_PremiumUnleaded  fuel_Biodiesel  fuel_E85  fuel_Electric  \\\n",
       "0                     0               0         0              0   \n",
       "1                     0               0         0              0   \n",
       "2                     0               0         0              0   \n",
       "\n",
       "   fuel_CompressedNaturalGas  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.drop(columns=col_to_drop,inplace = True)\n",
    "test_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46f146e-39d5-44f2-810a-829ac8315a27",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-san-check-train\">\n",
    "<h3> Sanity Check - Train </h3>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "0b3e73a4-6cf1-43c0-bc91-a0684398925b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_miles</th>\n",
       "      <th>year</th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>trim</th>\n",
       "      <th>body_type</th>\n",
       "      <th>vehicle_type</th>\n",
       "      <th>drivetrain</th>\n",
       "      <th>transmission</th>\n",
       "      <th>engine_size</th>\n",
       "      <th>engine_block</th>\n",
       "      <th>price_range</th>\n",
       "      <th>fuel_M85</th>\n",
       "      <th>fuel_Lpg</th>\n",
       "      <th>fuel_Diesel</th>\n",
       "      <th>fuel_Unleaded</th>\n",
       "      <th>fuel_Hydrogen</th>\n",
       "      <th>fuel_PremiumUnleaded</th>\n",
       "      <th>fuel_Biodiesel</th>\n",
       "      <th>fuel_E85</th>\n",
       "      <th>fuel_Electric</th>\n",
       "      <th>fuel_CompressedNaturalGas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.353497</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>Toyota</td>\n",
       "      <td>Tacoma</td>\n",
       "      <td>SR5</td>\n",
       "      <td>Pickup</td>\n",
       "      <td>Truck</td>\n",
       "      <td>4WD</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>3.5</td>\n",
       "      <td>V</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.780189</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>RAM</td>\n",
       "      <td>Ram 1500 Pickup</td>\n",
       "      <td>Big Horn</td>\n",
       "      <td>Pickup</td>\n",
       "      <td>Truck</td>\n",
       "      <td>RWD</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>3.0</td>\n",
       "      <td>V</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.922555</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Lexus</td>\n",
       "      <td>ES</td>\n",
       "      <td>350</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>Car</td>\n",
       "      <td>FWD</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>3.5</td>\n",
       "      <td>V</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.928507</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>BMW</td>\n",
       "      <td>X5</td>\n",
       "      <td>sDrive35i</td>\n",
       "      <td>SUV</td>\n",
       "      <td>Truck</td>\n",
       "      <td>RWD</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>3.0</td>\n",
       "      <td>I</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.268992</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Scion</td>\n",
       "      <td>tC</td>\n",
       "      <td>Release Series 9.0</td>\n",
       "      <td>Coupe</td>\n",
       "      <td>Car</td>\n",
       "      <td>FWD</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>2.5</td>\n",
       "      <td>I</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   log_miles    year    make            model                trim body_type  \\\n",
       "0   8.353497  2020.0  Toyota           Tacoma                 SR5    Pickup   \n",
       "1   9.780189  2018.0     RAM  Ram 1500 Pickup            Big Horn    Pickup   \n",
       "2   9.922555  2018.0   Lexus               ES                 350     Sedan   \n",
       "3  10.928507  2017.0     BMW               X5           sDrive35i       SUV   \n",
       "4  11.268992  2015.0   Scion               tC  Release Series 9.0     Coupe   \n",
       "\n",
       "  vehicle_type drivetrain transmission  engine_size engine_block  price_range  \\\n",
       "0        Truck        4WD    Automatic          3.5            V            3   \n",
       "1        Truck        RWD    Automatic          3.0            V            3   \n",
       "2          Car        FWD    Automatic          3.5            V            3   \n",
       "3        Truck        RWD    Automatic          3.0            I            3   \n",
       "4          Car        FWD    Automatic          2.5            I            1   \n",
       "\n",
       "   fuel_M85  fuel_Lpg  fuel_Diesel  fuel_Unleaded  fuel_Hydrogen  \\\n",
       "0         0         0            0              1              0   \n",
       "1         0         0            1              0              0   \n",
       "2         0         0            0              1              0   \n",
       "3         0         0            0              0              0   \n",
       "4         0         0            0              1              0   \n",
       "\n",
       "   fuel_PremiumUnleaded  fuel_Biodiesel  fuel_E85  fuel_Electric  \\\n",
       "0                     0               0         0              0   \n",
       "1                     0               0         0              0   \n",
       "2                     0               0         0              0   \n",
       "3                     1               0         0              0   \n",
       "4                     0               0         0              0   \n",
       "\n",
       "   fuel_CompressedNaturalGas  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a2471ff3-917f-48e4-b661-0241bf1c4ac0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1764450 rows and 22 columns in this dataframe now.\n"
     ]
    }
   ],
   "source": [
    "print_dim(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "9ec4c275-4627-41ca-85eb-6b6161bf92c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Null Count ===\n",
      "log_miles                    0\n",
      "year                         0\n",
      "make                         0\n",
      "model                        0\n",
      "trim                         0\n",
      "body_type                    0\n",
      "vehicle_type                 0\n",
      "drivetrain                   0\n",
      "transmission                 0\n",
      "engine_size                  0\n",
      "engine_block                 0\n",
      "price_range                  0\n",
      "fuel_M85                     0\n",
      "fuel_Lpg                     0\n",
      "fuel_Diesel                  0\n",
      "fuel_Unleaded                0\n",
      "fuel_Hydrogen                0\n",
      "fuel_PremiumUnleaded         0\n",
      "fuel_Biodiesel               0\n",
      "fuel_E85                     0\n",
      "fuel_Electric                0\n",
      "fuel_CompressedNaturalGas    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print_null_count(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "127407cd-7c49-4ce8-9919-4ac2d1371a15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 23968 duplicated rows\n"
     ]
    }
   ],
   "source": [
    "print_duplicated_count(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab521c1b-3c2f-4c91-aba2-44b997446ecd",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-san-check-test\">\n",
    "<h3> Sanity Check - Test </h3>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6bf414fd-ae9c-40ab-9a14-f5b6efd11a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_miles</th>\n",
       "      <th>year</th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>trim</th>\n",
       "      <th>body_type</th>\n",
       "      <th>vehicle_type</th>\n",
       "      <th>drivetrain</th>\n",
       "      <th>transmission</th>\n",
       "      <th>engine_size</th>\n",
       "      <th>engine_block</th>\n",
       "      <th>price_range</th>\n",
       "      <th>fuel_M85</th>\n",
       "      <th>fuel_Lpg</th>\n",
       "      <th>fuel_Diesel</th>\n",
       "      <th>fuel_Unleaded</th>\n",
       "      <th>fuel_Hydrogen</th>\n",
       "      <th>fuel_PremiumUnleaded</th>\n",
       "      <th>fuel_Biodiesel</th>\n",
       "      <th>fuel_E85</th>\n",
       "      <th>fuel_Electric</th>\n",
       "      <th>fuel_CompressedNaturalGas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.532738</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>Jeep</td>\n",
       "      <td>Wrangler Unlimited</td>\n",
       "      <td>Sahara</td>\n",
       "      <td>SUV</td>\n",
       "      <td>Truck</td>\n",
       "      <td>4WD</td>\n",
       "      <td>Manual</td>\n",
       "      <td>3.6</td>\n",
       "      <td>V</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.028894</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Honda</td>\n",
       "      <td>Accord</td>\n",
       "      <td>EX-L V6</td>\n",
       "      <td>Coupe</td>\n",
       "      <td>Car</td>\n",
       "      <td>FWD</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>3.5</td>\n",
       "      <td>V</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.063947</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>Kia</td>\n",
       "      <td>FORTE</td>\n",
       "      <td>LXS</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>Car</td>\n",
       "      <td>FWD</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.526024</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>Jeep</td>\n",
       "      <td>Grand Cherokee</td>\n",
       "      <td>High Altitude</td>\n",
       "      <td>SUV</td>\n",
       "      <td>Truck</td>\n",
       "      <td>4WD</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>3.6</td>\n",
       "      <td>V</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.633834</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>Tiguan</td>\n",
       "      <td>SE</td>\n",
       "      <td>SUV</td>\n",
       "      <td>Truck</td>\n",
       "      <td>4WD</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   log_miles    year        make               model           trim body_type  \\\n",
       "0  11.532738  2014.0        Jeep  Wrangler Unlimited         Sahara       SUV   \n",
       "1  12.028894  2012.0       Honda              Accord        EX-L V6     Coupe   \n",
       "2  10.063947  2020.0         Kia               FORTE            LXS     Sedan   \n",
       "3  10.526024  2019.0        Jeep      Grand Cherokee  High Altitude       SUV   \n",
       "4  10.633834  2018.0  Volkswagen              Tiguan             SE       SUV   \n",
       "\n",
       "  vehicle_type drivetrain transmission  engine_size engine_block  price_range  \\\n",
       "0        Truck        4WD       Manual          3.6            V            2   \n",
       "1          Car        FWD    Automatic          3.5            V            1   \n",
       "2          Car        FWD    Automatic          2.0            I            2   \n",
       "3        Truck        4WD    Automatic          3.6            V            4   \n",
       "4        Truck        4WD    Automatic          2.0            I            2   \n",
       "\n",
       "   fuel_M85  fuel_Lpg  fuel_Diesel  fuel_Unleaded  fuel_Hydrogen  \\\n",
       "0         0         0            0              1              0   \n",
       "1         0         0            0              1              0   \n",
       "2         0         0            0              1              0   \n",
       "3         0         0            0              1              0   \n",
       "4         0         0            0              1              0   \n",
       "\n",
       "   fuel_PremiumUnleaded  fuel_Biodiesel  fuel_E85  fuel_Electric  \\\n",
       "0                     0               0         0              0   \n",
       "1                     0               0         0              0   \n",
       "2                     0               0         0              0   \n",
       "3                     0               0         0              0   \n",
       "4                     0               0         0              0   \n",
       "\n",
       "   fuel_CompressedNaturalGas  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "eeeec794-2922-45d3-8e5a-499b6c4b4e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 588151 rows and 22 columns in this dataframe now.\n"
     ]
    }
   ],
   "source": [
    "print_dim(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b5072efb-a970-4a78-ae3d-48a3c9b9e322",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Null Count ===\n",
      "log_miles                    0\n",
      "year                         0\n",
      "make                         0\n",
      "model                        0\n",
      "trim                         0\n",
      "body_type                    0\n",
      "vehicle_type                 0\n",
      "drivetrain                   0\n",
      "transmission                 0\n",
      "engine_size                  0\n",
      "engine_block                 0\n",
      "price_range                  0\n",
      "fuel_M85                     0\n",
      "fuel_Lpg                     0\n",
      "fuel_Diesel                  0\n",
      "fuel_Unleaded                0\n",
      "fuel_Hydrogen                0\n",
      "fuel_PremiumUnleaded         0\n",
      "fuel_Biodiesel               0\n",
      "fuel_E85                     0\n",
      "fuel_Electric                0\n",
      "fuel_CompressedNaturalGas    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print_null_count(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "546460a4-6a97-4e0f-88c9-c55498230c37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4218 duplicated rows\n"
     ]
    }
   ],
   "source": [
    "print_duplicated_count(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f89346-46f5-4f49-be02-39c706855919",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-split-xy\">\n",
    "<h3> Split Data into X and y </h3>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "97af8a7f-748f-4e96-97f7-be0758c68861",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = train_data.drop(columns=[\"price_range\"])\n",
    "y_train = train_data[\"price_range\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "58f3b9ca-ca51-46ac-99dc-7aa018c60298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test= test_data.drop(columns=[\"price_range\"])\n",
    "y_test = test_data[\"price_range\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8a6db840-9302-41c6-a229-37b6fd79010b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1764450, 21) (588151, 21) (1764450,) (588151,)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2083c0c-a555-4173-a025-a783fad1d979",
   "metadata": {},
   "source": [
    "[Back-to-top](#4-toc)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f086ec80-ffb5-4e75-81bc-2a6a58b953c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-cat-encode\">\n",
    "    <h2> Categorical Encoding </h2>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00e02cd-7506-41f8-a506-228bd6128c46",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üß† Idea:</b></font>\n",
    "<br><br>\n",
    "After some researches, we concluded that these are the available encoding options for our dataset.  \n",
    "\n",
    "- `Dummy Encoding` (keep all categories)\n",
    "- `Dummy Encoding` (with fixed number of category, i.e. top 10 most frequent categories)\n",
    "- `Dummy Encoding` (with value counts percentage threshold, i.e. only keep categories with more than X% of total records)\n",
    "- `Ordinal Encoding` (for category with ordinal meaning)\n",
    "- `Count Encoding` (Data Leakage if no split data) `->` Use `sklearn`.`Pipeline`\n",
    "- `Target Encoding`  (Data Leakage if no split data) `->` Use `sklearn`.`Pipeline`\n",
    "\n",
    "**However**, some of them are not suitable for our columns.\n",
    "\n",
    "`Count Enconding`: It is not useful as we have > 7M records and some values may have over millions count.  \n",
    "`Ordinal Encoding`: Our categorical columns generally do not consist of any order or level, so ordinal enconding may not be useful at all in this scenario.  \n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f3c0c6-9390-4d72-a569-03dfa9676e12",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment:</b></font>\n",
    "\n",
    "- From the above table, as we do not want to expand our feature spaces too much. We will apply target encoding to columns `make`, `model`, `trim` and `body_type`.\n",
    "\n",
    "- Advantages of `target encoding`:\n",
    "    1. `Target encoding` will not expand our feature spaces.\n",
    "    1. `Target encoding` is encoded by averaging target variable (`price_range`) within each feature group (`model`), which mean the encoded value is the average `price_range` of that `model`. \n",
    "    \n",
    "- Disadvantage of `target encoding`:\n",
    "    1. As the averaging process will gather the information across different rows, this may lead to **data leakage**. So, we will not perform encoding transformation here. Instead, we will embed the transformation in modelling pipeline.\n",
    "\n",
    "- For other columns, we will apply `dummy variable encoding`, which may not consist of data leakage problem.\n",
    "    \n",
    "- In order to integrate encoder with cross validation, we will embed the encoder into pipeline in modelling section.\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76413640-d7b6-46a6-8aad-0b9a5f71bf98",
   "metadata": {},
   "source": [
    "[Back-to-top](#4-toc)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120b72b3-2eda-4554-b5b3-5ff742033e47",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-scale\">\n",
    "    <h2> Data Scaling </h2>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c8e696-d707-4399-8b5c-752cdc28561c",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üß† Idea:</b></font>\n",
    "<br><br>\n",
    "There are three available encoding options for our dataset.  \n",
    "    \n",
    "---\n",
    "- `Standard Scaling`\n",
    "    - Output space: `[-inf,inf]` with `mean` of 0 and `variance` of 1\n",
    "    - Characteristics:\n",
    "        - Useful when features have different units\n",
    "        - Sensitive to outliers\n",
    "---\n",
    "- `Min-Max Scaling`\n",
    "    - Output space: `[0,1]`\n",
    "    - Characteristics:\n",
    "        - Maintains the shape of the original distribution\n",
    "        - Sensitive to outliers\n",
    "---\n",
    "- `Robust Scaling`\n",
    "    - Output space: `[-inf,inf]`\n",
    "    - Characteristics:\n",
    "        - Effective in the presence of outliers, as it uses the median and IQR.\n",
    "        - Maintains the central tendency (Data tends to stay at the center)\n",
    "    \n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4a26da-83e9-4ff9-a4b8-e6958f7f4a2d",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment:</b></font>\n",
    "\n",
    "In our dataset, there are several feature types:\n",
    "- `Categorical Features` (e.g. `make`, `drivetrain`)\n",
    "- `Numerical Features` (e.g. `year`, `log_miles`)\n",
    "\n",
    "---\n",
    "    \n",
    "- For `Categorical Features`, as we will apply `one-hot encoding` and `target encoding`. The output space of both method are in `[0,1]` too. If we apply Min-Max Scaling, it does not change anything. If we apply `standard scaling`, it would change the scale from `[0,1]` to `[-inf, inf]`. Therefore, we should not apply any scaling on it.\n",
    "    \n",
    "- For `Numerical Features`, as they have different magnitude scales, we should apply `standard scaling` on it.\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abd5dde-721f-4e48-aaca-bf36a8b48cae",
   "metadata": {},
   "source": [
    "[Back-to-top](#4-toc)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1849b3cf-fb79-483a-81cb-ff671649084b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-imbalance\">\n",
    "    <h2> Class Imbalance</h2>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdc8a50-54cc-474c-abbf-d87b95783417",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>‚ùì What is Class Imbalance?</b></font>\n",
    "<br><br>\n",
    "    <p><b>Class imbalance</b> refers to a situation in a classification problem where the distribution of the classes is not uniform, meaning that one or more classes have significantly fewer instances than the others. In other words, there is an unequal distribution of target labels in the dataset, and one or more classes are underrepresented compared to the others.</p>\n",
    "<br>\n",
    "    <p>For our classification task, which is <b>multi-class classification</b>, class imbalance can refer to <b>unequal distribution</b> across multiple classes. </p>\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e331e6fb-3dfc-4fdc-b712-4459c3e77f51",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üß† How to tackle Class Imbalance?</b></font>\n",
    "<br><br>\n",
    "\n",
    "There are several techniques for **Class Imbalance**.  \n",
    "1. Resampling\n",
    "    - **Over Sampling**\n",
    "    - **Under Sampling**\n",
    "    - **Hybrid Sampling** (Over Sampling on Minority Class + Under Sampling on Majority Class) (To be tested)\n",
    "2. Synthetic Data Generation\n",
    "    - <b>S</b>ynthetic <b>M</b>inority <b>O</b>ver sampling <b>TE</b>chnique <b>(SMOTE)</b>\n",
    "    - <b>S</b>ynthetic <b>M</b>inority <b>O</b>ver sampling <b>TE</b>chnique for <b>N</b>ominal & <b>C</b>ontinuous <b>(SMOTENC)</b>\n",
    "\n",
    "    \n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "11f02163-1123-4b3e-a488-71f9d8c9e223",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price_range\n",
       "2    0.441874\n",
       "1    0.270052\n",
       "3    0.197019\n",
       "4    0.061067\n",
       "5    0.029989\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()/y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "1decf269-1617-403f-a2c6-0b8244dfdfb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(y_train.value_counts().index,y_train.value_counts().values)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Target Label Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a81b29f-efec-42c8-8f28-873aa8bb1a77",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment:</b></font>\n",
    "    \n",
    "- According to above table, we can see that the minority class only consist of **3%** of the total data, if we apply **undersampling**, we will result in **15%** of data and lose **85%** of the information.\n",
    "    \n",
    "- According to above table, we can see that the ratio of majority class and minority class is **15:1**. If we apply **oversampling**, we will duplicate **15** times of the minority class.\n",
    "    \n",
    "**[After first-round of experiment]**  \n",
    "- We find that oversampling on minority class does not improve our most of our model.\n",
    "- Perhaps we can also consider hybrid sampling in next sprint, which combines both the oversampling and undersampling\n",
    "    \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac152d5-64cd-465f-a3e8-89358b80fabc",
   "metadata": {},
   "source": [
    "[Back-to-top](#4-toc)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa74660b-8590-4490-9754-819374b9debc",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-models\">\n",
    "    <h2> Proposed Models </h2>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a081eab-8d8e-4403-8236-979c89388672",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üß† Ideas:</b></font>\n",
    "\n",
    "\n",
    "- [x] Logistic Regression\n",
    "- [x] Decision Tree\n",
    "- [x] Stochastic Gradient Boosting\n",
    "- [x] AdaBoost\n",
    "- [x] XGBoost\n",
    "- [ ] Random Forest\n",
    "- [ ] CatBoost\n",
    "- [ ] Naive Bayes\n",
    "- [ ] K-Nearest Neighbor\n",
    "- [ ] Neural Network (Less Deep)\n",
    "- [ ] Neural Network (Medium Deep)\n",
    "- [ ] Neural Network (More Deep)\n",
    "\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e8cc94-5696-42c1-b67b-bf7a04ae6b20",
   "metadata": {},
   "source": [
    "[Back-to-top](#4-toc)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc2a15c-df99-418f-b322-1cf9fea2e554",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-pipelines\">\n",
    "    <h2> Model Pipelines with 10% of data</h2>\n",
    "</a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4da34e7-493c-41ce-86b8-4e450c81c8b0",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>‚ö†Ô∏è Note:</b></font>\n",
    "\n",
    "<!-- - For faster training process, we will use 10% of our training data (Around 170K Rows) here by using `train_test_split` and setting `stratify` = `y`. -->\n",
    "- We will apply Oversampling technique to deal with class imbalance.\n",
    "- We will apply Grid Search CV with 5-fold for getting the optimized parameters.\n",
    "    \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91b2f43-be82-4116-8f5a-545e0b8b5ef8",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"bensembler-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-model\">\n",
    "    <h3> Classes for Model Pipelines </h3>\n",
    "</a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "44a49b73-088e-4185-9c2e-772d8c5aa143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pprint\n",
    "import pandas as pd\n",
    "\n",
    "from types import FunctionType\n",
    "\n",
    "import marshal\n",
    "import joblib\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold,train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, TargetEncoder, StandardScaler, MinMaxScaler, LabelEncoder,FunctionTransformer\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler,SMOTENC\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "\n",
    "model_performance_dict = {\n",
    "    \"timestamp\":None,\n",
    "    \"model_name\": None,\n",
    "    \"model\": None,\n",
    "    \n",
    "    \"sub_sampling\":False,\n",
    "    \"sub_sampling_time\":0,\n",
    "    \"sub_sampling_pct_X_train\":None,\n",
    "    \"sub_sampling_pct_X_test\":None,\n",
    "    \n",
    "    \"over_sampling\":False,\n",
    "    \"over_sampling_time\":0,\n",
    "    \n",
    "    \"X_train_shape\":None,\n",
    "    \"X_test_shape\":None,\n",
    "    \n",
    "    \"target_encoding\": False,\n",
    "    \"target_encoding_col\": [],\n",
    "    \"one_hot_encoding\": False,\n",
    "    \"one_hot_encoding_col\": [],\n",
    "    \n",
    "    \"standard_scaling\": False,\n",
    "    \"standard_scaling_col\": [],\n",
    "    \"min_max_scaling\": False,\n",
    "    \"min_max_scaling_col\": [],\n",
    "    \n",
    "    \"drop_column\": False,\n",
    "    \"dropped_columns\":[],\n",
    "    \n",
    "    \"pca\": False,\n",
    "    \"pca_n_components\": None,\n",
    "    \n",
    "    \"scoring\":None,\n",
    "    \n",
    "    \"grid_search\": False,\n",
    "    \"best_params\": None,\n",
    "    \"best_train_score\": None,\n",
    "    \n",
    "    \"params\": None,\n",
    "    \"train_score\": None,\n",
    "    \n",
    "    \n",
    "    \"test_score\": None,    \n",
    "    \"confusion_matrix\": None,\n",
    "}\n",
    "\n",
    "\n",
    "class ModelPerf:\n",
    "    \n",
    "    _data = None\n",
    "    _path = None\n",
    "\n",
    "    def __init__(self,path,read = True):\n",
    "        self._path = path\n",
    "        # try to load the previous file\n",
    "        if read:\n",
    "            self.read_csv()\n",
    "        \n",
    "    def _is_path_valid(self):\n",
    "        os.path.isfile(self._path)\n",
    "        \n",
    "    def read_csv(self):\n",
    "        try:\n",
    "            self._data = pd.read_csv(self._path)\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Creating new model performance file...\")\n",
    "            self._data = pd.DataFrame()\n",
    "            \n",
    "    def export_csv(self):\n",
    "        if self._data.shape[0] > 0:\n",
    "            self._data.to_csv(self._path,index = False)\n",
    "            logger.info(f\"Model Performance CSV is exported at {self._path}\")\n",
    "        else:\n",
    "            logger.warning(f\"Model Performance is empty\")\n",
    "            \n",
    "    def add_data(self,new_data:dict,export = True):\n",
    "        new_df = pd.DataFrame([new_data])\n",
    "        self._data = pd.concat([self._data,new_df])\n",
    "        self._data.sort_values(by=[\"model_name\"],ascending = [True],inplace = True)\n",
    "        self._data.reset_index(drop = True, inplace=True)\n",
    "        if export:\n",
    "            self.export_csv()\n",
    "            \n",
    "    def del_data(self,model_name: str):\n",
    "        self._data = self._data[self._data[\"model_name\"] != model_name]\n",
    "        logger.info(f\"Result for model {model_name} is deleted.\")\n",
    "        \n",
    "    def get_data(self):\n",
    "        return self._data\n",
    "    \n",
    "    def print_data(self):\n",
    "        print(self.get_data())\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "class MyModel:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 X_train,X_test,\n",
    "                 y_train,y_test,\n",
    "                 train_subsample_size = None,\n",
    "                 test_subsample_size = None):\n",
    "        # Random State\n",
    "        self._random_state = 42\n",
    "        self._train_subsample_size = train_subsample_size\n",
    "        self._test_subsample_size = test_subsample_size\n",
    "        \n",
    "        # Store the data\n",
    "        self.X_train = X_train.copy()\n",
    "        self.le = LabelEncoder()\n",
    "        self.y_train = pd.Series(self.le.fit_transform(y_train),index= y_train)\n",
    "            \n",
    "        self.X_test = X_test.copy()\n",
    "        self.y_test = pd.Series(self.le.transform(y_test),index= y_test)\n",
    "        \n",
    "        self.print_data_size()\n",
    "                \n",
    "        self._classes = sorted(list(set(y_train)))\n",
    "        \n",
    "        # Default is 5-fold\n",
    "        self.cv = None\n",
    "        \n",
    "        # Subsampling\n",
    "        self._subsampled = False\n",
    "        \n",
    "        # Diff Types of Sampling\n",
    "        self._sampled = False\n",
    "        self._sampling_name = None\n",
    "        \n",
    "        # Custom Encoder\n",
    "        self._one_hot_transformer = None\n",
    "        self._one_hot_col = []\n",
    "        self._tar_end_transformer = None\n",
    "        self._tar_end_col = []\n",
    "        self.encoder = None\n",
    "        self._encoder_steps = []\n",
    "        self._encoder_name = \"\"\n",
    "        \n",
    "        # Custom Scaler\n",
    "        self._standard_scaler = None\n",
    "        self._ss_col = []\n",
    "        self._min_max_scaler = None\n",
    "        self._mm_col = []\n",
    "        self.scaler = None\n",
    "        self._scaler_steps = []\n",
    "        self._scaler_name = \"\"\n",
    "        \n",
    "        # Column Dropper\n",
    "        self.dropped_col = False\n",
    "        self._dropped_columns = []\n",
    "        \n",
    "        # Custom PCA\n",
    "        self.pca = None\n",
    "        self._pca_name = None\n",
    "        self._pca_n_components = None\n",
    "        \n",
    "        # Grid Search\n",
    "        self._use_grid_search= False\n",
    "        self.grid_search = None\n",
    "        self.gs_params = {}\n",
    "        \n",
    "        # Custom Model\n",
    "        self.model = None\n",
    "        self._model_name = \"\"\n",
    "        \n",
    "        # Custom Score\n",
    "        self._scoring = \"f1_weighted\"\n",
    "        self._scoring_func = lambda y_true,y_pred: f1_score(y_true,y_pred,average = \"weighted\")\n",
    "        \n",
    "        # Custom Pipeline\n",
    "        self.pipeline = None\n",
    "        self._pipeline_steps = []\n",
    "        \n",
    "        # Prediction\n",
    "        self.train_y_pred = None\n",
    "        self.y_pred = None\n",
    "        self.y_prob = None\n",
    "        \n",
    "        # Timer\n",
    "        self._subsample_time = None\n",
    "        self._sampling_time = None\n",
    "        self._fit_time = None\n",
    "        self._predict_time = None\n",
    "        \n",
    "        \n",
    "        # Model Performance\n",
    "        self.model_perf = None\n",
    "        self._raw_cm = None\n",
    "        self._cm = None\n",
    "        self._cv_results = None\n",
    "        \n",
    "    def init_drop_columns(self,cols_to_drop=[]):\n",
    "        if len(cols_to_drop) == 0:\n",
    "            self.dropped_col = False\n",
    "        else:        \n",
    "            start_time = time.time()\n",
    "            logger.info(\"Dropping Columns...\")\n",
    "            available_col = self.X_train.columns\n",
    "            cols_to_drop = list(filter(lambda col: col in available_col,cols_to_drop))\n",
    "            self.dropped_col = cols_to_drop\n",
    "            self.dropped_col = True\n",
    "            self.X_train.drop(columns=cols_to_drop,inplace= True) \n",
    "            self.X_test.drop(columns=cols_to_drop,inplace= True) \n",
    "            end_time = time.time()\n",
    "            logger.info(f\"Dropping Columns Completed | Time elapsed: {self.time_to_str(end_time - start_time)}\")\n",
    "            self.print_data_size(title = \"After Dropping Columns\")\n",
    "        \n",
    "    def init_subsampling(self):\n",
    "        if self._train_subsample_size is not None or self._test_subsample_size is not None:\n",
    "            start_time = time.time()\n",
    "            logger.info(\"Sub-sampling on Train Data...\")\n",
    "            \n",
    "            if self._train_subsample_size is not None:\n",
    "                # Sample the train dataset with train_test_split function\n",
    "                self.X_train, _, self.y_train, _ = train_test_split(self.X_train, self.y_train,\n",
    "                                                                    stratify = self.y_train, \n",
    "                                                                    train_size = self._train_subsample_size,\n",
    "                                                                    random_state = self._random_state)\n",
    "            if self._test_subsample_size is not None:\n",
    "                # Sample the test dataset with train_test_split function\n",
    "                _, self.X_test, _ ,self.y_test= train_test_split(self.X_test, self.y_test,\n",
    "                                                                 stratify = self.y_test, \n",
    "                                                                 test_size = self._test_subsample_size, \n",
    "                                                                 random_state = self._random_state)\n",
    "\n",
    "            end_time = time.time()\n",
    "            \n",
    "            self._subsample_time = end_time - start_time\n",
    "            logger.info(f\"Subsampling Completed | Time elapsed: {self.time_to_str(self._subsample_time)}\")\n",
    "            self.print_data_size(title = \"After Subsampling\")\n",
    "            self._subsampled = True\n",
    "            \n",
    "        else: \n",
    "            logger.error(\"If init_subsampling(), 'train_subsample_size' & 'test_subsample_size' cannot be None.\")\n",
    "            logger.info(\"No Subsampling is applied.\")\n",
    "            self.print_empty()\n",
    "        \n",
    "    def init_customer_over_sampling(self):\n",
    "        assert not self._sampled,\"Already sampled.\"\n",
    "        start_time = time.time()\n",
    "        self._sampling_name = \"Custom Over Samping\"\n",
    "        \n",
    "        logger.info(f\"{self._sampling_name} on Train Data...\")\n",
    "        self.X_train, self.y_train = self.customer_random_over_sampling(self.X_train,self.y_train)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        self._sampling_time = end_time - start_time\n",
    "        logger.info(f\"{self._sampling_name} Completed | Time elapsed: {self.time_to_str(self._sampling_time)}\")\n",
    "        self.print_data_size(title = f\"After {self._sampling_name}\")\n",
    "        self._sampled = True\n",
    "        \n",
    "    \n",
    "    def init_smotenc(self):\n",
    "        assert not self._sampled,\"Already sampled.\"\n",
    "        start_time = time.time()\n",
    "        self._sampling_name = \"SMOTENC Over Sampling\"\n",
    "        \n",
    "        logger.info(f\"{self._sampling_name} on Train Data...\")\n",
    "        snc = SMOTENC(categorical_features = [\"make\",\"model\",\"trim\",\n",
    "                                              \"body_type\",\"vehicle_type\",\"transmission\",\n",
    "                                              \"drivetrain\",\"engine_block\"],random_state = 42)\n",
    "        self.X_train,self.y_train = snc.fit_resample(self.X_train,self.y_train)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        self._sampling_time = end_time - start_time\n",
    "        logger.info(f\"{self._sampling_name} Completed | Time elapsed: {self.time_to_str(self._sampling_time)}\")\n",
    "        self.print_data_size(title = f\"After {self._sampling_name}\")\n",
    "        self._sampled = True\n",
    "        \n",
    "    def init_over_sampling(self):\n",
    "        assert not self._sampled,\"Already sampled.\"\n",
    "        start_time = time.time()\n",
    "        self._sampling_name = \"Random Over Samping\"\n",
    "        \n",
    "        logger.info(f\"{self._sampling_name} on Train Data...\")\n",
    "        ros = RandomOverSampler(random_state = 42)\n",
    "        self.X_train,self.y_train = ros.fit_resample(self.X_train,self.y_train)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        self._sampling_time = end_time - start_time\n",
    "        logger.info(f\"{self._sampling_name} Completed | Time elapsed: {self.time_to_str(self._sampling_time)}\")\n",
    "        self.print_data_size(title = f\"After {self._sampling_name}\")\n",
    "        self._sampled = True\n",
    "    \n",
    "    # For Hybrid Samping\n",
    "    def _under_sample_target_size(self,target_class: int):\n",
    "        target_class = self.le.transform([target_class])[0]\n",
    "        size_dict = dict(self.y_train.value_counts())\n",
    "        target_size = size_dict[target_class]\n",
    "        for k,v in size_dict.items():\n",
    "            if v > target_size:\n",
    "                size_dict[k]=target_size\n",
    "        return size_dict\n",
    "    \n",
    "    # For Hybrid Samping\n",
    "    def _over_sample_target_size(self,target_class: int):\n",
    "        target_class = self.le.transform([target_class])[0]\n",
    "        size_dict = dict(self.y_train.value_counts())\n",
    "        target_size = size_dict[target_class]\n",
    "        for k,v in size_dict.items():\n",
    "            if v < target_size:\n",
    "                size_dict[k]=target_size\n",
    "        return size_dict\n",
    "    \n",
    "    def init_hybrid_sampling(self,target_class:int):\n",
    "        assert not self._sampled,\"Already sampled.\"\n",
    "        start_time = time.time()\n",
    "        self._sampling_name = \"Hybrid Samping\"\n",
    "        \n",
    "        logger.info(f\"{self._sampling_name} on Train Data...\")\n",
    "        \n",
    "        assert target_class in self.y_train.value_counts().index, \"'target_class' is not in target_variable\"\n",
    "        \n",
    "        rus = RandomUnderSampler(sampling_strategy = self._under_sample_target_size(target_class),\n",
    "                                 random_state=42)\n",
    "        self.X_train,self.y_train = rus.fit_resample(self.X_train,self.y_train)\n",
    "        \n",
    "        \n",
    "        ros = RandomOverSampler(sampling_strategy = self._over_sample_target_size(target_class),\n",
    "                                 random_state=42)\n",
    "        self.X_train,self.y_train = ros.fit_resample(self.X_train,self.y_train)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        self._sampling_time = end_time - start_time\n",
    "        logger.info(f\"{self._sampling_name} Completed | Time elapsed: {self.time_to_str(self._sampling_time)}\")\n",
    "        self.print_data_size(title = f\"After {self._sampling_name}\")\n",
    "        self._sampled = True\n",
    "    \n",
    "        \n",
    "    def init_one_hot_encoding(self,encoder = None, columns=[]):\n",
    "        if len(columns) > 0:\n",
    "            if encoder is None:\n",
    "                self._one_hot_transformer = Pipeline(\n",
    "                    steps=[(\"one_hot\",OneHotEncoder(sparse_output = False,drop=\"first\"))]\n",
    "                )\n",
    "            else:\n",
    "                self._one_hot_transformer = Pipeline(\n",
    "                    steps=[(\"one_hot\",encoder)]\n",
    "                )\n",
    "            \n",
    "            available_col = self.X_train.columns\n",
    "            columns = list(filter(lambda col: col in available_col,columns))\n",
    "            self._set_one_hot_col(columns)\n",
    "            \n",
    "            # Add to encoder steps\n",
    "            self._encoder_steps.append((\"one_hot\",self._one_hot_transformer,self._one_hot_col))\n",
    "            \n",
    "            logger.info(\"One-hot Encoder Initialization Completed\")\n",
    "            self.print_empty()\n",
    "        \n",
    "    def init_target_encoding(self,encoder = None, columns=[]):\n",
    "        if len(columns) > 0:\n",
    "            if encoder is None:\n",
    "                self._tar_end_transformer = Pipeline(\n",
    "                    steps=[(\"tar_end\",TargetEncoder(target_type=\"continuous\",\n",
    "                                                    random_state=self._random_state))]\n",
    "                )\n",
    "            else:\n",
    "                self._tar_end_transformer = Pipeline(\n",
    "                    steps=[(\"tar_end\",encoder)]\n",
    "                )\n",
    "                \n",
    "            available_col = self.X_train.columns\n",
    "            columns = list(filter(lambda col: col in available_col,columns))\n",
    "            self._set_target_encode_col(columns)\n",
    "            \n",
    "            # Add to encoder steps\n",
    "            self._encoder_steps.append((\"tar_end\",self._tar_end_transformer,self._tar_end_col))\n",
    "            \n",
    "            logger.info(\"Target Encoder Initialization Completed\")\n",
    "            self.print_empty()\n",
    "\n",
    "    def init_encoder(self,name = \"encoders\"):        \n",
    "        if len(self._encoder_steps) == 0:\n",
    "            self.encoder = None\n",
    "        else:\n",
    "            self.encoder = ColumnTransformer(\n",
    "                transformers=self._encoder_steps,\n",
    "                remainder = \"passthrough\",\n",
    "                verbose_feature_names_out = False\n",
    "            )\n",
    "            self._encoder_name = name\n",
    "            self._pipeline_steps.append((self._encoder_name,self.encoder))\n",
    "        \n",
    "    def init_standard_scaler(self,scaler = None, columns=[]):\n",
    "        if len(columns) > 0:\n",
    "            if scaler is None:\n",
    "                self._standard_scaler = Pipeline(\n",
    "                    steps=[(\"standard_scaler\",StandardScaler()),]\n",
    "                )\n",
    "            else:\n",
    "                self._standard_scaler = Pipeline(\n",
    "                    steps=[(\"standard_scaler\",scaler),]\n",
    "                )\n",
    "            \n",
    "            available_col = self.X_train.columns\n",
    "            columns = list(filter(lambda col: col in available_col,columns))\n",
    "            self._set_standard_scaling_col(columns)\n",
    "                \n",
    "            # Add to encoder steps\n",
    "            self._scaler_steps.append((\"standard_scaler\",self._standard_scaler,self._ss_col))\n",
    "            \n",
    "            logger.info(\"Standard Scaler Initialization Completed\")\n",
    "            self.print_empty()\n",
    "        \n",
    "    def init_min_max_scaler(self,scaler = None,columns=[]):\n",
    "        if len(columns) > 0:\n",
    "            if scaler is None:\n",
    "                self._min_max_scaler = Pipeline(\n",
    "                    steps=[(\"min_max_scaler\",MinMaxScaler()),]\n",
    "                )\n",
    "            else:\n",
    "                self._min_max_scaler = Pipeline(\n",
    "                    steps=[(\"min_max_scaler\",scaler),]\n",
    "                )\n",
    "            \n",
    "            available_col = self.X_train.columns\n",
    "            columns = list(filter(lambda col: col in available_col,columns))\n",
    "            self._set_min_max_scaling_col(columns)\n",
    "                \n",
    "            # Add to encoder steps\n",
    "            self._scaler_steps.append((\"min_max_scaler\",self._min_max_scaler,self._mm_col))\n",
    "            \n",
    "            logger.info(\"Min-Max Scaler Initialization Completed\")\n",
    "            self.print_empty()\n",
    "            \n",
    "    def init_scaler(self,name = \"scalers\"):\n",
    "        if len(self._scaler_steps) == 0:\n",
    "            self.scaler = None\n",
    "        else:        \n",
    "            self.scaler = ColumnTransformer(\n",
    "                transformers=self._scaler_steps,\n",
    "                remainder = \"passthrough\",\n",
    "                verbose_feature_names_out = False\n",
    "            )\n",
    "            self._scaler_name = name\n",
    "            self._pipeline_steps.append((self._scaler_name,self.scaler))\n",
    "            \n",
    "    def init_pca(self,name=\"pca\",n_components = None):\n",
    "        \n",
    "        if n_components is None:\n",
    "            self.pca = PCA()\n",
    "        else:\n",
    "            self._pca_n_components = n_components\n",
    "            self.pca = PCA(n_components = n_components)\n",
    "            \n",
    "        self._pca_name = name\n",
    "        self._pipeline_steps.append((self._pca_name,self.pca))\n",
    "        logger.info(\"PCA Initialization Completed\")\n",
    "        self.print_empty()        \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_df_val(df):\n",
    "        val = df.values\n",
    "        return val\n",
    "    \n",
    "    def init_model(self,name,model):\n",
    "        self.model = model\n",
    "        self._model_name = name\n",
    "        \n",
    "        if \"knn\" in name:\n",
    "            get_value_transformer = FunctionTransformer(self.get_df_val)\n",
    "            self._pipeline_steps.append((\"get_value_steps\",get_value_transformer))\n",
    "        \n",
    "        self._pipeline_steps.append((self._model_name,self.model))\n",
    "        logger.info(f\"{self.model.__class__.__name__} (name: {self._model_name}) Initialization Completed\")\n",
    "        self.print_empty()\n",
    "                \n",
    "        \n",
    "    def init_grid_search(self,params,n_job = 1,no_cv = False):\n",
    "        if self.pipeline is None:\n",
    "            logger.error(\"Please initialize the model pipeline first\")\n",
    "            return\n",
    "        if len(params) == 0:\n",
    "            logger.error(f\"Grid Search params cannot be empty.\")\n",
    "        else:\n",
    "            self._set_grid_search_params(params)\n",
    "            self.grid_search = GridSearchCV(self.pipeline,\n",
    "                                            self.gs_params,\n",
    "                                            n_jobs = n_job,\n",
    "                                            cv = self.cv if not no_cv else None,\n",
    "                                            scoring = self._scoring,\n",
    "                                            verbose= 0)\n",
    "            self._use_grid_search = True\n",
    "            if self.cv is not None:\n",
    "                logger.info(f\"Grid Search with {self.cv if isinstance(self.cv, int) else self.cv.get_n_splits()}-folds cross-validation Initialized Completed\")\n",
    "            else:\n",
    "                logger.info(f\"Grid Search without cross-validation Initialized Completed\")\n",
    "            \n",
    "            self.print_empty()\n",
    "     \n",
    "    def init_pipeline(self):\n",
    "        if self.model is None:\n",
    "            logger.error(f\"No Model is initiated\")\n",
    "            return\n",
    "        if len(self._pipeline_steps) == 0:\n",
    "            logger.error(f\"No steps in pipeline\")\n",
    "            return\n",
    "        else:           \n",
    "            self.pipeline = Pipeline(self._pipeline_steps,verbose=False)\n",
    "            self.pipeline.set_output(transform=\"pandas\")\n",
    "            \n",
    "            \n",
    "    # ===================== Set Function ============================== \n",
    "        \n",
    "    def set_kfold_cv(self,cv = 5):\n",
    "        if cv is None:\n",
    "            self.cv = None\n",
    "        assert isinstance(cv,int), \"'cv' must be integer\"\n",
    "        assert cv >= 2, \"'cv' must be greater than 1\"\n",
    "        self.cv = StratifiedKFold(n_splits = cv)\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    def _set_one_hot_col(self,columns=[]):\n",
    "        if len(columns) == 0:\n",
    "            return\n",
    "        else:\n",
    "            for col in columns:\n",
    "                assert col in self.X_train.columns, f\"{col} not found in X_train\"\n",
    "                assert col in self.X_test.columns, f\"{col} not found in X_test\"\n",
    "            self._one_hot_col = columns\n",
    "            \n",
    "            \n",
    "    def _set_target_encode_col(self,columns=[]):\n",
    "        if len(columns) == 0:\n",
    "            return\n",
    "        else:\n",
    "            for col in columns:\n",
    "                assert col in self.X_train.columns, f\"{col} not found in X_train\"\n",
    "                assert col in self.X_test.columns, f\"{col} not found in X_test\"\n",
    "            self._tar_end_col = columns\n",
    "            \n",
    "            \n",
    "    def _set_standard_scaling_col(self,columns=[]):\n",
    "        if len(columns) == 0:\n",
    "            return\n",
    "        else:\n",
    "            for col in columns:\n",
    "                assert col in self.X_train.columns, f\"{col} not found in X_train\"\n",
    "                assert col in self.X_test.columns, f\"{col} not found in X_test\"\n",
    "            self._ss_col = columns\n",
    "        \n",
    "    def _set_min_max_scaling_col(self,columns=[]):\n",
    "        if len(columns) == 0:\n",
    "            return\n",
    "        else:\n",
    "            for col in columns:\n",
    "                assert col in self.X_train.columns, f\"{col} not found in X_train\"\n",
    "                assert col in self.X_test.columns, f\"{col} not found in X_test\"\n",
    "            self._mm_col = columns\n",
    "            \n",
    "    def _set_grid_search_params(self,params):\n",
    "        available_prefix = [step[0] for step in self.pipeline.steps]\n",
    "        for param in params:\n",
    "            assert param.split(\"__\")[0] in available_prefix, f\"Grid Search Params {param} not found. Only found: {available_prefix}\"\n",
    "        self.gs_params = params    \n",
    "    \n",
    "    # ================== Model Fitting =================\n",
    "    def fit(self):\n",
    "        if self._use_grid_search:\n",
    "            start_time = time.time()\n",
    "            logger.info(f\"Model Fitting with Grid Search...\")\n",
    "            self.grid_search.fit(self.X_train,self.y_train)\n",
    "            self.cv_results = pd.DataFrame(self.grid_search.cv_results_)\n",
    "            logger.info(\"Grid Search Cross Validationn Results Exported.\")\n",
    "        else:\n",
    "            start_time = time.time()\n",
    "            logger.info(f\"Model Fitting...\")\n",
    "            self.pipeline.fit(self.X_train,self.y_train)\n",
    "            self.train_y_pred = self.pipeline.predict(self.X_train)\n",
    "            \n",
    "        end_time = time.time()\n",
    "        self._fit_time = end_time - start_time\n",
    "        logger.info(f\"Total Fitting Time:{self.time_to_str(self._fit_time)}\")\n",
    "        self.print_empty()\n",
    "        \n",
    "    \n",
    "    # ================== Model Prediction =================\n",
    "    def predict(self):\n",
    "        logger.info(\"Predicting on Test Data...\")\n",
    "        start_time = time.time()\n",
    "        if self._use_grid_search:\n",
    "            try:\n",
    "                self.y_prob = self.grid_search.predict_proba(self.X_test)\n",
    "            except AttributeError as e:\n",
    "                logger.error(f\"{e}\")\n",
    "            self.y_pred = self.grid_search.predict(self.X_test)\n",
    "        else:\n",
    "            try:\n",
    "                self.y_prob = self.pipeline.predict_proba(self.X_test)\n",
    "            except AttributeError as e:\n",
    "                logger.error(f\"{e}\")\n",
    "            self.y_pred = self.pipeline.predict(self.X_test)\n",
    "\n",
    "        end_time = time.time()\n",
    "        self._predict_time = end_time - start_time\n",
    "        logger.info(f\"Total Predicting Time:{self.time_to_str(self._predict_time)}\")\n",
    "        self.print_empty()\n",
    "        \n",
    "        \n",
    "    # ================== Model Evaluation =================\n",
    "    def get_best_train_score(self):\n",
    "        if self._use_grid_search:\n",
    "            return self.grid_search.best_score_\n",
    "        else:\n",
    "            return self._scoring_func(self.y_train,self.train_y_pred)\n",
    "        \n",
    "    def print_best_train_score(self):\n",
    "        logger.info(f\"Best Train Score: {self.get_best_train_score()}\")\n",
    "        \n",
    "    def get_test_score(self):\n",
    "        if self.y_pred is not None:\n",
    "            return self._scoring_func(self.y_test,self.y_pred)\n",
    "        else:\n",
    "            logger.error(f\"No y_pred found for testing score.\")\n",
    "    \n",
    "    def print_test_score(self):\n",
    "        if self.y_pred is not None:\n",
    "            logger.info(f\"Test Score: {self.get_test_score()}\")\n",
    "        else:\n",
    "            logger.error(f\"No y_pred found for testing score.\")\n",
    "        \n",
    "    def get_best_params(self):\n",
    "        if self._use_grid_search:\n",
    "            return self.grid_search.best_params_\n",
    "        else:\n",
    "            return self.pipeline.steps[-1][1].get_params()\n",
    "        \n",
    "    def print_best_params(self):\n",
    "        self.print_title(title = \"Parameters\")\n",
    "        pprint.pprint(self.get_best_params())\n",
    "        \n",
    "    def compute_raw_confusion_matrix(self):\n",
    "        if self.y_pred is not None:\n",
    "            logger.info(\"Computing Confusion Matrix...\")\n",
    "            self._raw_cm = confusion_matrix(self.y_test,self.y_pred)\n",
    "        else:\n",
    "            logger.error(\"Please make prediction first.\")\n",
    "    \n",
    "    def compute_confusion_matrix(self):\n",
    "        if self._raw_cm is None:\n",
    "            self.compute_raw_confusion_matrix()\n",
    "        self._cm = pd.DataFrame(self._raw_cm,\n",
    "                                index = [f\"actual_{i}\" for i in self._classes],\n",
    "                                columns = [f\"predict_{i}\" for i in self._classes])\n",
    "        \n",
    "    def print_confusion_matrix(self):\n",
    "        if self._cm is None:\n",
    "            self.compute_confusion_matrix()\n",
    "                    \n",
    "        self.print_title(title = \"Confusion Matrix on Test Data\")\n",
    "        print(self._cm)\n",
    "        \n",
    "    def _save_cv_result(self):\n",
    "        if self._use_grid_search:\n",
    "            self.cv_results.to_csv(f\"log/gs-{self._model_name}.csv\")\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    def save_model(self):\n",
    "        self.print_title(title = \"Saving Model\")\n",
    "        save_path = f\"model/{self._model_name}.joblib\"\n",
    "        if self._use_grid_search:\n",
    "            self._save_cv_result()\n",
    "            joblib.dump(self.grid_search.best_estimator_,save_path)\n",
    "            # print(self.grid_search.best_estimator_)\n",
    "        else:\n",
    "            joblib.dump(self.pipeline,save_path)\n",
    "            # print(self.pipeline)\n",
    "        logger.info(f\"Model ({self._model_name}) saved at {save_path}\")\n",
    "    \n",
    "    # ================== Helper Function =================\n",
    "    @staticmethod\n",
    "    def time_to_str(t):\n",
    "        time_list = str(datetime.timedelta(seconds=t)).split(\".\")[0].split(\":\")\n",
    "        return f\"\\t{time_list[0]} hours {time_list[1]} minutes {time_list[2]} seconds\"\n",
    "    \n",
    "    def print_data_size(self, title = \"Data Shape\"):\n",
    "        self.print_title(title)\n",
    "        logger.info(f\"X_train: {self.X_train.shape} | y_train: {self.y_train.shape}\")\n",
    "        logger.info(f\"X_test : {self.X_test.shape} | y_test : {self.y_test.shape}\")\n",
    "        self.print_empty()\n",
    "        \n",
    "    @staticmethod\n",
    "    def print_title(title):\n",
    "        num_of_equal = 15\n",
    "        logger.info(f\"{'='*num_of_equal} {title} {'='*num_of_equal}\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def print_empty():\n",
    "        logger.info(\"\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def customer_random_over_sampling(X,y):\n",
    "        assert X.shape[0] == y.shape[0]\n",
    "\n",
    "        X.reset_index(drop = True,inplace = True)\n",
    "        y.reset_index(drop = True,inplace = True)\n",
    "\n",
    "        # Join both table\n",
    "        target_col = \"price_range\"\n",
    "        X[target_col] = y\n",
    "\n",
    "        # Get the class in target variable\n",
    "        _classes = sorted(list(set(y)))\n",
    "\n",
    "        # Value Count for each class\n",
    "        val_count = y.value_counts()\n",
    "        # print(val_count)\n",
    "\n",
    "        # Get the maximum count\n",
    "        max_count = max(val_count)\n",
    "\n",
    "        # list to store sampled data\n",
    "        new_data_list = []\n",
    "\n",
    "        for i in _classes:\n",
    "            diff = max_count - val_count[i]\n",
    "\n",
    "            # set seed to for reproducibilty\n",
    "            rand_seed = 0\n",
    "\n",
    "            # cache the filter dataframe\n",
    "            this_X = X[X[target_col]==i]\n",
    "\n",
    "            sample_batch_size = min(diff,min(val_count[i],5000)) # Round off to nearest thousand, to avoid duplicate the whole sample set => more variance\n",
    "\n",
    "            while diff > 0:\n",
    "                # For eac\n",
    "                sampled_data = this_X.sample(min(diff,sample_batch_size), # for last iteration, use diff to make sure all classes have same same sample size\n",
    "                                  replace = False, # Ensure no duplicates in this batch\n",
    "                                  random_state = rand_seed)\n",
    "                new_data_list.append(sampled_data)\n",
    "                diff -= sample_batch_size\n",
    "                rand_seed += 1\n",
    "\n",
    "        new_data_list.append(X)\n",
    "        new_data = pd.concat(new_data_list)\n",
    "        new_data.reset_index(drop = True, inplace = True)\n",
    "        new_X = new_data.drop(columns = [target_col])\n",
    "        X.drop(columns = [target_col],inplace = True)\n",
    "        new_y = new_data[target_col]\n",
    "\n",
    "        return new_X, new_y\n",
    "\n",
    "    \n",
    "\n",
    "    def add_perf(self,model_res: ModelPerf):\n",
    "        this_dict = model_performance_dict.copy()\n",
    "        this_dict[\"timestamp\"] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        this_dict[\"model_name\"] = self._model_name\n",
    "        this_dict[\"model\"] = self.model.__class__.__name__\n",
    "        \n",
    "        this_dict[\"sub_sampling\"] = self._subsampled\n",
    "        this_dict[\"sub_sampling_time\"] = self._subsample_time\n",
    "        this_dict[\"sub_sampling_pct_X_train\"] = self._train_subsample_size\n",
    "        this_dict[\"sub_sampling_pct_X_test\"] = self._train_subsample_size\n",
    "        \n",
    "        this_dict[\"sampling\"] = self._sampled\n",
    "        this_dict[\"sampling_method\"] = self._sampling_name\n",
    "        this_dict[\"sampling_time\"] = self._sampling_time\n",
    "        \n",
    "        this_dict[\"X_train_shape\"] = self.X_train.shape\n",
    "        this_dict[\"X_test_shape\"] = self.X_test.shape\n",
    "        \n",
    "        if self._tar_end_transformer is not None:\n",
    "            this_dict[\"target_encoding\"] = True\n",
    "            this_dict[\"target_encoding_col\"] = self._tar_end_col\n",
    "            \n",
    "        \n",
    "        if self._one_hot_transformer is not None:\n",
    "            this_dict[\"one_hot_encoding\"] = True\n",
    "            this_dict[\"one_hot_encoding_col\"] = self._one_hot_col\n",
    "            \n",
    "        if self._standard_scaler is not None:\n",
    "            this_dict[\"standard_scaling\"] = True\n",
    "            this_dict[\"standard_scaling_col\"] = self._ss_col\n",
    "            \n",
    "        if self._min_max_scaler is not None:\n",
    "            this_dict[\"min_max_scaling\"] = True\n",
    "            this_dict[\"min_max_scaling_col\"] = self._mm_col\n",
    "            \n",
    "        if self.dropped_col: \n",
    "            this_dict[\"drop_column\"] = True\n",
    "            this_dict[\"dropped_columns\"] = self._dropped_columns\n",
    "            \n",
    "        if self.pca is not None:\n",
    "            this_dict[\"pca\"] = True\n",
    "            this_dict[\"pca_n_components\"] = self._pca_n_components\n",
    "            \n",
    "        this_dict[\"scoring\"] = self._scoring\n",
    "        if self._use_grid_search:\n",
    "            this_dict[\"grid_search\"] = self._use_grid_search \n",
    "            this_dict[\"best_params\"] = self.get_best_params()\n",
    "            this_dict[\"best_train_score\"] = self.get_best_train_score()\n",
    "        else:\n",
    "            this_dict[\"params\"] = self.get_best_params()\n",
    "            this_dict[\"train_score\"] = self.get_best_train_score()\n",
    "            \n",
    "        \n",
    "        this_dict[\"test_score\"] = self.get_test_score()\n",
    "        if self._cm is not None:\n",
    "            this_dict[\"confusion_matrix\"] = self._raw_cm\n",
    "            \n",
    "            \n",
    "        model_res.add_data(this_dict)\n",
    "        return model_res\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "661cde7f-8547-4624-ab7c-974155dc7b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log the model performance\n",
    "perf = ModelPerf(\"model/model_performance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "a61e4366-3317-4a9f-b68c-421bde979f2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Global Subsampling Percentage\n",
    "global_subsampling_pct = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d5f29b-440b-4750-a700-3087ceaa6382",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-model\">\n",
    "    <h3> Model - Logistic Regression (Baseline Model) </h3>\n",
    "</a>\n",
    "<br>\n",
    "<b>Why Logistic Regression Model is the baseline model?</b>\n",
    "<ol>\n",
    "    <li><b>Simplicity</b></li>\n",
    "    <p>For classification task, <b>Logistic Regression</b> is a simple and interpretable linear model. It models the relationship between the input features and the binary outcome by applying the logistic function to a linear combination of the input features. </p>       \n",
    "    <li><b>Interpretability</b></li>\n",
    "    <p>The coefficients in Logistic Regression provide a direct interpretation of the impact of each feature on the log-odds of the outcome. This interpretability is valuable for gaining insights into the relationships between predictors and the target variable. </p>         \n",
    "    <li><b>Robustness to Irrelevant Features</b></li>\n",
    "    <p>Logistic Regression can be robust to irrelevant features, as its regularization techniques (e.g., L1 or L2 regularization) help prevent overfitting and suppress the impact of less informative features. </p>     \n",
    "\n",
    "</ol>\n",
    "\n",
    "Therefore, any model tested later on should be compared with Logistic Regression baseline model.    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc141f4-6bd4-4cb0-8415-dcdf5f1c2a2d",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>‚ö†Ô∏è Note:</b></font>\n",
    "\n",
    "For baseline model, we just use Logistic Regression to get our first and simple modelling result.\n",
    "    \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca9297-1c87-4abe-bc58-6fcf7a413f3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "e7bc2aa2-b40d-418b-8cd1-193738465d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 17:01:12 - INFO >>> =============== Data Shape ===============\n",
      "2023-11-18 17:01:12 - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-18 17:01:12 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:01:12 - INFO >>> \n",
      "2023-11-18 17:01:12 - INFO >>> Sub-sampling on Train Data...\n",
      "2023-11-18 17:01:14 - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 01 seconds\n",
      "2023-11-18 17:01:14 - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-18 17:01:14 - INFO >>> X_train: (176445, 21) | y_train: (176445,)\n",
      "2023-11-18 17:01:14 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:01:14 - INFO >>> \n",
      "2023-11-18 17:01:14 - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-18 17:01:14 - INFO >>> \n",
      "2023-11-18 17:01:14 - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-18 17:01:14 - INFO >>> \n",
      "2023-11-18 17:01:14 - INFO >>> Standard Scaler Initialization Completed\n",
      "2023-11-18 17:01:14 - INFO >>> \n",
      "2023-11-18 17:01:14 - INFO >>> LogisticRegression (name: logit_v1) Initialization Completed\n",
      "2023-11-18 17:01:14 - INFO >>> \n",
      "2023-11-18 17:01:14 - INFO >>> Model Fitting...\n",
      "2023-11-18 17:02:11 - INFO >>> Total Fitting Time:\t0 hours 00 minutes 56 seconds\n",
      "2023-11-18 17:02:11 - INFO >>> \n",
      "2023-11-18 17:02:11 - INFO >>> Predicting on Test Data...\n",
      "2023-11-18 17:02:15 - INFO >>> Total Predicting Time:\t0 hours 00 minutes 03 seconds\n",
      "2023-11-18 17:02:15 - INFO >>> \n",
      "2023-11-18 17:02:15 - INFO >>> Best Train Score: 0.7185644499123088\n",
      "2023-11-18 17:02:15 - INFO >>> Test Score: 0.7165310470426126\n",
      "2023-11-18 17:02:15 - INFO >>> =============== Parameters ===============\n",
      "{'C': 1,\n",
      " 'class_weight': None,\n",
      " 'dual': False,\n",
      " 'fit_intercept': True,\n",
      " 'intercept_scaling': 1,\n",
      " 'l1_ratio': None,\n",
      " 'max_iter': 10000,\n",
      " 'multi_class': 'auto',\n",
      " 'n_jobs': None,\n",
      " 'penalty': 'l2',\n",
      " 'random_state': None,\n",
      " 'solver': 'lbfgs',\n",
      " 'tol': 0.0001,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n",
      "2023-11-18 17:02:15 - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-18 17:02:15 - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     125152      29888       3153        471        167\n",
      "actual_2      26486     210382      21389       1263        368\n",
      "actual_3        354      37810      73110       3518       1085\n",
      "actual_4         57       1784      22809       8120       3147\n",
      "actual_5         35        281       3495       5287       8540\n",
      "2023-11-18 17:02:15 - INFO >>> Model Performance CSV is exported at model/model_performance.csv\n",
      "2023-11-18 17:02:15 - INFO >>> =============== Saving Model ===============\n",
      "2023-11-18 17:02:15 - INFO >>> Model (logit_v1) saved at model/logit_v1.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    model.init_standard_scaler(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_scaler() # Activate all scalers and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    model_name = \"logit_v1\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = LogisticRegression(penalty = \"l2\", # Avoid Overfitting\n",
    "                                                max_iter=10000,\n",
    "                                                C = 1))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "logit_v1 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7bc1de-53cd-47a0-98a6-d5090c161d11",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üß† Idea:</b></font>\n",
    "    \n",
    "- Let check on the model coefficients.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cae945e-805f-4c72-b2c6-9de203502023",
   "metadata": {},
   "source": [
    "##### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "162987a1-b45a-4bac-8fdd-7b5971655212",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>trim</th>\n",
       "      <th>body_type</th>\n",
       "      <th>vehicle_type_Truck</th>\n",
       "      <th>drivetrain_FWD</th>\n",
       "      <th>drivetrain_RWD</th>\n",
       "      <th>transmission_Manual</th>\n",
       "      <th>engine_block_I</th>\n",
       "      <th>engine_block_V</th>\n",
       "      <th>log_miles</th>\n",
       "      <th>year</th>\n",
       "      <th>engine_size</th>\n",
       "      <th>fuel_M85</th>\n",
       "      <th>fuel_Lpg</th>\n",
       "      <th>fuel_Diesel</th>\n",
       "      <th>fuel_Unleaded</th>\n",
       "      <th>fuel_Hydrogen</th>\n",
       "      <th>fuel_PremiumUnleaded</th>\n",
       "      <th>fuel_Biodiesel</th>\n",
       "      <th>fuel_E85</th>\n",
       "      <th>fuel_Electric</th>\n",
       "      <th>fuel_CompressedNaturalGas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>class_1</th>\n",
       "      <td>0.024467</td>\n",
       "      <td>-2.019613</td>\n",
       "      <td>-1.248588</td>\n",
       "      <td>0.042960</td>\n",
       "      <td>-0.472248</td>\n",
       "      <td>0.746472</td>\n",
       "      <td>0.306837</td>\n",
       "      <td>0.590740</td>\n",
       "      <td>0.481194</td>\n",
       "      <td>-0.313404</td>\n",
       "      <td>2.463421</td>\n",
       "      <td>-0.012568</td>\n",
       "      <td>-0.018043</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.043905</td>\n",
       "      <td>0.179178</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>-0.156791</td>\n",
       "      <td>-0.002301</td>\n",
       "      <td>0.393653</td>\n",
       "      <td>-0.082464</td>\n",
       "      <td>0.003614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_2</th>\n",
       "      <td>-0.059722</td>\n",
       "      <td>-0.978103</td>\n",
       "      <td>-0.734619</td>\n",
       "      <td>0.061005</td>\n",
       "      <td>-0.053702</td>\n",
       "      <td>0.264833</td>\n",
       "      <td>0.186551</td>\n",
       "      <td>-0.265136</td>\n",
       "      <td>0.180250</td>\n",
       "      <td>-0.424122</td>\n",
       "      <td>0.159196</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>-0.189753</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>0.105659</td>\n",
       "      <td>0.151649</td>\n",
       "      <td>-0.000305</td>\n",
       "      <td>-0.197451</td>\n",
       "      <td>0.011817</td>\n",
       "      <td>0.500822</td>\n",
       "      <td>-0.050702</td>\n",
       "      <td>0.001714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_3</th>\n",
       "      <td>-0.047129</td>\n",
       "      <td>0.227006</td>\n",
       "      <td>0.158036</td>\n",
       "      <td>-0.021594</td>\n",
       "      <td>0.420328</td>\n",
       "      <td>-0.365247</td>\n",
       "      <td>-0.051097</td>\n",
       "      <td>-0.250116</td>\n",
       "      <td>-0.233421</td>\n",
       "      <td>0.286393</td>\n",
       "      <td>-0.575990</td>\n",
       "      <td>0.004055</td>\n",
       "      <td>-0.215980</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>-0.321995</td>\n",
       "      <td>0.258950</td>\n",
       "      <td>-0.000854</td>\n",
       "      <td>0.067893</td>\n",
       "      <td>-0.005208</td>\n",
       "      <td>-0.431778</td>\n",
       "      <td>0.088286</td>\n",
       "      <td>-0.002563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_4</th>\n",
       "      <td>-0.055627</td>\n",
       "      <td>1.124845</td>\n",
       "      <td>0.672287</td>\n",
       "      <td>-0.025406</td>\n",
       "      <td>0.261283</td>\n",
       "      <td>-0.462146</td>\n",
       "      <td>-0.346152</td>\n",
       "      <td>-0.061128</td>\n",
       "      <td>-0.154295</td>\n",
       "      <td>0.199279</td>\n",
       "      <td>-0.896588</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.087335</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000310</td>\n",
       "      <td>-0.044436</td>\n",
       "      <td>-0.026939</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.031315</td>\n",
       "      <td>-0.002970</td>\n",
       "      <td>-0.334820</td>\n",
       "      <td>0.007166</td>\n",
       "      <td>-0.002026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_5</th>\n",
       "      <td>0.138011</td>\n",
       "      <td>1.645864</td>\n",
       "      <td>1.152884</td>\n",
       "      <td>-0.056966</td>\n",
       "      <td>-0.155661</td>\n",
       "      <td>-0.183913</td>\n",
       "      <td>-0.096139</td>\n",
       "      <td>-0.014360</td>\n",
       "      <td>-0.273728</td>\n",
       "      <td>0.251854</td>\n",
       "      <td>-1.150038</td>\n",
       "      <td>0.003459</td>\n",
       "      <td>0.336440</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>0.304677</td>\n",
       "      <td>-0.562838</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.255035</td>\n",
       "      <td>-0.001337</td>\n",
       "      <td>-0.127877</td>\n",
       "      <td>0.037714</td>\n",
       "      <td>-0.000740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             make     model      trim  body_type  vehicle_type_Truck  \\\n",
       "class_1  0.024467 -2.019613 -1.248588   0.042960           -0.472248   \n",
       "class_2 -0.059722 -0.978103 -0.734619   0.061005           -0.053702   \n",
       "class_3 -0.047129  0.227006  0.158036  -0.021594            0.420328   \n",
       "class_4 -0.055627  1.124845  0.672287  -0.025406            0.261283   \n",
       "class_5  0.138011  1.645864  1.152884  -0.056966           -0.155661   \n",
       "\n",
       "         drivetrain_FWD  drivetrain_RWD  transmission_Manual  engine_block_I  \\\n",
       "class_1        0.746472        0.306837             0.590740        0.481194   \n",
       "class_2        0.264833        0.186551            -0.265136        0.180250   \n",
       "class_3       -0.365247       -0.051097            -0.250116       -0.233421   \n",
       "class_4       -0.462146       -0.346152            -0.061128       -0.154295   \n",
       "class_5       -0.183913       -0.096139            -0.014360       -0.273728   \n",
       "\n",
       "         engine_block_V  log_miles      year  engine_size  fuel_M85  fuel_Lpg  \\\n",
       "class_1       -0.313404   2.463421 -0.012568    -0.018043  0.000102 -0.000011   \n",
       "class_2       -0.424122   0.159196  0.001086    -0.189753 -0.000088 -0.000036   \n",
       "class_3        0.286393  -0.575990  0.004055    -0.215980 -0.000010  0.000493   \n",
       "class_4        0.199279  -0.896588  0.003968     0.087335 -0.000003 -0.000310   \n",
       "class_5        0.251854  -1.150038  0.003459     0.336440 -0.000002 -0.000137   \n",
       "\n",
       "         fuel_Diesel  fuel_Unleaded  fuel_Hydrogen  fuel_PremiumUnleaded  \\\n",
       "class_1    -0.043905       0.179178       0.000382             -0.156791   \n",
       "class_2     0.105659       0.151649      -0.000305             -0.197451   \n",
       "class_3    -0.321995       0.258950      -0.000854              0.067893   \n",
       "class_4    -0.044436      -0.026939       0.000794              0.031315   \n",
       "class_5     0.304677      -0.562838      -0.000017              0.255035   \n",
       "\n",
       "         fuel_Biodiesel  fuel_E85  fuel_Electric  fuel_CompressedNaturalGas  \n",
       "class_1       -0.002301  0.393653      -0.082464                   0.003614  \n",
       "class_2        0.011817  0.500822      -0.050702                   0.001714  \n",
       "class_3       -0.005208 -0.431778       0.088286                  -0.002563  \n",
       "class_4       -0.002970 -0.334820       0.007166                  -0.002026  \n",
       "class_5       -0.001337 -0.127877       0.037714                  -0.000740  "
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_model = logit_v1.pipeline[-1]\n",
    "feature_importance = pd.DataFrame(this_model.coef_,\n",
    "                                  index = [f\"class_{i+1}\" for i in this_model.classes_],\n",
    "                                  columns=this_model.feature_names_in_)\n",
    "\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8568ff5a-b61e-455a-953f-8283bb9d89ad",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment:</b></font>\n",
    "    \n",
    "- From the coefficient table, we see that some columns are very close 0. It means these columns may not be useful for our model.\n",
    "- Let's try to drop them away. Convert all values to absolute values and take average of all 5 models. If the aggregated value is smaller then 0.001.\n",
    "    \n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "fbff8c08-542f-40b0-b98d-2b114de70c76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fuel_M85', 'fuel_Lpg', 'fuel_Hydrogen']"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg = feature_importance.abs().mean()\n",
    "col_to_drop = list(agg[agg <= 0.001].index)\n",
    "col_to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d930a8e-b9c9-49be-a718-72e5afacac85",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Logistic Regression (after dropping some columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "2b270a8c-fa03-4083-8c06-a639583bfbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 17:02:16 - INFO >>> =============== Data Shape ===============\n",
      "2023-11-18 17:02:16 - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-18 17:02:16 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:02:16 - INFO >>> \n",
      "2023-11-18 17:02:16 - INFO >>> Dropping columns...\n",
      "2023-11-18 17:02:16 - INFO >>> =============== After Dropping Columns ===============\n",
      "2023-11-18 17:02:16 - INFO >>> X_train: (1764450, 18) | y_train: (1764450,)\n",
      "2023-11-18 17:02:16 - INFO >>> X_test : (588151, 18) | y_test : (588151,)\n",
      "2023-11-18 17:02:16 - INFO >>> \n",
      "2023-11-18 17:02:16 - INFO >>> Sub-sampling on Train Data...\n",
      "2023-11-18 17:02:18 - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 01 seconds\n",
      "2023-11-18 17:02:18 - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-18 17:02:18 - INFO >>> X_train: (176445, 18) | y_train: (176445,)\n",
      "2023-11-18 17:02:18 - INFO >>> X_test : (588151, 18) | y_test : (588151,)\n",
      "2023-11-18 17:02:18 - INFO >>> \n",
      "2023-11-18 17:02:18 - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-18 17:02:18 - INFO >>> \n",
      "2023-11-18 17:02:18 - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-18 17:02:18 - INFO >>> \n",
      "2023-11-18 17:02:18 - INFO >>> Standard Scaler Initialization Completed\n",
      "2023-11-18 17:02:18 - INFO >>> \n",
      "2023-11-18 17:02:18 - INFO >>> LogisticRegression (name: logit_v2) Initialization Completed\n",
      "2023-11-18 17:02:18 - INFO >>> \n",
      "2023-11-18 17:02:18 - INFO >>> Model Fitting...\n",
      "2023-11-18 17:03:00 - INFO >>> Total Fitting Time:\t0 hours 00 minutes 42 seconds\n",
      "2023-11-18 17:03:00 - INFO >>> \n",
      "2023-11-18 17:03:00 - INFO >>> Predicting on Test Data...\n",
      "2023-11-18 17:03:04 - INFO >>> Total Predicting Time:\t0 hours 00 minutes 03 seconds\n",
      "2023-11-18 17:03:04 - INFO >>> \n",
      "2023-11-18 17:03:04 - INFO >>> Best Train Score: 0.7195722985426606\n",
      "2023-11-18 17:03:04 - INFO >>> Test Score: 0.7176162082891397\n",
      "2023-11-18 17:03:04 - INFO >>> =============== Parameters ===============\n",
      "{'C': 1,\n",
      " 'class_weight': None,\n",
      " 'dual': False,\n",
      " 'fit_intercept': True,\n",
      " 'intercept_scaling': 1,\n",
      " 'l1_ratio': None,\n",
      " 'max_iter': 10000,\n",
      " 'multi_class': 'auto',\n",
      " 'n_jobs': None,\n",
      " 'penalty': 'l2',\n",
      " 'random_state': None,\n",
      " 'solver': 'lbfgs',\n",
      " 'tol': 0.0001,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n",
      "2023-11-18 17:03:04 - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-18 17:03:04 - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     124691      30159       3319        483        179\n",
      "actual_2      26729     209619      21861       1303        376\n",
      "actual_3        371      36867      73765       3735       1139\n",
      "actual_4         57       1703      22420       8761       2976\n",
      "actual_5         34        281       3284       5379       8660\n",
      "2023-11-18 17:03:04 - INFO >>> Model Performance CSV is exported at model/model_performance.csv\n",
      "2023-11-18 17:03:04 - INFO >>> =============== Saving Model ===============\n",
      "2023-11-18 17:03:04 - INFO >>> Model (logit_v2) saved at model/logit_v2.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "    \n",
    "    # Drop non-important columns\n",
    "    global col_to_drop\n",
    "    model.init_drop_columns(col_to_drop)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate scaler steps\n",
    "    model.init_standard_scaler(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_scaler() # Activate all scalers and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    model_name = \"logit_v2\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = LogisticRegression(penalty = \"l2\", # Avoid Overfitting\n",
    "                                                max_iter=10000,\n",
    "                                                C = 1))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "logit_v2 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ca8006-58af-4381-9f85-f9392baa4c7a",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment:</b></font>\n",
    "    \n",
    "- From the above results for logistic regression, we obtained:\n",
    "|Model|Column Dropped|Best Params / Params|training F1|testing F1|\n",
    "|---|---|---|:---:|:---:|\n",
    "|`Logistic Regression`|`None`|`{'C': 1}`|`71.86%`|`71.65%`|\n",
    "|`Logistic Regression`|`['fuel_M85', 'fuel_Lpg', 'fuel_Hydrogen']`|`{'C': 1}`|`71.96%`|`71.76%`|\n",
    "\n",
    "- There is a slightly increase in both train and test score after dropping some non-important columns.\n",
    "    \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4d4023-42d3-4c55-8d31-8db5b16c33f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üß† Idea:</b></font>\n",
    "    \n",
    "- From the plot in the section <a href=\"#4-imbalance\">Class Imbalance</a>, there is class imbalance issue. Let's try to apply oversampling.\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1357c5c6-155c-4cdb-821b-814cff66a7ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Logistic Regression + Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "f91f0abb-9be6-4cb3-8863-cdae9546c460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 17:03:49 - INFO >>> =============== Data Shape ===============\n",
      "2023-11-18 17:03:49 - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-18 17:03:49 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:03:49 - INFO >>> \n",
      "2023-11-18 17:03:49 - INFO >>> Sub-sampling on Train Data...\n",
      "2023-11-18 17:03:52 - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 02 seconds\n",
      "2023-11-18 17:03:52 - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-18 17:03:52 - INFO >>> X_train: (176445, 21) | y_train: (176445,)\n",
      "2023-11-18 17:03:52 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:03:52 - INFO >>> \n",
      "2023-11-18 17:03:52 - INFO >>> Custom Over Samping on Train Data...\n",
      "2023-11-18 17:03:52 - INFO >>> Custom Over Samping Completed | Time elapsed: \t0 hours 00 minutes 00 seconds\n",
      "2023-11-18 17:03:52 - INFO >>> =============== After Custom Over Samping ===============\n",
      "2023-11-18 17:03:52 - INFO >>> X_train: (389830, 21) | y_train: (389830,)\n",
      "2023-11-18 17:03:52 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:03:52 - INFO >>> \n",
      "2023-11-18 17:03:52 - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-18 17:03:52 - INFO >>> \n",
      "2023-11-18 17:03:52 - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-18 17:03:52 - INFO >>> \n",
      "2023-11-18 17:03:52 - INFO >>> Standard Scaler Initialization Completed\n",
      "2023-11-18 17:03:52 - INFO >>> \n",
      "2023-11-18 17:03:52 - INFO >>> LogisticRegression (name: logit_v3) Initialization Completed\n",
      "2023-11-18 17:03:52 - INFO >>> \n",
      "2023-11-18 17:03:52 - INFO >>> Model Fitting...\n",
      "2023-11-18 17:05:13 - INFO >>> Total Fitting Time:\t0 hours 01 minutes 21 seconds\n",
      "2023-11-18 17:05:13 - INFO >>> \n",
      "2023-11-18 17:05:13 - INFO >>> Predicting on Test Data...\n",
      "2023-11-18 17:05:18 - INFO >>> Total Predicting Time:\t0 hours 00 minutes 05 seconds\n",
      "2023-11-18 17:05:18 - INFO >>> \n",
      "2023-11-18 17:05:19 - INFO >>> Best Train Score: 0.6876074738096731\n",
      "2023-11-18 17:05:19 - INFO >>> Test Score: 0.6926254970926574\n",
      "2023-11-18 17:05:19 - INFO >>> =============== Parameters ===============\n",
      "{'C': 1,\n",
      " 'class_weight': None,\n",
      " 'dual': False,\n",
      " 'fit_intercept': True,\n",
      " 'intercept_scaling': 1,\n",
      " 'l1_ratio': None,\n",
      " 'max_iter': 10000,\n",
      " 'multi_class': 'auto',\n",
      " 'n_jobs': None,\n",
      " 'penalty': 'l2',\n",
      " 'random_state': None,\n",
      " 'solver': 'lbfgs',\n",
      " 'tol': 0.0001,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n",
      "2023-11-18 17:05:19 - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-18 17:05:19 - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     136680      16312       3315       1900        624\n",
      "actual_2      49484     161784      41740       5149       1731\n",
      "actual_3        865      16098      73829      21304       3781\n",
      "actual_4         84        408       7512      19206       8707\n",
      "actual_5         41         75        425       3344      13753\n",
      "2023-11-18 17:05:19 - INFO >>> Model Performance CSV is exported at model/model_performance.csv\n",
      "2023-11-18 17:05:19 - INFO >>> =============== Saving Model ===============\n",
      "2023-11-18 17:05:19 - INFO >>> Model (logit_v3) saved at model/logit_v3.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "    model.init_customer_over_sampling() # Oversampling for class imabalance\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "    \n",
    "    # Instantiate scaler steps\n",
    "    model.init_standard_scaler(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_scaler() # Activate all scalers and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    model_name = \"logit_v3\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = LogisticRegression(penalty = \"l2\", # Avoid Overfitting\n",
    "                                                max_iter=10000,\n",
    "                                                C = 1))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "logit_v3 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385fa71b-b335-4029-abf5-df1cab9ceb58",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üß† Idea:</b></font>\n",
    "    \n",
    "- Let check on the model coefficients.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1293f54d-20f3-4374-9a80-bce54d99718f",
   "metadata": {},
   "source": [
    "##### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "10b22ec1-f0b9-475e-aa5c-d1d8fd986243",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>trim</th>\n",
       "      <th>body_type</th>\n",
       "      <th>vehicle_type_Truck</th>\n",
       "      <th>drivetrain_FWD</th>\n",
       "      <th>drivetrain_RWD</th>\n",
       "      <th>transmission_Manual</th>\n",
       "      <th>engine_block_I</th>\n",
       "      <th>engine_block_V</th>\n",
       "      <th>log_miles</th>\n",
       "      <th>year</th>\n",
       "      <th>engine_size</th>\n",
       "      <th>fuel_M85</th>\n",
       "      <th>fuel_Lpg</th>\n",
       "      <th>fuel_Diesel</th>\n",
       "      <th>fuel_Unleaded</th>\n",
       "      <th>fuel_Hydrogen</th>\n",
       "      <th>fuel_PremiumUnleaded</th>\n",
       "      <th>fuel_Biodiesel</th>\n",
       "      <th>fuel_E85</th>\n",
       "      <th>fuel_Electric</th>\n",
       "      <th>fuel_CompressedNaturalGas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>class_1</th>\n",
       "      <td>0.065475</td>\n",
       "      <td>-2.283144</td>\n",
       "      <td>-1.294527</td>\n",
       "      <td>-0.050078</td>\n",
       "      <td>-0.806970</td>\n",
       "      <td>0.902357</td>\n",
       "      <td>0.474603</td>\n",
       "      <td>0.629479</td>\n",
       "      <td>0.464659</td>\n",
       "      <td>-0.299924</td>\n",
       "      <td>2.696359</td>\n",
       "      <td>-0.014646</td>\n",
       "      <td>0.036011</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.090758</td>\n",
       "      <td>0.184620</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>-0.093912</td>\n",
       "      <td>-0.001084</td>\n",
       "      <td>0.435618</td>\n",
       "      <td>-0.084817</td>\n",
       "      <td>0.003022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_2</th>\n",
       "      <td>-0.084530</td>\n",
       "      <td>-1.483670</td>\n",
       "      <td>-0.946898</td>\n",
       "      <td>0.024001</td>\n",
       "      <td>-0.079404</td>\n",
       "      <td>0.331328</td>\n",
       "      <td>0.397358</td>\n",
       "      <td>-0.362163</td>\n",
       "      <td>0.306790</td>\n",
       "      <td>-0.516722</td>\n",
       "      <td>0.357246</td>\n",
       "      <td>-0.001061</td>\n",
       "      <td>-0.074396</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.003088</td>\n",
       "      <td>0.160715</td>\n",
       "      <td>-0.000145</td>\n",
       "      <td>-0.089682</td>\n",
       "      <td>0.009067</td>\n",
       "      <td>0.596672</td>\n",
       "      <td>-0.086397</td>\n",
       "      <td>0.001456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_3</th>\n",
       "      <td>-0.039253</td>\n",
       "      <td>-0.424419</td>\n",
       "      <td>-0.127936</td>\n",
       "      <td>0.068684</td>\n",
       "      <td>0.566779</td>\n",
       "      <td>-0.178561</td>\n",
       "      <td>0.138543</td>\n",
       "      <td>-0.215112</td>\n",
       "      <td>-0.157741</td>\n",
       "      <td>0.174016</td>\n",
       "      <td>-0.539753</td>\n",
       "      <td>0.003649</td>\n",
       "      <td>-0.126155</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.001573</td>\n",
       "      <td>-0.319960</td>\n",
       "      <td>0.161548</td>\n",
       "      <td>-0.001260</td>\n",
       "      <td>0.205795</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.224212</td>\n",
       "      <td>0.065091</td>\n",
       "      <td>0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_4</th>\n",
       "      <td>-0.050457</td>\n",
       "      <td>1.215415</td>\n",
       "      <td>0.715591</td>\n",
       "      <td>0.011335</td>\n",
       "      <td>0.529529</td>\n",
       "      <td>-0.691864</td>\n",
       "      <td>-0.476786</td>\n",
       "      <td>-0.001926</td>\n",
       "      <td>-0.074158</td>\n",
       "      <td>0.190452</td>\n",
       "      <td>-1.075548</td>\n",
       "      <td>0.005604</td>\n",
       "      <td>0.087802</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.001169</td>\n",
       "      <td>-0.150173</td>\n",
       "      <td>-0.026446</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.070267</td>\n",
       "      <td>-0.003590</td>\n",
       "      <td>-0.629971</td>\n",
       "      <td>-0.025321</td>\n",
       "      <td>-0.003416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_5</th>\n",
       "      <td>0.108765</td>\n",
       "      <td>2.975818</td>\n",
       "      <td>1.653770</td>\n",
       "      <td>-0.053943</td>\n",
       "      <td>-0.209934</td>\n",
       "      <td>-0.363259</td>\n",
       "      <td>-0.533718</td>\n",
       "      <td>-0.050278</td>\n",
       "      <td>-0.539550</td>\n",
       "      <td>0.452178</td>\n",
       "      <td>-1.438305</td>\n",
       "      <td>0.006453</td>\n",
       "      <td>0.076737</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000377</td>\n",
       "      <td>0.563980</td>\n",
       "      <td>-0.480438</td>\n",
       "      <td>-0.000247</td>\n",
       "      <td>-0.092469</td>\n",
       "      <td>-0.004338</td>\n",
       "      <td>-0.178107</td>\n",
       "      <td>0.131445</td>\n",
       "      <td>-0.001102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             make     model      trim  body_type  vehicle_type_Truck  \\\n",
       "class_1  0.065475 -2.283144 -1.294527  -0.050078           -0.806970   \n",
       "class_2 -0.084530 -1.483670 -0.946898   0.024001           -0.079404   \n",
       "class_3 -0.039253 -0.424419 -0.127936   0.068684            0.566779   \n",
       "class_4 -0.050457  1.215415  0.715591   0.011335            0.529529   \n",
       "class_5  0.108765  2.975818  1.653770  -0.053943           -0.209934   \n",
       "\n",
       "         drivetrain_FWD  drivetrain_RWD  transmission_Manual  engine_block_I  \\\n",
       "class_1        0.902357        0.474603             0.629479        0.464659   \n",
       "class_2        0.331328        0.397358            -0.362163        0.306790   \n",
       "class_3       -0.178561        0.138543            -0.215112       -0.157741   \n",
       "class_4       -0.691864       -0.476786            -0.001926       -0.074158   \n",
       "class_5       -0.363259       -0.533718            -0.050278       -0.539550   \n",
       "\n",
       "         engine_block_V  log_miles      year  engine_size  fuel_M85  fuel_Lpg  \\\n",
       "class_1       -0.299924   2.696359 -0.014646     0.036011  0.000050 -0.000013   \n",
       "class_2       -0.516722   0.357246 -0.001061    -0.074396 -0.000038 -0.000015   \n",
       "class_3        0.174016  -0.539753  0.003649    -0.126155 -0.000005  0.001573   \n",
       "class_4        0.190452  -1.075548  0.005604     0.087802 -0.000003 -0.001169   \n",
       "class_5        0.452178  -1.438305  0.006453     0.076737 -0.000003 -0.000377   \n",
       "\n",
       "         fuel_Diesel  fuel_Unleaded  fuel_Hydrogen  fuel_PremiumUnleaded  \\\n",
       "class_1    -0.090758       0.184620       0.000309             -0.093912   \n",
       "class_2    -0.003088       0.160715      -0.000145             -0.089682   \n",
       "class_3    -0.319960       0.161548      -0.001260              0.205795   \n",
       "class_4    -0.150173      -0.026446       0.001342              0.070267   \n",
       "class_5     0.563980      -0.480438      -0.000247             -0.092469   \n",
       "\n",
       "         fuel_Biodiesel  fuel_E85  fuel_Electric  fuel_CompressedNaturalGas  \n",
       "class_1       -0.001084  0.435618      -0.084817                   0.003022  \n",
       "class_2        0.009067  0.596672      -0.086397                   0.001456  \n",
       "class_3       -0.000054 -0.224212       0.065091                   0.000040  \n",
       "class_4       -0.003590 -0.629971      -0.025321                  -0.003416  \n",
       "class_5       -0.004338 -0.178107       0.131445                  -0.001102  "
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_model = logit_v3.pipeline[-1]\n",
    "feature_importance = pd.DataFrame(this_model.coef_,\n",
    "                                  index = [f\"class_{i+1}\" for i in this_model.classes_],\n",
    "                                  columns=this_model.feature_names_in_)\n",
    "\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb9b98-11a3-47b8-86f2-c2deb3e7bf27",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment:</b></font>\n",
    "    \n",
    "- From the coefficient table, we see that some columns are very close 0. It means these columns may not be useful for our model.\n",
    "- Let's try to drop them away. Convert all values to absolute values and take average of all 5 models. If the aggregated value is smaller then 0.001.\n",
    "    \n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "c0edefc2-9b78-4fa4-8386-d4c3d9ebef42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fuel_M85', 'fuel_Lpg', 'fuel_Hydrogen']"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg = feature_importance.abs().mean()\n",
    "col_to_drop = list(agg[agg <= 0.001].index)\n",
    "col_to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13c10b6-b89c-4234-a62e-1473fcfcfd1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Logistic Regression + OverSampling (after dropping some columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "013ed09f-52ac-4196-8c1a-15530f3b7013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 17:06:57 - INFO >>> =============== Data Shape ===============\n",
      "2023-11-18 17:06:57 - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-18 17:06:57 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:06:57 - INFO >>> \n",
      "2023-11-18 17:06:57 - INFO >>> Dropping columns...\n",
      "2023-11-18 17:06:57 - INFO >>> =============== After Dropping Columns ===============\n",
      "2023-11-18 17:06:57 - INFO >>> X_train: (1764450, 18) | y_train: (1764450,)\n",
      "2023-11-18 17:06:57 - INFO >>> X_test : (588151, 18) | y_test : (588151,)\n",
      "2023-11-18 17:06:57 - INFO >>> \n",
      "2023-11-18 17:06:57 - INFO >>> Sub-sampling on Train Data...\n",
      "2023-11-18 17:06:59 - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 02 seconds\n",
      "2023-11-18 17:06:59 - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-18 17:06:59 - INFO >>> X_train: (176445, 18) | y_train: (176445,)\n",
      "2023-11-18 17:06:59 - INFO >>> X_test : (588151, 18) | y_test : (588151,)\n",
      "2023-11-18 17:06:59 - INFO >>> \n",
      "2023-11-18 17:06:59 - INFO >>> Custom Over Samping on Train Data...\n",
      "2023-11-18 17:07:00 - INFO >>> Custom Over Samping Completed | Time elapsed: \t0 hours 00 minutes 00 seconds\n",
      "2023-11-18 17:07:00 - INFO >>> =============== After Custom Over Samping ===============\n",
      "2023-11-18 17:07:00 - INFO >>> X_train: (389830, 18) | y_train: (389830,)\n",
      "2023-11-18 17:07:00 - INFO >>> X_test : (588151, 18) | y_test : (588151,)\n",
      "2023-11-18 17:07:00 - INFO >>> \n",
      "2023-11-18 17:07:00 - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-18 17:07:00 - INFO >>> \n",
      "2023-11-18 17:07:00 - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-18 17:07:00 - INFO >>> \n",
      "2023-11-18 17:07:00 - INFO >>> Standard Scaler Initialization Completed\n",
      "2023-11-18 17:07:00 - INFO >>> \n",
      "2023-11-18 17:07:00 - INFO >>> LogisticRegression (name: logit_v4) Initialization Completed\n",
      "2023-11-18 17:07:00 - INFO >>> \n",
      "2023-11-18 17:07:00 - INFO >>> Model Fitting...\n",
      "2023-11-18 17:08:56 - INFO >>> Total Fitting Time:\t0 hours 01 minutes 55 seconds\n",
      "2023-11-18 17:08:56 - INFO >>> \n",
      "2023-11-18 17:08:56 - INFO >>> Predicting on Test Data...\n",
      "2023-11-18 17:09:00 - INFO >>> Total Predicting Time:\t0 hours 00 minutes 04 seconds\n",
      "2023-11-18 17:09:00 - INFO >>> \n",
      "2023-11-18 17:09:00 - INFO >>> Best Train Score: 0.6889341051544143\n",
      "2023-11-18 17:09:00 - INFO >>> Test Score: 0.6928884070830426\n",
      "2023-11-18 17:09:00 - INFO >>> =============== Parameters ===============\n",
      "{'C': 1,\n",
      " 'class_weight': None,\n",
      " 'dual': False,\n",
      " 'fit_intercept': True,\n",
      " 'intercept_scaling': 1,\n",
      " 'l1_ratio': None,\n",
      " 'max_iter': 10000,\n",
      " 'multi_class': 'auto',\n",
      " 'n_jobs': None,\n",
      " 'penalty': 'l2',\n",
      " 'random_state': None,\n",
      " 'solver': 'lbfgs',\n",
      " 'tol': 0.0001,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n",
      "2023-11-18 17:09:00 - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-18 17:09:00 - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     136286      15862       4157       1898        628\n",
      "actual_2      49206     161396      42542       5088       1656\n",
      "actual_3        877      16050      74437      20973       3540\n",
      "actual_4         83        378       7511      19588       8357\n",
      "actual_5         40         74        429       3411      13684\n",
      "2023-11-18 17:09:01 - INFO >>> Model Performance CSV is exported at model/model_performance.csv\n",
      "2023-11-18 17:09:01 - INFO >>> =============== Saving Model ===============\n",
      "2023-11-18 17:09:01 - INFO >>> Model (logit_v4) saved at model/logit_v4.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Drop non-important columns\n",
    "    global col_to_drop\n",
    "    model.init_drop_columns(col_to_drop)\n",
    "        \n",
    "        \n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "    model.init_customer_over_sampling() # Oversampling for class imabalance\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "    \n",
    "    # Instantiate scaler steps\n",
    "    model.init_standard_scaler(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_scaler() # Activate all scalers and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    model_name = \"logit_v4\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = LogisticRegression(penalty = \"l2\", # Avoid Overfitting\n",
    "                                                max_iter=10000,\n",
    "                                                C = 1))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "logit_v4 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384c1be3-8b04-4243-94bc-13b38a4178c3",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment:</b></font>\n",
    "    \n",
    "- From the above results for logistic regression with oversampling, we obtained:\n",
    "|Model|Column Dropped|Best Params / Params|training F1|testing F1|\n",
    "|---|---|---|:---:|:---:|\n",
    "|`Logistic Regression` (Oversampled)|`None`|`{'C': 1}`|`68.76%`|`69.26%`|\n",
    "|`Logistic Regression` (Oversampled)|`['fuel_M85', 'fuel_Lpg', 'fuel_Hydrogen']`|`{'C': 1}`|`68.89%`|`69.29%`|\n",
    "\n",
    "- There is a slightly increase in both train and test score after dropping some non-important columns.\n",
    "    \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e4c76a-d1c2-4c71-8248-aa89998b1b1a",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment:</b></font>\n",
    "\n",
    "- From the above confusion matrix result, it seems that `Oversampling` does not help much with Logistic Regression.\n",
    "- However, dropping some non-important columns (`['fuel_M85', 'fuel_Lpg', 'fuel_Hydrogen']`) help improving the model performance.\n",
    "- Let's apply `grid search` for the best parameters as well.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76efddbc-9699-4451-980a-fd3a90d0c3ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Logistic Regression + Grid Search 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "ae31efab-ae1a-4857-a114-3b5b608ab15a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 17:12:06 - INFO >>> =============== Data Shape ===============\n",
      "2023-11-18 17:12:06 - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-18 17:12:06 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:12:06 - INFO >>> \n",
      "2023-11-18 17:12:06 - INFO >>> Dropping columns...\n",
      "2023-11-18 17:12:06 - INFO >>> =============== After Dropping Columns ===============\n",
      "2023-11-18 17:12:06 - INFO >>> X_train: (1764450, 18) | y_train: (1764450,)\n",
      "2023-11-18 17:12:06 - INFO >>> X_test : (588151, 18) | y_test : (588151,)\n",
      "2023-11-18 17:12:06 - INFO >>> \n",
      "2023-11-18 17:12:06 - INFO >>> Sub-sampling on Train Data...\n",
      "2023-11-18 17:12:08 - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 02 seconds\n",
      "2023-11-18 17:12:08 - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-18 17:12:08 - INFO >>> X_train: (176445, 18) | y_train: (176445,)\n",
      "2023-11-18 17:12:08 - INFO >>> X_test : (588151, 18) | y_test : (588151,)\n",
      "2023-11-18 17:12:08 - INFO >>> \n",
      "2023-11-18 17:12:08 - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-18 17:12:08 - INFO >>> \n",
      "2023-11-18 17:12:08 - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-18 17:12:08 - INFO >>> \n",
      "2023-11-18 17:12:08 - INFO >>> Standard Scaler Initialization Completed\n",
      "2023-11-18 17:12:08 - INFO >>> \n",
      "2023-11-18 17:12:08 - INFO >>> LogisticRegression (name: logit_v5) Initialization Completed\n",
      "2023-11-18 17:12:08 - INFO >>> \n",
      "2023-11-18 17:12:08 - INFO >>> Grid Search with 5-folds cross-validation Initialized Completed\n",
      "2023-11-18 17:12:08 - INFO >>> \n",
      "2023-11-18 17:12:08 - INFO >>> Model Fitting with Grid Search...\n",
      "2023-11-18 17:19:18 - INFO >>> Grid Search Cross Validationn Results Exported.\n",
      "2023-11-18 17:19:18 - INFO >>> Total Fitting Time:\t0 hours 07 minutes 09 seconds\n",
      "2023-11-18 17:19:18 - INFO >>> \n",
      "2023-11-18 17:19:18 - INFO >>> Predicting on Test Data...\n",
      "2023-11-18 17:19:22 - INFO >>> Total Predicting Time:\t0 hours 00 minutes 04 seconds\n",
      "2023-11-18 17:19:22 - INFO >>> \n",
      "2023-11-18 17:19:22 - INFO >>> Best Train Score: 0.7165932164951866\n",
      "2023-11-18 17:19:22 - INFO >>> Test Score: 0.7174754201235759\n",
      "2023-11-18 17:19:22 - INFO >>> =============== Parameters ===============\n",
      "{'logit_v5__C': 0.1}\n",
      "2023-11-18 17:19:22 - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-18 17:19:23 - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     124682      30105       3337        554        153\n",
      "actual_2      26760     209508      21874       1403        343\n",
      "actual_3        395      36987      73493       3916       1086\n",
      "actual_4         65       1736      22195       9080       2841\n",
      "actual_5         34        287       3249       5580       8488\n",
      "2023-11-18 17:19:23 - INFO >>> Model Performance CSV is exported at model/model_performance.csv\n",
      "2023-11-18 17:19:23 - INFO >>> =============== Saving Model ===============\n",
      "2023-11-18 17:19:23 - INFO >>> Model (logit_v5) saved at model/logit_v5.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Drop non-important columns\n",
    "    model.init_drop_columns([\"fuel_M85\", \"fuel_Lpg\", \"fuel_Hydrogen\"])\n",
    "    \n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    model.init_standard_scaler(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_scaler() # Activate all scalers and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    model_name = \"logit_v5\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = LogisticRegression(penalty = \"l2\", # Avoid Overfitting\n",
    "                                                max_iter=10000))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Set K-Fold CV\n",
    "    model.set_kfold_cv(5)\n",
    "    \n",
    "    # Instantiate the grid search\n",
    "    model.init_grid_search(params={f\"{model_name}__C\":[10 ** i for i in [-3,-1,0,1,3]],\n",
    "                                  },\n",
    "                           n_job = 3\n",
    "                          )\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "logit_v5 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96d1e23-89e7-425c-af7e-f68ee0c61d34",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Logistic Regression Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23296607-1079-4c36-b154-c36311464b50",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment - Logistic Regression</b></font>\n",
    "\n",
    "\n",
    "- From the above results for logistic regression, we obtained:\n",
    "|Model|Columns Dropped|Best Params / Params|training F1|testing F1|\n",
    "|---|---|---|:---:|:---:|\n",
    "|`Logistic Regression`|`['fuel_M85', 'fuel_Lpg', 'fuel_Hydrogen']`|`{'C': 1}`|`71.96%`|`71.76%`|\n",
    "|`Logistic Regression` (Oversampling)|`['fuel_M85', 'fuel_Lpg', 'fuel_Hydrogen']`|`{'C': 1}`|`68.89%`|`69.29%`|\n",
    "|`Logistic Regression` (5-fold Grid Search CV)|`['fuel_M85', 'fuel_Lpg', 'fuel_Hydrogen']`|`{'C': 0.1}`|`71.66%`|`71.75%`|\n",
    "\n",
    "\n",
    "- Seems that OverSampling method does not work well in our case.    \n",
    "- While the first model give us a general insight on how well a logistic model can perform on our task. We should alway use cross validation to obtain a more accurate score for model evaluation.\n",
    "- For model comparison between different models, we will use 5-fold CV to ensure our model is appropriately validated.\n",
    "- Therefore, we will use the third model (`Logistic Regression with 5-fold Grid Search CV`) as our baseline model.\n",
    "\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c028979-b100-4ac8-9d65-0eb91c6417ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-model\">\n",
    "    <h3> Model - Decision Tree </h3>\n",
    "</a>\n",
    "    \n",
    "Other than Logistics Regression, we can also try to use Decision Tree.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcaeb59-87ef-4209-bca2-3092f98c6aa5",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>‚ö†Ô∏è Note:</b></font>\n",
    "\n",
    "- For Decision Tree, we can skip the scaling the data as decision tree optimization is not based on distance of value.\n",
    "    \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf23501-5b4b-487d-a919-61cc96c82315",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "6190c67a-fdaa-456a-aa00-15eb28dd3f98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 17:26:16 - INFO >>> =============== Data Shape ===============\n",
      "2023-11-18 17:26:16 - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-18 17:26:16 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:26:16 - INFO >>> \n",
      "2023-11-18 17:26:16 - INFO >>> Sub-sampling on Train Data...\n",
      "2023-11-18 17:26:18 - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 02 seconds\n",
      "2023-11-18 17:26:18 - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-18 17:26:18 - INFO >>> X_train: (176445, 21) | y_train: (176445,)\n",
      "2023-11-18 17:26:18 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:26:18 - INFO >>> \n",
      "2023-11-18 17:26:18 - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-18 17:26:18 - INFO >>> \n",
      "2023-11-18 17:26:18 - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-18 17:26:18 - INFO >>> \n",
      "2023-11-18 17:26:18 - INFO >>> DecisionTreeClassifier (name: dtc_v1) Initialization Completed\n",
      "2023-11-18 17:26:18 - INFO >>> \n",
      "2023-11-18 17:26:18 - INFO >>> Model Fitting...\n",
      "2023-11-18 17:26:20 - INFO >>> Total Fitting Time:\t0 hours 00 minutes 02 seconds\n",
      "2023-11-18 17:26:20 - INFO >>> \n",
      "2023-11-18 17:26:20 - INFO >>> Predicting on Test Data...\n",
      "2023-11-18 17:26:24 - INFO >>> Total Predicting Time:\t0 hours 00 minutes 03 seconds\n",
      "2023-11-18 17:26:24 - INFO >>> \n",
      "2023-11-18 17:26:24 - INFO >>> Best Train Score: 0.8447015147376263\n",
      "2023-11-18 17:26:24 - INFO >>> Test Score: 0.8245545259043304\n",
      "2023-11-18 17:26:24 - INFO >>> =============== Parameters ===============\n",
      "{'ccp_alpha': 0.0,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 25,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 100,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'random_state': None,\n",
      " 'splitter': 'best'}\n",
      "2023-11-18 17:26:24 - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-18 17:26:24 - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     137713      20913        178          7         20\n",
      "actual_2      18716     224584      16394        127         67\n",
      "actual_3        351      19036      89254       6799        437\n",
      "actual_4         68        657      10600      21870       2722\n",
      "actual_5         56        229        994       4447      11912\n",
      "2023-11-18 17:26:24 - INFO >>> Model Performance CSV is exported at model/model_performance.csv\n",
      "2023-11-18 17:26:24 - INFO >>> =============== Saving Model ===============\n",
      "2023-11-18 17:26:24 - INFO >>> Model (dtc_v1) saved at model/dtc_v1.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    this_model_name = \"dtc_v1\"\n",
    "    model.init_model(name = this_model_name,\n",
    "                     model = DecisionTreeClassifier(max_depth = 25,\n",
    "                                                    min_samples_split = 100))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "dtc_v1 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5572cb51-f407-4d26-9609-ffb9c3a618ac",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment:</b></font>\n",
    "\n",
    "- Let's try to apply oversampling to see if it works with Decision Tree.\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca4864a-00f3-4149-b519-fdd2abb92202",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Decision Tree + Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "2bb995cf-05e2-4162-aa6f-e3e7c008dabb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 17:26:25 - INFO >>> =============== Data Shape ===============\n",
      "2023-11-18 17:26:25 - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-18 17:26:25 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:26:25 - INFO >>> \n",
      "2023-11-18 17:26:25 - INFO >>> Sub-sampling on Train Data...\n",
      "2023-11-18 17:26:27 - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 02 seconds\n",
      "2023-11-18 17:26:27 - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-18 17:26:27 - INFO >>> X_train: (176445, 21) | y_train: (176445,)\n",
      "2023-11-18 17:26:27 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:26:27 - INFO >>> \n",
      "2023-11-18 17:26:27 - INFO >>> Custom Over Samping on Train Data...\n",
      "2023-11-18 17:26:27 - INFO >>> Custom Over Samping Completed | Time elapsed: \t0 hours 00 minutes 00 seconds\n",
      "2023-11-18 17:26:27 - INFO >>> =============== After Custom Over Samping ===============\n",
      "2023-11-18 17:26:27 - INFO >>> X_train: (389830, 21) | y_train: (389830,)\n",
      "2023-11-18 17:26:27 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:26:27 - INFO >>> \n",
      "2023-11-18 17:26:27 - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-18 17:26:27 - INFO >>> \n",
      "2023-11-18 17:26:27 - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-18 17:26:27 - INFO >>> \n",
      "2023-11-18 17:26:27 - INFO >>> DecisionTreeClassifier (name: dtc_v2) Initialization Completed\n",
      "2023-11-18 17:26:27 - INFO >>> \n",
      "2023-11-18 17:26:27 - INFO >>> Model Fitting...\n",
      "2023-11-18 17:26:32 - INFO >>> Total Fitting Time:\t0 hours 00 minutes 04 seconds\n",
      "2023-11-18 17:26:32 - INFO >>> \n",
      "2023-11-18 17:26:32 - INFO >>> Predicting on Test Data...\n",
      "2023-11-18 17:26:35 - INFO >>> Total Predicting Time:\t0 hours 00 minutes 03 seconds\n",
      "2023-11-18 17:26:35 - INFO >>> \n",
      "2023-11-18 17:26:35 - INFO >>> Best Train Score: 0.8727459547660219\n",
      "2023-11-18 17:26:36 - INFO >>> Test Score: 0.8162164580767767\n",
      "2023-11-18 17:26:36 - INFO >>> =============== Parameters ===============\n",
      "{'ccp_alpha': 0.0,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 25,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 100,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'random_state': None,\n",
      " 'splitter': 'best'}\n",
      "2023-11-18 17:26:36 - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-18 17:26:36 - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     143216      14876        401        147        191\n",
      "actual_2      25090     208308      24857       1238        395\n",
      "actual_3        382      11984      88326      14234        951\n",
      "actual_4         69        279       6136      25170       4263\n",
      "actual_5         53         81        297       3137      14070\n",
      "2023-11-18 17:26:36 - INFO >>> Model Performance CSV is exported at model/model_performance.csv\n",
      "2023-11-18 17:26:36 - INFO >>> =============== Saving Model ===============\n",
      "2023-11-18 17:26:36 - INFO >>> Model (dtc_v2) saved at model/dtc_v2.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "    model.init_customer_over_sampling() # Oversampling for class imabalance\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    this_model_name = \"dtc_v2\"\n",
    "    model.init_model(name = this_model_name,\n",
    "                     model = DecisionTreeClassifier(max_depth = 25,\n",
    "                                                    min_samples_split = 100))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "dtc_v2 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f831e5-8566-41f0-9fbd-e39921019fac",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment:</b></font>\n",
    "\n",
    "- It seems oversampling does not improve the performance when working with Decision Tree.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04133db0-98a1-489e-8192-6da2c54c703c",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üß† Idea:</b></font>\n",
    "\n",
    "What about if we try to use PCA to selct the perform feature selection?\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225579b2-a7e9-4c58-a91a-fd6726940f1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Decision Tree + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "1bc6e54d-35ac-4201-8e33-eb8d31027536",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 17:29:23 - INFO >>> =============== Data Shape ===============\n",
      "2023-11-18 17:29:23 - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-18 17:29:23 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:29:23 - INFO >>> \n",
      "2023-11-18 17:29:23 - INFO >>> Sub-sampling on Train Data...\n",
      "2023-11-18 17:29:25 - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 02 seconds\n",
      "2023-11-18 17:29:25 - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-18 17:29:25 - INFO >>> X_train: (176445, 21) | y_train: (176445,)\n",
      "2023-11-18 17:29:25 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:29:25 - INFO >>> \n",
      "2023-11-18 17:29:25 - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-18 17:29:25 - INFO >>> \n",
      "2023-11-18 17:29:25 - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-18 17:29:25 - INFO >>> \n",
      "2023-11-18 17:29:25 - INFO >>> PCA Initialization Completed\n",
      "2023-11-18 17:29:25 - INFO >>> \n",
      "2023-11-18 17:29:25 - INFO >>> DecisionTreeClassifier (name: dtc_v3) Initialization Completed\n",
      "2023-11-18 17:29:25 - INFO >>> \n",
      "2023-11-18 17:29:25 - INFO >>> Model Fitting...\n",
      "2023-11-18 17:29:28 - INFO >>> Total Fitting Time:\t0 hours 00 minutes 03 seconds\n",
      "2023-11-18 17:29:28 - INFO >>> \n",
      "2023-11-18 17:29:28 - INFO >>> Predicting on Test Data...\n",
      "2023-11-18 17:29:31 - INFO >>> Total Predicting Time:\t0 hours 00 minutes 03 seconds\n",
      "2023-11-18 17:29:31 - INFO >>> \n",
      "2023-11-18 17:29:31 - INFO >>> Best Train Score: 0.8326082812266538\n",
      "2023-11-18 17:29:32 - INFO >>> Test Score: 0.8053202216721514\n",
      "2023-11-18 17:29:32 - INFO >>> =============== Parameters ===============\n",
      "{'ccp_alpha': 0.0,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 25,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 100,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'random_state': None,\n",
      " 'splitter': 'best'}\n",
      "2023-11-18 17:29:32 - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-18 17:29:32 - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     136609      22035        140         12         35\n",
      "actual_2      20832     221572      17029        286        169\n",
      "actual_3        414      22443      85042       7309        669\n",
      "actual_4        101        946      12169      19757       2944\n",
      "actual_5         77        316       1664       4194      11387\n",
      "2023-11-18 17:29:32 - INFO >>> Model Performance CSV is exported at model/model_performance.csv\n",
      "2023-11-18 17:29:32 - INFO >>> =============== Saving Model ===============\n",
      "2023-11-18 17:29:32 - INFO >>> Model (dtc_v3) saved at model/dtc_v3.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # # Instantiate PCA step\n",
    "    pca_name = \"pca\"\n",
    "    model.init_pca(name=pca_name,n_components=0.95)\n",
    "\n",
    "    # Instantiate model\n",
    "    model_name = \"dtc_v3\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = DecisionTreeClassifier(max_depth = 25,\n",
    "                                                    min_samples_split = 100))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "    \n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "dtc_v3 = model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ad2c6f-eb51-46aa-845b-45b7a0885aab",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment - Decision Tree</b></font>\n",
    "    \n",
    "- From the above results for Decision Tree, we obtained:\n",
    "|Model|Params|training F1|testing F1|\n",
    "|---|---|:---:|:---:|\n",
    "|`Decision Tree` |`{'max_depth': 25, 'min_samples_split': 100}`|`84.47%`|`82.46%`|\n",
    "|`Decision Tree` (Oversampling)|`{'max_depth': 25, 'min_samples_split': 100}`|`87.27%`|`81.62%`|\n",
    "|`Decision Tree` (PCA)|`{'max_depth': 25, 'min_samples_split': 100, 'n_components': 0.95}`|`83.26%`|`80.53%`|\n",
    "\n",
    "\n",
    "- Recall that our baseline model (Logistic Regression) return weighted f1 score of around 70%, decision tree seems works better with our data.\n",
    "- Although oversampling improve a lot in terms of training score, it does not improve the testing score.\n",
    "- For PCA, it does not helping us to improve our model. Maybe our data is already cleaned that any PCA will only remove useful informaiton from our data.\n",
    "    \n",
    "- Let's apply Cross Validation as well.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac363e-9b32-4173-8c24-f05a6a97e32a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Decision Tree (5-fold CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "f7743260-e7a9-444b-8410-241ae90bc2bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 17:30:56 - INFO >>> =============== Data Shape ===============\n",
      "2023-11-18 17:30:56 - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-18 17:30:56 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:30:56 - INFO >>> \n",
      "2023-11-18 17:30:56 - INFO >>> Sub-sampling on Train Data...\n",
      "2023-11-18 17:30:58 - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 02 seconds\n",
      "2023-11-18 17:30:58 - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-18 17:30:58 - INFO >>> X_train: (176445, 21) | y_train: (176445,)\n",
      "2023-11-18 17:30:58 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:30:58 - INFO >>> \n",
      "2023-11-18 17:30:58 - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-18 17:30:58 - INFO >>> \n",
      "2023-11-18 17:30:58 - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-18 17:30:58 - INFO >>> \n",
      "2023-11-18 17:30:58 - INFO >>> DecisionTreeClassifier (name: dtc_v4) Initialization Completed\n",
      "2023-11-18 17:30:58 - INFO >>> \n",
      "2023-11-18 17:30:59 - INFO >>> Grid Search with 5-folds cross-validation Initialized Completed\n",
      "2023-11-18 17:30:59 - INFO >>> \n",
      "2023-11-18 17:30:59 - INFO >>> Model Fitting with Grid Search...\n",
      "2023-11-18 17:31:45 - INFO >>> Grid Search Cross Validationn Results Exported.\n",
      "2023-11-18 17:31:45 - INFO >>> Total Fitting Time:\t0 hours 00 minutes 46 seconds\n",
      "2023-11-18 17:31:45 - INFO >>> \n",
      "2023-11-18 17:31:45 - INFO >>> Predicting on Test Data...\n",
      "2023-11-18 17:31:48 - INFO >>> Total Predicting Time:\t0 hours 00 minutes 03 seconds\n",
      "2023-11-18 17:31:48 - INFO >>> \n",
      "2023-11-18 17:31:48 - INFO >>> Best Train Score: 0.8216465318707055\n",
      "2023-11-18 17:31:48 - INFO >>> Test Score: 0.8280389837142368\n",
      "2023-11-18 17:31:48 - INFO >>> =============== Parameters ===============\n",
      "{'dtc_v4__max_depth': 25, 'dtc_v4__min_samples_split': 50}\n",
      "2023-11-18 17:31:48 - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-18 17:31:48 - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     138451      20121        230         15         14\n",
      "actual_2      19719     224352      15515        230         72\n",
      "actual_3        340      18924      89639       6546        428\n",
      "actual_4         61        674      10045      22217       2920\n",
      "actual_5         58        189        707       3956      12728\n",
      "2023-11-18 17:31:49 - INFO >>> Model Performance CSV is exported at model/model_performance.csv\n",
      "2023-11-18 17:31:49 - INFO >>> =============== Saving Model ===============\n",
      "2023-11-18 17:31:49 - INFO >>> Model (dtc_v4) saved at model/dtc_v4.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    this_model_name = \"dtc_v4\"\n",
    "    model.init_model(name = this_model_name,\n",
    "                     model = DecisionTreeClassifier())\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "    \n",
    "    # Set K-Fold CV\n",
    "    model.set_kfold_cv(5)\n",
    "    \n",
    "    # Instantiate the grid search\n",
    "    model.init_grid_search(params={f\"{this_model_name}__max_depth\":[5,10,25,50],\n",
    "                                   f\"{this_model_name}__min_samples_split\":[10,25,50,100]\n",
    "                                  },\n",
    "                           n_job = 3\n",
    "                          )\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "dtc_v4 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9049d82-19cf-4635-9857-5d79e6b548a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Decision Tree + Oversampling (5-fold CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "7c86fa25-3c09-4921-8140-d0023df67017",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 17:32:55 - INFO >>> =============== Data Shape ===============\n",
      "2023-11-18 17:32:55 - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-18 17:32:55 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:32:55 - INFO >>> \n",
      "2023-11-18 17:32:55 - INFO >>> Sub-sampling on Train Data...\n",
      "2023-11-18 17:32:57 - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 02 seconds\n",
      "2023-11-18 17:32:57 - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-18 17:32:57 - INFO >>> X_train: (176445, 21) | y_train: (176445,)\n",
      "2023-11-18 17:32:57 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:32:57 - INFO >>> \n",
      "2023-11-18 17:32:57 - INFO >>> Custom Over Samping on Train Data...\n",
      "2023-11-18 17:32:58 - INFO >>> Custom Over Samping Completed | Time elapsed: \t0 hours 00 minutes 00 seconds\n",
      "2023-11-18 17:32:58 - INFO >>> =============== After Custom Over Samping ===============\n",
      "2023-11-18 17:32:58 - INFO >>> X_train: (389830, 21) | y_train: (389830,)\n",
      "2023-11-18 17:32:58 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:32:58 - INFO >>> \n",
      "2023-11-18 17:32:58 - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-18 17:32:58 - INFO >>> \n",
      "2023-11-18 17:32:58 - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-18 17:32:58 - INFO >>> \n",
      "2023-11-18 17:32:58 - INFO >>> DecisionTreeClassifier (name: dtc_v5) Initialization Completed\n",
      "2023-11-18 17:32:58 - INFO >>> \n",
      "2023-11-18 17:32:58 - INFO >>> Grid Search with 5-folds cross-validation Initialized Completed\n",
      "2023-11-18 17:32:58 - INFO >>> \n",
      "2023-11-18 17:32:58 - INFO >>> Model Fitting with Grid Search...\n",
      "2023-11-18 17:34:31 - INFO >>> Grid Search Cross Validationn Results Exported.\n",
      "2023-11-18 17:34:31 - INFO >>> Total Fitting Time:\t0 hours 01 minutes 33 seconds\n",
      "2023-11-18 17:34:31 - INFO >>> \n",
      "2023-11-18 17:34:31 - INFO >>> Predicting on Test Data...\n",
      "2023-11-18 17:34:35 - INFO >>> Total Predicting Time:\t0 hours 00 minutes 03 seconds\n",
      "2023-11-18 17:34:35 - INFO >>> \n",
      "2023-11-18 17:34:35 - INFO >>> Best Train Score: 0.9133162691766301\n",
      "2023-11-18 17:34:35 - INFO >>> Test Score: 0.8086285171630496\n",
      "2023-11-18 17:34:35 - INFO >>> =============== Parameters ===============\n",
      "{'dtc_v5__max_depth': 50, 'dtc_v5__min_samples_split': 10}\n",
      "2023-11-18 17:34:35 - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-18 17:34:35 - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     140169      18087        398        120         57\n",
      "actual_2      25276     210964      22727        793        128\n",
      "actual_3        403      16127      87370      11426        551\n",
      "actual_4         72        388       8684      23309       3464\n",
      "actual_5         58        103        481       3645      13351\n",
      "2023-11-18 17:34:35 - INFO >>> Model Performance CSV is exported at model/model_performance.csv\n",
      "2023-11-18 17:34:35 - INFO >>> =============== Saving Model ===============\n",
      "2023-11-18 17:34:36 - INFO >>> Model (dtc_v5) saved at model/dtc_v5.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "    model.init_customer_over_sampling() # Oversampling for class imabalance\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    this_model_name = \"dtc_v5\"\n",
    "    model.init_model(name = this_model_name,\n",
    "                     model = DecisionTreeClassifier(max_depth = 25,\n",
    "                                                    min_samples_split = 100))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "    \n",
    "    # Set K-Fold CV\n",
    "    model.set_kfold_cv(5)\n",
    "    \n",
    "    # Instantiate the grid search\n",
    "    model.init_grid_search(params={f\"{this_model_name}__max_depth\":[5,10,25,50],\n",
    "                                   f\"{this_model_name}__min_samples_split\":[10,25,50,100]\n",
    "                                  },\n",
    "                           n_job = 3\n",
    "                          )\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "dtc_v5 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce872a3-764e-47c4-874d-039ef7e9b30a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Decision Tree Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9f9a52-2c8d-401c-8c07-0bfc1934c222",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment - Decision Tree</b></font>\n",
    "    \n",
    "- From all the above results for Decision Tree, we obtained:\n",
    "|Model|Best Params / Params|training F1|testing F1|\n",
    "|---|---|:---:|:---:|\n",
    "|`Decision Tree` |`{'max_depth': 25, 'min_samples_split': 100}`|`84.47%`|`82.46%`|\n",
    "|`Decision Tree` (Oversampling)|`{'max_depth': 25, 'min_samples_split': 100}`|`87.27%`|`81.62%`|\n",
    "|`Decision Tree` (PCA)|`{'max_depth': 25, 'min_samples_split': 100, 'n_components': 0.95}`|`83.26%`|`80.53%`|\n",
    "|`Decision Tree` (5-fold CV)|`{'max_depth': 25, 'min_samples_split': 50}`|`82.16%`|`82.80%`|\n",
    "|`Decision Tree` (Oversampling & 5-fold CV)|`{'max_depth': 50, 'min_samples_split': 10}`|`91.33%`|`80.86%`|\n",
    "\n",
    "\n",
    "- Recall that our baseline model (Logistic Regression) return weighted f1 score of around 70%, decision tree seems works better with our data.\n",
    "- Although oversampling improve a lot in terms of training score, it does not improve the testing score.\n",
    "- We will use `{'dtc_v5__max_depth': 25, 'dtc_v5__min_samples_split': 100}` for boosting and random forest below.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a90c5a-1f50-42a9-a398-7a0b315099ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-model\">\n",
    "    <h3> Model - Adaptive Boosting</h3>\n",
    "</a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c7407-4510-4f23-9c96-65b468673ce6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "f3b3a25a-59da-4b53-a5a3-c518685bcb4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 17:42:53 - INFO >>> =============== Data Shape ===============\n",
      "2023-11-18 17:42:53 - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-18 17:42:53 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:42:53 - INFO >>> \n",
      "2023-11-18 17:42:53 - INFO >>> Sub-sampling on Train Data...\n",
      "2023-11-18 17:42:55 - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 01 seconds\n",
      "2023-11-18 17:42:55 - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-18 17:42:55 - INFO >>> X_train: (176445, 21) | y_train: (176445,)\n",
      "2023-11-18 17:42:55 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:42:55 - INFO >>> \n",
      "2023-11-18 17:42:55 - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-18 17:42:55 - INFO >>> \n",
      "2023-11-18 17:42:55 - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-18 17:42:55 - INFO >>> \n",
      "2023-11-18 17:42:55 - INFO >>> AdaBoostClassifier (name: abc_v1) Initialization Completed\n",
      "2023-11-18 17:42:55 - INFO >>> \n",
      "2023-11-18 17:42:55 - INFO >>> Model Fitting...\n",
      "2023-11-18 17:45:48 - INFO >>> Total Fitting Time:\t0 hours 02 minutes 53 seconds\n",
      "2023-11-18 17:45:48 - INFO >>> \n",
      "2023-11-18 17:45:48 - INFO >>> Predicting on Test Data...\n",
      "2023-11-18 17:46:32 - INFO >>> Total Predicting Time:\t0 hours 00 minutes 43 seconds\n",
      "2023-11-18 17:46:32 - INFO >>> \n",
      "2023-11-18 17:46:32 - INFO >>> Best Train Score: 0.9033060752072366\n",
      "2023-11-18 17:46:32 - INFO >>> Test Score: 0.8564770024810823\n",
      "2023-11-18 17:46:32 - INFO >>> =============== Parameters ===============\n",
      "{'algorithm': 'SAMME.R',\n",
      " 'base_estimator': 'deprecated',\n",
      " 'estimator': DecisionTreeClassifier(max_depth=25, min_samples_split=50),\n",
      " 'estimator__ccp_alpha': 0.0,\n",
      " 'estimator__class_weight': None,\n",
      " 'estimator__criterion': 'gini',\n",
      " 'estimator__max_depth': 25,\n",
      " 'estimator__max_features': None,\n",
      " 'estimator__max_leaf_nodes': None,\n",
      " 'estimator__min_impurity_decrease': 0.0,\n",
      " 'estimator__min_samples_leaf': 1,\n",
      " 'estimator__min_samples_split': 50,\n",
      " 'estimator__min_weight_fraction_leaf': 0.0,\n",
      " 'estimator__random_state': None,\n",
      " 'estimator__splitter': 'best',\n",
      " 'learning_rate': 1.0,\n",
      " 'n_estimators': 100,\n",
      " 'random_state': None}\n",
      "2023-11-18 17:46:32 - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-18 17:46:32 - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     141161      17550        104         11          5\n",
      "actual_2      14907     232578      12338         50         15\n",
      "actual_3        185      17040      93899       4685         68\n",
      "actual_4         44        371      10278      23419       1805\n",
      "actual_5         38        117        552       3836      13095\n",
      "2023-11-18 17:46:32 - INFO >>> Model Performance CSV is exported at model/model_performance.csv\n",
      "2023-11-18 17:46:32 - INFO >>> =============== Saving Model ===============\n",
      "2023-11-18 17:46:33 - INFO >>> Model (abc_v1) saved at model/abc_v1.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model\n",
    "    model_name = \"abc_v1\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = AdaBoostClassifier(estimator = DecisionTreeClassifier(max_depth = 25,\n",
    "                                                                                   min_samples_split = 50),\n",
    "                                                n_estimators = 100,\n",
    "                                                )\n",
    "                    )\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "    \n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "abc_v1 = model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dec5185-2957-4faf-87e7-00b9fcf4867a",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üß† Idea:</b></font>\n",
    "    \n",
    "- We know that there are class imbalance issue. \n",
    "- Let's try different sampling method.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0573012a-c216-465a-86c9-954c3dd1fc44",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adaboost + Over Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "db1ac2f4-863a-4379-bc41-03ed7df21489",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 17:46:33 - INFO >>> =============== Data Shape ===============\n",
      "2023-11-18 17:46:33 - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-18 17:46:33 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:46:33 - INFO >>> \n",
      "2023-11-18 17:46:33 - INFO >>> Sub-sampling on Train Data...\n",
      "2023-11-18 17:46:35 - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 02 seconds\n",
      "2023-11-18 17:46:35 - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-18 17:46:35 - INFO >>> X_train: (176445, 21) | y_train: (176445,)\n",
      "2023-11-18 17:46:35 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:46:35 - INFO >>> \n",
      "2023-11-18 17:46:35 - INFO >>> Random Over Samping on Train Data...\n",
      "2023-11-18 17:46:36 - INFO >>> Random Over Samping Completed | Time elapsed: \t0 hours 00 minutes 00 seconds\n",
      "2023-11-18 17:46:36 - INFO >>> =============== After Random Over Samping ===============\n",
      "2023-11-18 17:46:36 - INFO >>> X_train: (389830, 21) | y_train: (389830,)\n",
      "2023-11-18 17:46:36 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:46:36 - INFO >>> \n",
      "2023-11-18 17:46:36 - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-18 17:46:36 - INFO >>> \n",
      "2023-11-18 17:46:36 - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-18 17:46:36 - INFO >>> \n",
      "2023-11-18 17:46:36 - INFO >>> AdaBoostClassifier (name: abc_v2) Initialization Completed\n",
      "2023-11-18 17:46:36 - INFO >>> \n",
      "2023-11-18 17:46:36 - INFO >>> Model Fitting...\n",
      "2023-11-18 17:49:48 - INFO >>> Total Fitting Time:\t0 hours 03 minutes 12 seconds\n",
      "2023-11-18 17:49:48 - INFO >>> \n",
      "2023-11-18 17:49:48 - INFO >>> Predicting on Test Data...\n",
      "2023-11-18 17:50:15 - INFO >>> Total Predicting Time:\t0 hours 00 minutes 27 seconds\n",
      "2023-11-18 17:50:15 - INFO >>> \n",
      "2023-11-18 17:50:16 - INFO >>> Best Train Score: 0.959988080879792\n",
      "2023-11-18 17:50:16 - INFO >>> Test Score: 0.8514432209887827\n",
      "2023-11-18 17:50:16 - INFO >>> =============== Parameters ===============\n",
      "{'algorithm': 'SAMME.R',\n",
      " 'base_estimator': 'deprecated',\n",
      " 'estimator': DecisionTreeClassifier(max_depth=25, min_samples_split=50),\n",
      " 'estimator__ccp_alpha': 0.0,\n",
      " 'estimator__class_weight': None,\n",
      " 'estimator__criterion': 'gini',\n",
      " 'estimator__max_depth': 25,\n",
      " 'estimator__max_features': None,\n",
      " 'estimator__max_leaf_nodes': None,\n",
      " 'estimator__min_impurity_decrease': 0.0,\n",
      " 'estimator__min_samples_leaf': 1,\n",
      " 'estimator__min_samples_split': 50,\n",
      " 'estimator__min_weight_fraction_leaf': 0.0,\n",
      " 'estimator__random_state': None,\n",
      " 'estimator__splitter': 'best',\n",
      " 'learning_rate': 1.0,\n",
      " 'n_estimators': 50,\n",
      " 'random_state': None}\n",
      "2023-11-18 17:50:16 - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-18 17:50:16 - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     145056      13634        122         14          5\n",
      "actual_2      19822     221569      18371        105         21\n",
      "actual_3        198      12286      95239       8007        147\n",
      "actual_4         45        253       8106      24969       2544\n",
      "actual_5         42         87        402       3332      13775\n",
      "2023-11-18 17:50:17 - INFO >>> Model Performance CSV is exported at model/model_performance.csv\n",
      "2023-11-18 17:50:17 - INFO >>> =============== Saving Model ===============\n",
      "2023-11-18 17:50:17 - INFO >>> Model (abc_v2) saved at model/abc_v2.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "    \n",
    "    model.init_over_sampling() # Oversampling for class imabalance\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model\n",
    "    model_name = \"abc_v2\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = AdaBoostClassifier(estimator = DecisionTreeClassifier(max_depth = 25,\n",
    "                                                                                   min_samples_split = 50),\n",
    "                                                )\n",
    "                    )\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "    \n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "abc_v2 = model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a3f70d-d0df-45da-8511-e84d4d96e988",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adaboost + SMOTENC Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "02ffbfeb-a78a-4326-be9a-1daf8f99b073",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 17:50:18 - INFO >>> =============== Data Shape ===============\n",
      "2023-11-18 17:50:18 - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-18 17:50:18 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:50:18 - INFO >>> \n",
      "2023-11-18 17:50:18 - INFO >>> Sub-sampling on Train Data...\n",
      "2023-11-18 17:50:21 - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 02 seconds\n",
      "2023-11-18 17:50:21 - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-18 17:50:21 - INFO >>> X_train: (176445, 21) | y_train: (176445,)\n",
      "2023-11-18 17:50:21 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:50:21 - INFO >>> \n",
      "2023-11-18 17:50:21 - INFO >>> SMOTENC Over Sampling on Train Data...\n",
      "2023-11-18 17:54:39 - INFO >>> SMOTENC Over Sampling Completed | Time elapsed: \t0 hours 04 minutes 17 seconds\n",
      "2023-11-18 17:54:39 - INFO >>> =============== After SMOTENC Over Sampling ===============\n",
      "2023-11-18 17:54:39 - INFO >>> X_train: (389830, 21) | y_train: (389830,)\n",
      "2023-11-18 17:54:39 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:54:39 - INFO >>> \n",
      "2023-11-18 17:54:39 - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-18 17:54:39 - INFO >>> \n",
      "2023-11-18 17:54:39 - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-18 17:54:39 - INFO >>> \n",
      "2023-11-18 17:54:39 - INFO >>> AdaBoostClassifier (name: abc_v3) Initialization Completed\n",
      "2023-11-18 17:54:39 - INFO >>> \n",
      "2023-11-18 17:54:39 - INFO >>> Model Fitting...\n",
      "2023-11-18 17:58:16 - INFO >>> Total Fitting Time:\t0 hours 03 minutes 37 seconds\n",
      "2023-11-18 17:58:16 - INFO >>> \n",
      "2023-11-18 17:58:16 - INFO >>> Predicting on Test Data...\n",
      "2023-11-18 17:58:39 - INFO >>> Total Predicting Time:\t0 hours 00 minutes 23 seconds\n",
      "2023-11-18 17:58:39 - INFO >>> \n",
      "2023-11-18 17:58:39 - INFO >>> Best Train Score: 0.9229155501159019\n",
      "2023-11-18 17:58:39 - INFO >>> Test Score: 0.8479839222511951\n",
      "2023-11-18 17:58:39 - INFO >>> =============== Parameters ===============\n",
      "{'algorithm': 'SAMME.R',\n",
      " 'base_estimator': 'deprecated',\n",
      " 'estimator': DecisionTreeClassifier(max_depth=25, min_samples_split=100),\n",
      " 'estimator__ccp_alpha': 0.0,\n",
      " 'estimator__class_weight': None,\n",
      " 'estimator__criterion': 'gini',\n",
      " 'estimator__max_depth': 25,\n",
      " 'estimator__max_features': None,\n",
      " 'estimator__max_leaf_nodes': None,\n",
      " 'estimator__min_impurity_decrease': 0.0,\n",
      " 'estimator__min_samples_leaf': 1,\n",
      " 'estimator__min_samples_split': 100,\n",
      " 'estimator__min_weight_fraction_leaf': 0.0,\n",
      " 'estimator__random_state': None,\n",
      " 'estimator__splitter': 'best',\n",
      " 'learning_rate': 1.0,\n",
      " 'n_estimators': 50,\n",
      " 'random_state': None}\n",
      "2023-11-18 17:58:39 - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-18 17:58:39 - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     145580      13074        154         16          7\n",
      "actual_2      21292     219339      19109        121         27\n",
      "actual_3        202      12100      93337      10078        160\n",
      "actual_4         41        247       6690      25926       3013\n",
      "actual_5         39         81        298       2981      14239\n",
      "2023-11-18 17:58:40 - INFO >>> Model Performance CSV is exported at model/model_performance.csv\n",
      "2023-11-18 17:58:40 - INFO >>> =============== Saving Model ===============\n",
      "2023-11-18 17:58:40 - INFO >>> Model (abc_v3) saved at model/abc_v3.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "    \n",
    "    model.init_smotenc() # SMOTENC Oversampling for class imabalance\n",
    "    \n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model\n",
    "    model_name = \"abc_v3\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = AdaBoostClassifier(estimator = DecisionTreeClassifier(max_depth = 25,\n",
    "                                                                                   min_samples_split = 100),\n",
    "                                                )\n",
    "                    )\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()    \n",
    "    \n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "abc_v3 = model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a15d085-ccd8-4260-bf5b-645429735c20",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adaboost + Hybrid Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "b0c2fdd9-709d-42b1-9ec4-0ed83514bcd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 17:58:41 - INFO >>> =============== Data Shape ===============\n",
      "2023-11-18 17:58:41 - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-18 17:58:41 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:58:41 - INFO >>> \n",
      "2023-11-18 17:58:41 - INFO >>> Sub-sampling on Train Data...\n",
      "2023-11-18 17:58:43 - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 02 seconds\n",
      "2023-11-18 17:58:43 - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-18 17:58:43 - INFO >>> X_train: (176445, 21) | y_train: (176445,)\n",
      "2023-11-18 17:58:43 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:58:43 - INFO >>> \n",
      "2023-11-18 17:58:43 - INFO >>> Hybrid Samping on Train Data...\n",
      "2023-11-18 17:58:43 - INFO >>> Hybrid Samping Completed | Time elapsed: \t0 hours 00 minutes 00 seconds\n",
      "2023-11-18 17:58:43 - INFO >>> =============== After Hybrid Samping ===============\n",
      "2023-11-18 17:58:43 - INFO >>> X_train: (173815, 21) | y_train: (173815,)\n",
      "2023-11-18 17:58:43 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 17:58:43 - INFO >>> \n",
      "2023-11-18 17:58:43 - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-18 17:58:43 - INFO >>> \n",
      "2023-11-18 17:58:43 - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-18 17:58:43 - INFO >>> \n",
      "2023-11-18 17:58:43 - INFO >>> AdaBoostClassifier (name: abc_v4) Initialization Completed\n",
      "2023-11-18 17:58:43 - INFO >>> \n",
      "2023-11-18 17:58:43 - INFO >>> Model Fitting...\n",
      "2023-11-18 17:59:58 - INFO >>> Total Fitting Time:\t0 hours 01 minutes 14 seconds\n",
      "2023-11-18 17:59:58 - INFO >>> \n",
      "2023-11-18 17:59:58 - INFO >>> Predicting on Test Data...\n",
      "2023-11-18 18:00:21 - INFO >>> Total Predicting Time:\t0 hours 00 minutes 23 seconds\n",
      "2023-11-18 18:00:21 - INFO >>> \n",
      "2023-11-18 18:00:21 - INFO >>> Best Train Score: 0.9309611298821869\n",
      "2023-11-18 18:00:21 - INFO >>> Test Score: 0.8439356768451748\n",
      "2023-11-18 18:00:21 - INFO >>> =============== Parameters ===============\n",
      "{'algorithm': 'SAMME.R',\n",
      " 'base_estimator': 'deprecated',\n",
      " 'estimator': DecisionTreeClassifier(max_depth=25, min_samples_split=50),\n",
      " 'estimator__ccp_alpha': 0.0,\n",
      " 'estimator__class_weight': None,\n",
      " 'estimator__criterion': 'gini',\n",
      " 'estimator__max_depth': 25,\n",
      " 'estimator__max_features': None,\n",
      " 'estimator__max_leaf_nodes': None,\n",
      " 'estimator__min_impurity_decrease': 0.0,\n",
      " 'estimator__min_samples_leaf': 1,\n",
      " 'estimator__min_samples_split': 50,\n",
      " 'estimator__min_weight_fraction_leaf': 0.0,\n",
      " 'estimator__random_state': None,\n",
      " 'estimator__splitter': 'best',\n",
      " 'learning_rate': 1.0,\n",
      " 'n_estimators': 50,\n",
      " 'random_state': None}\n",
      "2023-11-18 18:00:21 - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-18 18:00:21 - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     146270      12306        230         16          9\n",
      "actual_2      22362     212868      24488        138         32\n",
      "actual_3        223       9520      96753       9247        134\n",
      "actual_4         55        173       7002      26037       2650\n",
      "actual_5         47         56        355       3237      13943\n",
      "2023-11-18 18:00:22 - INFO >>> Model Performance CSV is exported at model/model_performance.csv\n",
      "2023-11-18 18:00:22 - INFO >>> =============== Saving Model ===============\n",
      "2023-11-18 18:00:22 - INFO >>> Model (abc_v4) saved at model/abc_v4.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "    \n",
    "    # Classes with less sample than class 3 will be oversampled\n",
    "    # Classes with more sample than class 3 will be undersampled\n",
    "    model.init_hybrid_sampling(3) # Hybrid Sampling for class imabalance\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model\n",
    "    model_name = \"abc_v4\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = AdaBoostClassifier(estimator = DecisionTreeClassifier(max_depth = 25,\n",
    "                                                                                   min_samples_split = 50),\n",
    "                                                )\n",
    "                    )\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "    \n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "abc_v4 = model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc55a442-9e7a-498e-9c63-6ff13a39a089",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adaboost (5-fold CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "671b9e0d-db92-438f-bfcd-51ab731f1e43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 18:02:34 - INFO >>> =============== Data Shape ===============\n",
      "2023-11-18 18:02:34 - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-18 18:02:34 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 18:02:34 - INFO >>> \n",
      "2023-11-18 18:02:34 - INFO >>> Sub-sampling on Train Data...\n",
      "2023-11-18 18:02:36 - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 02 seconds\n",
      "2023-11-18 18:02:36 - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-18 18:02:36 - INFO >>> X_train: (176445, 21) | y_train: (176445,)\n",
      "2023-11-18 18:02:36 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 18:02:36 - INFO >>> \n",
      "2023-11-18 18:02:36 - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-18 18:02:36 - INFO >>> \n",
      "2023-11-18 18:02:36 - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-18 18:02:36 - INFO >>> \n",
      "2023-11-18 18:02:36 - INFO >>> AdaBoostClassifier (name: abc_v5) Initialization Completed\n",
      "2023-11-18 18:02:36 - INFO >>> \n",
      "2023-11-18 18:02:36 - INFO >>> Grid Search with 5-folds cross-validation Initialized Completed\n",
      "2023-11-18 18:02:36 - INFO >>> \n",
      "2023-11-18 18:02:36 - INFO >>> Model Fitting with Grid Search...\n",
      "2023-11-18 18:22:08 - INFO >>> Grid Search Cross Validationn Results Exported.\n",
      "2023-11-18 18:22:08 - INFO >>> Total Fitting Time:\t0 hours 19 minutes 31 seconds\n",
      "2023-11-18 18:22:08 - INFO >>> \n",
      "2023-11-18 18:22:08 - INFO >>> Predicting on Test Data...\n",
      "2023-11-18 18:22:48 - INFO >>> Total Predicting Time:\t0 hours 00 minutes 40 seconds\n",
      "2023-11-18 18:22:48 - INFO >>> \n",
      "2023-11-18 18:22:48 - INFO >>> Best Train Score: 0.853168040606883\n",
      "2023-11-18 18:22:48 - INFO >>> Test Score: 0.8559523895822082\n",
      "2023-11-18 18:22:48 - INFO >>> =============== Parameters ===============\n",
      "{'abc_v5__learning_rate': 0.9, 'abc_v5__n_estimators': 100}\n",
      "2023-11-18 18:22:48 - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-18 18:22:48 - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     140985      17725        106         12          3\n",
      "actual_2      14884     232045      12897         45         17\n",
      "actual_3        194      16741      94281       4594         67\n",
      "actual_4         45        389      10382      23401       1700\n",
      "actual_5         38        114        573       3815      13098\n",
      "2023-11-18 18:22:49 - INFO >>> Model Performance CSV is exported at model/model_performance.csv\n",
      "2023-11-18 18:22:49 - INFO >>> =============== Saving Model ===============\n",
      "2023-11-18 18:22:49 - INFO >>> Model (abc_v5) saved at model/abc_v5.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model\n",
    "    model_name = \"abc_v5\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = AdaBoostClassifier(estimator = DecisionTreeClassifier(max_depth = 25,\n",
    "                                                                                   min_samples_split = 50),\n",
    "                                                )\n",
    "                    )\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Set K-Fold CV\n",
    "    model.set_kfold_cv(5)\n",
    "    \n",
    "    # Instantiate the grid search\n",
    "    model.init_grid_search(params={f\"{model_name}__n_estimators\":[25,50,100], \n",
    "                                   f\"{model_name}__learning_rate\":[0.9,1],\n",
    "                                  },\n",
    "                           n_job = 3)\n",
    "    \n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "abc_v5 = model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc367c6f-4b87-492a-8bf9-b3b77da8cfe6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adaboost + Over Sampling (5-fold CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "bab6ba5d-9002-43d2-9315-c30db707b763",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 18:34:26 - INFO >>> =============== Data Shape ===============\n",
      "2023-11-18 18:34:26 - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-18 18:34:26 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 18:34:26 - INFO >>> \n",
      "2023-11-18 18:34:26 - INFO >>> Sub-sampling on Train Data...\n",
      "2023-11-18 18:34:28 - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 01 seconds\n",
      "2023-11-18 18:34:28 - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-18 18:34:28 - INFO >>> X_train: (176445, 21) | y_train: (176445,)\n",
      "2023-11-18 18:34:28 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 18:34:28 - INFO >>> \n",
      "2023-11-18 18:34:28 - INFO >>> Random Over Samping on Train Data...\n",
      "2023-11-18 18:34:28 - INFO >>> Random Over Samping Completed | Time elapsed: \t0 hours 00 minutes 00 seconds\n",
      "2023-11-18 18:34:28 - INFO >>> =============== After Random Over Samping ===============\n",
      "2023-11-18 18:34:28 - INFO >>> X_train: (389830, 21) | y_train: (389830,)\n",
      "2023-11-18 18:34:28 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 18:34:28 - INFO >>> \n",
      "2023-11-18 18:34:28 - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-18 18:34:28 - INFO >>> \n",
      "2023-11-18 18:34:28 - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-18 18:34:28 - INFO >>> \n",
      "2023-11-18 18:34:28 - INFO >>> AdaBoostClassifier (name: abc_v6) Initialization Completed\n",
      "2023-11-18 18:34:28 - INFO >>> \n",
      "2023-11-18 18:34:28 - INFO >>> Grid Search with 5-folds cross-validation Initialized Completed\n",
      "2023-11-18 18:34:28 - INFO >>> \n",
      "2023-11-18 18:34:28 - INFO >>> Model Fitting with Grid Search...\n",
      "2023-11-18 19:16:11 - INFO >>> Grid Search Cross Validationn Results Exported.\n",
      "2023-11-18 19:16:11 - INFO >>> Total Fitting Time:\t0 hours 41 minutes 42 seconds\n",
      "2023-11-18 19:16:11 - INFO >>> \n",
      "2023-11-18 19:16:11 - INFO >>> Predicting on Test Data...\n",
      "2023-11-18 19:16:53 - INFO >>> Total Predicting Time:\t0 hours 00 minutes 42 seconds\n",
      "2023-11-18 19:16:53 - INFO >>> \n",
      "2023-11-18 19:16:53 - INFO >>> Best Train Score: 0.9349536275633378\n",
      "2023-11-18 19:16:53 - INFO >>> Test Score: 0.8541083849179941\n",
      "2023-11-18 19:16:53 - INFO >>> =============== Parameters ===============\n",
      "{'abc_v6__learning_rate': 1, 'abc_v6__n_estimators': 100}\n",
      "2023-11-18 19:16:53 - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-18 19:16:54 - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     145169      13508        134         14          6\n",
      "actual_2      19325     222414      18034         93         22\n",
      "actual_3        191      12004      95621       7910        151\n",
      "actual_4         48        255       7935      25151       2528\n",
      "actual_5         45         89        405       3280      13819\n",
      "2023-11-18 19:16:54 - INFO >>> Model Performance CSV is exported at model/model_performance.csv\n",
      "2023-11-18 19:16:54 - INFO >>> =============== Saving Model ===============\n",
      "2023-11-18 19:16:54 - INFO >>> Model (abc_v6) saved at model/abc_v6.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "    \n",
    "    model.init_over_sampling() # Oversampling for class imabalance\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model\n",
    "    model_name = \"abc_v6\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = AdaBoostClassifier(estimator = DecisionTreeClassifier(max_depth = 25,\n",
    "                                                                                   min_samples_split = 50),\n",
    "                                                )\n",
    "                    )\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Set K-Fold CV\n",
    "    model.set_kfold_cv(5)\n",
    "    \n",
    "    # Instantiate the grid search\n",
    "    model.init_grid_search(params={f\"{model_name}__n_estimators\":[25,50,100], \n",
    "                                   f\"{model_name}__learning_rate\":[0.9,1],\n",
    "                                  },\n",
    "                           n_job = 3)\n",
    "    \n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "abc_v6 = model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60fc34-21c4-4572-88b5-75c59ed03da5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Adaboosting Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b740c72-58ae-40f8-bf6b-89c9bf013a46",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment - AdaBoost</b></font>\n",
    "    \n",
    "- For adaboost, the base estimator is `DecisionTreeClassifier(max_depth=25, min_samples_split=50)`\n",
    "- From the above results for Adaptive Boosting, we obtained:\n",
    "|Model|Best Params / Params|training F1|testing F1|\n",
    "|---|---|:---:|:---:|\n",
    "|`Adaboost`|`{'learning_rate': 1.0, 'n_estimators': 100}`|`95.33%`|`85.65%`|\n",
    "|`Adaboost` (Oversampling)|`{'learning_rate': 1.0, 'n_estimators': 100}`|`96.00%`|`85.14%`|\n",
    "|`Adaboost` (SMOTENC)|`{'learning_rate': 1.0, 'n_estimators': 100}`|`92.29%`|`84.80%`|\n",
    "|`Adaboost` (Hybrid)|`{'learning_rate': 1.0, 'n_estimators': 100}`|`93.10%`|`84.39%`|\n",
    "\n",
    "    \n",
    "- From the above results, we can see that most of the sampling methods for tackling class imbalance does not work with our data.\n",
    "- We use the model with top 2 result to train with 5-fold CV.\n",
    "|Model|Best Params / Params|training F1|testing F1|\n",
    "|---|---|:---:|:---:|\n",
    "|`Adaboost` (5-fold CV)|`{'learning_rate': 0.9, 'n_estimators': 100}`|`85.32%`|`85.60%`|\n",
    "|`Adaboost` (5-fold CV & Oversampling)|`{'learning_rate': 1.0, 'n_estimators': 100}`|`93.50%`|`85.41%`|\n",
    "- We can see that `Oversampling` does not help improving the model.\n",
    "    \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7059b8aa-383e-46ee-b695-4732082129d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-model\">\n",
    "    <h3> Model - Stochastic Gradient Boosting</h3>\n",
    "</a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45994550-6d2b-469e-96ba-7148b3f42420",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Stochastic Gradient Boosting (SGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "9f167f7b-02a5-4622-9380-bcf5b9cc9b0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 21:59:12 - INFO >>> =============== Data Shape ===============\n",
      "2023-11-18 21:59:12 - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-18 21:59:12 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 21:59:12 - INFO >>> \n",
      "2023-11-18 21:59:13 - INFO >>> Sub-sampling on Train Data...\n",
      "2023-11-18 21:59:15 - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 02 seconds\n",
      "2023-11-18 21:59:15 - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-18 21:59:15 - INFO >>> X_train: (176445, 21) | y_train: (176445,)\n",
      "2023-11-18 21:59:15 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 21:59:15 - INFO >>> \n",
      "2023-11-18 21:59:15 - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-18 21:59:15 - INFO >>> \n",
      "2023-11-18 21:59:15 - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-18 21:59:15 - INFO >>> \n",
      "2023-11-18 21:59:15 - INFO >>> GradientBoostingClassifier (name: gbc_v1) Initialization Completed\n",
      "2023-11-18 21:59:15 - INFO >>> \n",
      "2023-11-18 21:59:15 - INFO >>> Model Fitting...\n",
      "2023-11-18 22:02:40 - INFO >>> Total Fitting Time:\t0 hours 03 minutes 25 seconds\n",
      "2023-11-18 22:02:40 - INFO >>> \n",
      "2023-11-18 22:02:40 - INFO >>> Predicting on Test Data...\n",
      "2023-11-18 22:03:12 - INFO >>> Total Predicting Time:\t0 hours 00 minutes 32 seconds\n",
      "2023-11-18 22:03:12 - INFO >>> \n",
      "2023-11-18 22:03:12 - INFO >>> Best Train Score: 0.9097360675183417\n",
      "2023-11-18 22:03:13 - INFO >>> Test Score: 0.8574268833355121\n",
      "2023-11-18 22:03:13 - INFO >>> =============== Parameters ===============\n",
      "{'ccp_alpha': 0.0,\n",
      " 'criterion': 'friedman_mse',\n",
      " 'init': None,\n",
      " 'learning_rate': 0.1,\n",
      " 'loss': 'log_loss',\n",
      " 'max_depth': 25,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 50,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 50,\n",
      " 'n_iter_no_change': 3,\n",
      " 'random_state': None,\n",
      " 'subsample': 0.5,\n",
      " 'tol': 0.0001,\n",
      " 'validation_fraction': 0.1,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n",
      "2023-11-18 22:03:13 - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-18 22:03:13 - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     141879      16815        100         21         16\n",
      "actual_2      15751     230791      13220         81         45\n",
      "actual_3        195      15563      94522       5467        130\n",
      "actual_4         54        367       9443      23859       2194\n",
      "actual_5         43         96        458       3524      13517\n",
      "2023-11-18 22:03:13 - INFO >>> Model Performance CSV is exported at model/model_performance.csv\n",
      "2023-11-18 22:03:13 - INFO >>> =============== Saving Model ===============\n",
      "2023-11-18 22:03:13 - INFO >>> Model (gbc_v1) saved at model/gbc_v1.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model\n",
    "    model_name = \"gbc_v1\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = GradientBoostingClassifier(n_estimators = 50,\n",
    "                                                        max_depth = 25,\n",
    "                                                        min_samples_split = 50,\n",
    "                                                        n_iter_no_change = 3,\n",
    "                                                        subsample = 0.5, # Apply Stochastic Gradient Boosting\n",
    "                                                        verbose= 0))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "    \n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "gbc_v1 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5095224-3349-4e4b-9720-2f4e9471ffdb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SGB + Over Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "91428678-a037-420d-ac27-1e867dff1f75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 22:03:14 - INFO >>> =============== Data Shape ===============\n",
      "2023-11-18 22:03:14 - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-18 22:03:14 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 22:03:14 - INFO >>> \n",
      "2023-11-18 22:03:14 - INFO >>> Sub-sampling on Train Data...\n",
      "2023-11-18 22:03:16 - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 02 seconds\n",
      "2023-11-18 22:03:16 - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-18 22:03:16 - INFO >>> X_train: (176445, 21) | y_train: (176445,)\n",
      "2023-11-18 22:03:16 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 22:03:16 - INFO >>> \n",
      "2023-11-18 22:03:16 - INFO >>> Random Over Samping on Train Data...\n",
      "2023-11-18 22:03:17 - INFO >>> Random Over Samping Completed | Time elapsed: \t0 hours 00 minutes 00 seconds\n",
      "2023-11-18 22:03:17 - INFO >>> =============== After Random Over Samping ===============\n",
      "2023-11-18 22:03:17 - INFO >>> X_train: (389830, 21) | y_train: (389830,)\n",
      "2023-11-18 22:03:17 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 22:03:17 - INFO >>> \n",
      "2023-11-18 22:03:17 - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-18 22:03:17 - INFO >>> \n",
      "2023-11-18 22:03:17 - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-18 22:03:17 - INFO >>> \n",
      "2023-11-18 22:03:17 - INFO >>> GradientBoostingClassifier (name: gbc_v2) Initialization Completed\n",
      "2023-11-18 22:03:17 - INFO >>> \n",
      "2023-11-18 22:03:17 - INFO >>> Model Fitting...\n",
      "2023-11-18 22:14:26 - INFO >>> Total Fitting Time:\t0 hours 11 minutes 09 seconds\n",
      "2023-11-18 22:14:26 - INFO >>> \n",
      "2023-11-18 22:14:26 - INFO >>> Predicting on Test Data...\n",
      "2023-11-18 22:15:11 - INFO >>> Total Predicting Time:\t0 hours 00 minutes 44 seconds\n",
      "2023-11-18 22:15:11 - INFO >>> \n",
      "2023-11-18 22:15:11 - INFO >>> Best Train Score: 0.9664776257698687\n",
      "2023-11-18 22:15:11 - INFO >>> Test Score: 0.8517949057182926\n",
      "2023-11-18 22:15:11 - INFO >>> =============== Parameters ===============\n",
      "{'ccp_alpha': 0.0,\n",
      " 'criterion': 'friedman_mse',\n",
      " 'init': None,\n",
      " 'learning_rate': 0.1,\n",
      " 'loss': 'log_loss',\n",
      " 'max_depth': 25,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 50,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 50,\n",
      " 'n_iter_no_change': 3,\n",
      " 'random_state': None,\n",
      " 'subsample': 0.5,\n",
      " 'tol': 0.0001,\n",
      " 'validation_fraction': 0.1,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n",
      "2023-11-18 22:15:11 - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-18 22:15:11 - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     145087      13546        151         30         17\n",
      "actual_2      19712     220768      19161        201         46\n",
      "actual_3        200      11114      94705       9590        268\n",
      "actual_4         43        251       6731      26001       2891\n",
      "actual_5         47         84        281       3186      14040\n",
      "2023-11-18 22:15:12 - INFO >>> Model Performance CSV is exported at model/model_performance.csv\n",
      "2023-11-18 22:15:12 - INFO >>> =============== Saving Model ===============\n",
      "2023-11-18 22:15:13 - INFO >>> Model (gbc_v2) saved at model/gbc_v2.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "    \n",
    "    model.init_over_sampling() # Oversampling for class imabalance\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model\n",
    "    model_name = \"gbc_v2\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = GradientBoostingClassifier(n_estimators = 50,\n",
    "                                                        max_depth = 25,\n",
    "                                                        min_samples_split = 50,\n",
    "                                                        n_iter_no_change = 3,\n",
    "                                                        subsample = 0.5, # Apply Stochastic Gradient Boosting\n",
    "                                                        verbose= 0))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "    \n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "gbc_v2 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a0c7b5-44b2-47c4-bed5-287e1f4949ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SGB + SMOTENC Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "0d372c25-9935-4ccc-a03d-54165cdd510c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 22:15:13 - INFO >>> =============== Data Shape ===============\n",
      "2023-11-18 22:15:13 - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-18 22:15:13 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 22:15:13 - INFO >>> \n",
      "2023-11-18 22:15:13 - INFO >>> Sub-sampling on Train Data...\n",
      "2023-11-18 22:15:15 - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 02 seconds\n",
      "2023-11-18 22:15:15 - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-18 22:15:15 - INFO >>> X_train: (176445, 21) | y_train: (176445,)\n",
      "2023-11-18 22:15:15 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 22:15:15 - INFO >>> \n",
      "2023-11-18 22:15:15 - INFO >>> SMOTENC Over Sampling on Train Data...\n",
      "2023-11-18 22:18:52 - INFO >>> SMOTENC Over Sampling Completed | Time elapsed: \t0 hours 03 minutes 36 seconds\n",
      "2023-11-18 22:18:52 - INFO >>> =============== After SMOTENC Over Sampling ===============\n",
      "2023-11-18 22:18:52 - INFO >>> X_train: (389830, 21) | y_train: (389830,)\n",
      "2023-11-18 22:18:52 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 22:18:52 - INFO >>> \n",
      "2023-11-18 22:18:52 - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-18 22:18:52 - INFO >>> \n",
      "2023-11-18 22:18:52 - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-18 22:18:52 - INFO >>> \n",
      "2023-11-18 22:18:52 - INFO >>> GradientBoostingClassifier (name: gbc_v3) Initialization Completed\n",
      "2023-11-18 22:18:52 - INFO >>> \n",
      "2023-11-18 22:18:52 - INFO >>> Model Fitting...\n",
      "2023-11-18 22:30:05 - INFO >>> Total Fitting Time:\t0 hours 11 minutes 13 seconds\n",
      "2023-11-18 22:30:05 - INFO >>> \n",
      "2023-11-18 22:30:05 - INFO >>> Predicting on Test Data...\n",
      "2023-11-18 22:30:49 - INFO >>> Total Predicting Time:\t0 hours 00 minutes 43 seconds\n",
      "2023-11-18 22:30:49 - INFO >>> \n",
      "2023-11-18 22:30:49 - INFO >>> Best Train Score: 0.9482775602509373\n",
      "2023-11-18 22:30:49 - INFO >>> Test Score: 0.8502584130690947\n",
      "2023-11-18 22:30:49 - INFO >>> =============== Parameters ===============\n",
      "{'ccp_alpha': 0.0,\n",
      " 'criterion': 'friedman_mse',\n",
      " 'init': None,\n",
      " 'learning_rate': 0.1,\n",
      " 'loss': 'log_loss',\n",
      " 'max_depth': 25,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 50,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 50,\n",
      " 'n_iter_no_change': 3,\n",
      " 'random_state': None,\n",
      " 'subsample': 0.5,\n",
      " 'tol': 0.0001,\n",
      " 'validation_fraction': 0.1,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n",
      "2023-11-18 22:30:49 - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-18 22:30:49 - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     145092      13522        172         32         13\n",
      "actual_2      19865     221152      18628        194         49\n",
      "actual_3        221      11594      94045       9763        254\n",
      "actual_4         60        276       7087      25227       3267\n",
      "actual_5         45         65        314       2971      14243\n",
      "2023-11-18 22:30:50 - INFO >>> Model Performance CSV is exported at model/model_performance.csv\n",
      "2023-11-18 22:30:50 - INFO >>> =============== Saving Model ===============\n",
      "2023-11-18 22:30:51 - INFO >>> Model (gbc_v3) saved at model/gbc_v3.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "    \n",
    "    model.init_smotenc() # SMOTENC Oversampling for class imabalance\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model\n",
    "    model_name = \"gbc_v3\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = GradientBoostingClassifier(n_estimators = 50,\n",
    "                                                        max_depth = 25,\n",
    "                                                        min_samples_split = 50,\n",
    "                                                        n_iter_no_change = 3,\n",
    "                                                        subsample = 0.5, # Apply Stochastic Gradient Boosting\n",
    "                                                        verbose= 0))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "    \n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "gbc_v3 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc5582c-e993-4686-b08f-eed8c81c68f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SGB + Hybrid Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "db3f9a91-60df-4d1a-988c-66f157d4bc3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 22:30:52 - INFO >>> =============== Data Shape ===============\n",
      "2023-11-18 22:30:52 - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-18 22:30:52 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 22:30:52 - INFO >>> \n",
      "2023-11-18 22:30:52 - INFO >>> Sub-sampling on Train Data...\n",
      "2023-11-18 22:30:54 - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 02 seconds\n",
      "2023-11-18 22:30:54 - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-18 22:30:54 - INFO >>> X_train: (176445, 21) | y_train: (176445,)\n",
      "2023-11-18 22:30:54 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 22:30:54 - INFO >>> \n",
      "2023-11-18 22:30:54 - INFO >>> Hybrid Samping on Train Data...\n",
      "2023-11-18 22:30:54 - INFO >>> Hybrid Samping Completed | Time elapsed: \t0 hours 00 minutes 00 seconds\n",
      "2023-11-18 22:30:54 - INFO >>> =============== After Hybrid Samping ===============\n",
      "2023-11-18 22:30:54 - INFO >>> X_train: (173815, 21) | y_train: (173815,)\n",
      "2023-11-18 22:30:54 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 22:30:54 - INFO >>> \n",
      "2023-11-18 22:30:54 - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-18 22:30:54 - INFO >>> \n",
      "2023-11-18 22:30:54 - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-18 22:30:54 - INFO >>> \n",
      "2023-11-18 22:30:54 - INFO >>> GradientBoostingClassifier (name: gbc_v4) Initialization Completed\n",
      "2023-11-18 22:30:54 - INFO >>> \n",
      "2023-11-18 22:30:54 - INFO >>> Model Fitting...\n",
      "2023-11-18 22:34:46 - INFO >>> Total Fitting Time:\t0 hours 03 minutes 51 seconds\n",
      "2023-11-18 22:34:46 - INFO >>> \n",
      "2023-11-18 22:34:46 - INFO >>> Predicting on Test Data...\n",
      "2023-11-18 22:35:27 - INFO >>> Total Predicting Time:\t0 hours 00 minutes 40 seconds\n",
      "2023-11-18 22:35:27 - INFO >>> \n",
      "2023-11-18 22:35:27 - INFO >>> Best Train Score: 0.9450736219423767\n",
      "2023-11-18 22:35:27 - INFO >>> Test Score: 0.8439385949191396\n",
      "2023-11-18 22:35:27 - INFO >>> =============== Parameters ===============\n",
      "{'ccp_alpha': 0.0,\n",
      " 'criterion': 'friedman_mse',\n",
      " 'init': None,\n",
      " 'learning_rate': 0.1,\n",
      " 'loss': 'log_loss',\n",
      " 'max_depth': 25,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 50,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 50,\n",
      " 'n_iter_no_change': 3,\n",
      " 'random_state': None,\n",
      " 'subsample': 0.5,\n",
      " 'tol': 0.0001,\n",
      " 'validation_fraction': 0.1,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n",
      "2023-11-18 22:35:27 - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-18 22:35:27 - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     146114      12464        200         37         16\n",
      "actual_2      22314     213238      23989        252         95\n",
      "actual_3        281       9224      95743      10369        260\n",
      "actual_4         57        191       6223      26422       3024\n",
      "actual_5         56         49        279       2976      14278\n",
      "2023-11-18 22:35:27 - INFO >>> Model Performance CSV is exported at model/model_performance.csv\n",
      "2023-11-18 22:35:27 - INFO >>> =============== Saving Model ===============\n",
      "2023-11-18 22:35:28 - INFO >>> Model (gbc_v4) saved at model/gbc_v4.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "    \n",
    "    model.init_hybrid_sampling(3) # Hybrid Sampling for class imabalance\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model\n",
    "    model_name = \"gbc_v4\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = GradientBoostingClassifier(n_estimators = 50,\n",
    "                                                        max_depth = 25,\n",
    "                                                        min_samples_split = 50,\n",
    "                                                        n_iter_no_change = 3,\n",
    "                                                        subsample = 0.5, # Apply Stochastic Gradient Boosting\n",
    "                                                        verbose= 0))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "    \n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "gbc_v4 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d662ceb-6436-499d-980b-64b2691beada",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SGB (5-fold CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "f2accb1f-6081-4dc2-8d6b-2a7b72c87623",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 22:35:28 - INFO >>> =============== Data Shape ===============\n",
      "2023-11-18 22:35:28 - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-18 22:35:28 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 22:35:28 - INFO >>> \n",
      "2023-11-18 22:35:28 - INFO >>> Sub-sampling on Train Data...\n",
      "2023-11-18 22:35:31 - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 02 seconds\n",
      "2023-11-18 22:35:31 - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-18 22:35:31 - INFO >>> X_train: (176445, 21) | y_train: (176445,)\n",
      "2023-11-18 22:35:31 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 22:35:31 - INFO >>> \n",
      "2023-11-18 22:35:31 - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-18 22:35:31 - INFO >>> \n",
      "2023-11-18 22:35:31 - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-18 22:35:31 - INFO >>> \n",
      "2023-11-18 22:35:31 - INFO >>> GradientBoostingClassifier (name: gbc_v5) Initialization Completed\n",
      "2023-11-18 22:35:31 - INFO >>> \n",
      "2023-11-18 22:35:31 - INFO >>> Grid Search with 5-folds cross-validation Initialized Completed\n",
      "2023-11-18 22:35:31 - INFO >>> \n",
      "2023-11-18 22:35:31 - INFO >>> Model Fitting with Grid Search...\n",
      "2023-11-18 23:18:25 - INFO >>> Grid Search Cross Validationn Results Exported.\n",
      "2023-11-18 23:18:25 - INFO >>> Total Fitting Time:\t0 hours 42 minutes 53 seconds\n",
      "2023-11-18 23:18:25 - INFO >>> \n",
      "2023-11-18 23:18:25 - INFO >>> Predicting on Test Data...\n",
      "2023-11-18 23:18:57 - INFO >>> Total Predicting Time:\t0 hours 00 minutes 32 seconds\n",
      "2023-11-18 23:18:57 - INFO >>> \n",
      "2023-11-18 23:18:57 - INFO >>> Best Train Score: 0.8540710203112019\n",
      "2023-11-18 23:18:57 - INFO >>> Test Score: 0.8570286864207576\n",
      "2023-11-18 23:18:57 - INFO >>> =============== Parameters ===============\n",
      "{'gbc_v5__n_estimators': 100, 'gbc_v5__subsample': 0.75}\n",
      "2023-11-18 23:18:57 - INFO >>> Computing Confusion Matrix...\n",
      "2023-11-18 23:18:57 - INFO >>> =============== Confusion Matrix on Test Data ===============\n",
      "          predict_1  predict_2  predict_3  predict_4  predict_5\n",
      "actual_1     141876      16809        115         19         12\n",
      "actual_2      16009     230364      13414         68         33\n",
      "actual_3        213      15436      94702       5383        143\n",
      "actual_4         55        369       9498      23714       2281\n",
      "actual_5         41        103        464       3340      13690\n",
      "2023-11-18 23:18:58 - INFO >>> Model Performance CSV is exported at model/model_performance.csv\n",
      "2023-11-18 23:18:58 - INFO >>> =============== Saving Model ===============\n",
      "2023-11-18 23:18:58 - INFO >>> Model (gbc_v5) saved at model/gbc_v5.joblib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "    \n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model\n",
    "    model_name = \"gbc_v5\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = GradientBoostingClassifier(n_estimators = 50,\n",
    "                                                        max_depth = 25,\n",
    "                                                        min_samples_split = 50,\n",
    "                                                        n_iter_no_change = 3,\n",
    "                                                        subsample = 0.5, # Apply Stochastic Gradient Boosting\n",
    "                                                        verbose= 0))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Set K-Fold CV\n",
    "    model.set_kfold_cv(5)\n",
    "    \n",
    "    # Instantiate the grid search\n",
    "    model.init_grid_search(params={f\"{model_name}__subsample\":[0.25,0.5,0.75],\n",
    "                                   f\"{model_name}__n_estimators\":[25,50,100],\n",
    "                                  },\n",
    "                           n_job = 3)\n",
    "    \n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "gbc_v5 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ccf90e-82ef-4e5e-a6ef-9955865d17bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SGB + Over Sampling (5-fold CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5877af3b-7b60-4c4e-ae5a-14802fb4e204",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 23:18:59 - INFO >>> =============== Data Shape ===============\n",
      "2023-11-18 23:18:59 - INFO >>> X_train: (1764450, 21) | y_train: (1764450,)\n",
      "2023-11-18 23:18:59 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 23:18:59 - INFO >>> \n",
      "2023-11-18 23:18:59 - INFO >>> Sub-sampling on Train Data...\n",
      "2023-11-18 23:19:01 - INFO >>> Subsampling Completed | Time elapsed: \t0 hours 00 minutes 02 seconds\n",
      "2023-11-18 23:19:01 - INFO >>> =============== After Subsampling ===============\n",
      "2023-11-18 23:19:01 - INFO >>> X_train: (176445, 21) | y_train: (176445,)\n",
      "2023-11-18 23:19:01 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 23:19:01 - INFO >>> \n",
      "2023-11-18 23:19:01 - INFO >>> Random Over Samping on Train Data...\n",
      "2023-11-18 23:19:01 - INFO >>> Random Over Samping Completed | Time elapsed: \t0 hours 00 minutes 00 seconds\n",
      "2023-11-18 23:19:01 - INFO >>> =============== After Random Over Samping ===============\n",
      "2023-11-18 23:19:01 - INFO >>> X_train: (389830, 21) | y_train: (389830,)\n",
      "2023-11-18 23:19:01 - INFO >>> X_test : (588151, 21) | y_test : (588151,)\n",
      "2023-11-18 23:19:01 - INFO >>> \n",
      "2023-11-18 23:19:01 - INFO >>> One-hot Encoder Initialization Completed\n",
      "2023-11-18 23:19:01 - INFO >>> \n",
      "2023-11-18 23:19:01 - INFO >>> Target Encoder Initialization Completed\n",
      "2023-11-18 23:19:01 - INFO >>> \n",
      "2023-11-18 23:19:01 - INFO >>> GradientBoostingClassifier (name: gbc_v6) Initialization Completed\n",
      "2023-11-18 23:19:01 - INFO >>> \n",
      "2023-11-18 23:19:01 - INFO >>> Grid Search with 5-folds cross-validation Initialized Completed\n",
      "2023-11-18 23:19:01 - INFO >>> \n",
      "2023-11-18 23:19:01 - INFO >>> Model Fitting with Grid Search...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "    \n",
    "    model.init_over_sampling() # Oversampling for class imabalance\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model\n",
    "    model_name = \"gbc_v6\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = GradientBoostingClassifier(n_estimators = 50,\n",
    "                                                        max_depth = 25,\n",
    "                                                        min_samples_split = 50,\n",
    "                                                        n_iter_no_change = 3,\n",
    "                                                        subsample = 0.5, # Apply Stochastic Gradient Boosting\n",
    "                                                        verbose= 0))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Set K-Fold CV\n",
    "    model.set_kfold_cv(5)\n",
    "    \n",
    "    # Instantiate the grid search\n",
    "    model.init_grid_search(params={f\"{model_name}__subsample\":[0.25,0.5,0.75],\n",
    "                                   f\"{model_name}__n_estimators\":[25,50,100],\n",
    "                                  },\n",
    "                           n_job = 3)\n",
    "    \n",
    "    \n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "gbc_v6 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef22b4c4-dd20-4688-8af4-f5e9f8449311",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SGB Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30ac495-ee85-4f52-9a3c-15dd8b824ba7",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment - Stochastic Gradient Boosting</b></font> \n",
    "    \n",
    "- From the above results for Stochastic Gradient Boosting, we obtained:\n",
    "|Model|Best Params / Params|training F1|testing F1|\n",
    "|---|---|:---:|:---:|\n",
    "|`SGB`|`{'n_estimators': 50,'subsample': 0.5}`|`90.97%`|`85.74%`|\n",
    "|`SGB` (Oversampling)|`{'n_estimators': 50,'subsample': 0.5}`|`96.65%`|`85.18%`|\n",
    "|`SGB` (SMOTENC)|`{'n_estimators': 50,'subsample': 0.5}`|`94.83%`|`85.03%`|\n",
    "|`SGB` (Hybrid)|`{'n_estimators': 50,'subsample': 0.5}`|`94.51%`|`84.39%`|\n",
    "    \n",
    "- From the above results, we can see that most of the sampling methods for tackling class imbalance does not work with our data.\n",
    "- We use the model with top 2 result to train with 5-fold CV.\n",
    "|Model|Best Params / Params|training F1|testing F1|\n",
    "|---|---|:---:|:---:|\n",
    "|`SGB` (5-fold CV)|`{'n_estimators': 100,'subsample': 0.75'}`|`85.41%`|`85.70%`|\n",
    "|`SGB` (5-fold CV & Oversampling)|`{'n_estimators': 50,'subsample': 0.5}`|`_%`|`_%`|\n",
    "    \n",
    "- We can see that `Oversampling` does not help improving the model.\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6347de93-c0d2-4443-9921-4186a5c479ba",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-model\">\n",
    "    <h3> Model - XGBoost </h3>\n",
    "</a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ec0ad-43ad-4333-9bea-98a4ca685522",
   "metadata": {},
   "source": [
    "#### XG Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f442ec-76b1-49a3-aac5-45a574cdbd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    model_name = \"xgb_v1\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = xgb.XGBClassifier(n_estimators = 100,max_depth = 25))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "xgb_v1 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acbb270-95e2-4ab5-abf2-f47435c09c63",
   "metadata": {},
   "source": [
    "#### XG Boosting + Over Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d17433-8364-48e0-b724-014417c145a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "    \n",
    "    model.init_over_sampling() # Oversampling for class imabalance\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    model_name = \"xgb_v2\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = xgb.XGBClassifier(n_estimators = 100,max_depth = 25))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "xgb_v2 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7dcaf8-db06-4973-8a66-a4572c05b895",
   "metadata": {},
   "source": [
    "#### XG Boosting + SMOTENC Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b7ba84-43c5-4cd2-bb1f-59d84eee6dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "    \n",
    "    model.init_smotenc() # SMOTENC Oversampling for class imabalance\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    model_name = \"xgb_v3\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = xgb.XGBClassifier(n_estimators = 100,max_depth = 25))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "xgb_v3 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26230ba2-b81b-4886-a75b-96db1e5f310c",
   "metadata": {},
   "source": [
    "#### XG Boosting + Hybrid Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6788cdc8-7371-4b67-82cb-1965f8595dc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "    \n",
    "    model.init_hybrid_sampling(3) # Hybrid Sampling for class imabalance\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    model_name = \"xgb_v4\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = xgb.XGBClassifier(n_estimators = 100,max_depth = 25))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "xgb_v4 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c104c2-1384-47c9-bd65-4ba13e70f2db",
   "metadata": {},
   "source": [
    "#### XG Boosting + Grid Search 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f63e2a-30fa-4573-ac20-694fae074c54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    model_name = \"xgb_v5\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = xgb.XGBClassifier(n_estimators = 100,max_depth = 25))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Set K-Fold CV\n",
    "    model.set_kfold_cv(5)\n",
    "    \n",
    "    # Instantiate the grid search\n",
    "    model.init_grid_search(params={f\"{model_name}__max_depth\":[5,10,15,25,50],\n",
    "                                   f\"{model_name}__n_estimators\":[50,75,100],\n",
    "                                  },\n",
    "                           n_job = 3)\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "xgb_v5 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1770dd-52f0-4f0b-b213-2b4daa6dd1fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### XGB Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2205a58-7e91-4b1d-a953-afe701dfd132",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment - XG Boost</b></font>\n",
    "    \n",
    "- From the above results for XG Boost, we obtained:\n",
    "|Model|Best Params / Params|training F1|testing F1|\n",
    "|---|---|:---:|:---:|\n",
    "|`XGB`|`{'n_estimators': 50,'subsample': 0.5}`|`_%`|`_%`|\n",
    "|`XGB` (Oversampling)|`{'n_estimators': 50,'subsample': 0.5}`|`_%`|`_%`|\n",
    "|`XGB` (SMOTENC)|`{'n_estimators': 50,'subsample': 0.5}`|`_%`|`_%`|\n",
    "|`XGB` (Hybrid)|`{'n_estimators': 50,'subsample': 0.5}`|`_%`|`_%`|\n",
    "    \n",
    "- From the above results, we can see that most of the sampling methods for tackling class imbalance does not work with our data.\n",
    "- We use the model with top 2 result to train with 5-fold CV.\n",
    "|Model|Best Params / Params|training F1|testing F1|\n",
    "|---|---|:---:|:---:|\n",
    "|`XGB` (5-fold CV)|`{'n_estimators': 100,'subsample': 0.75'}`|`_%`|`_%`|\n",
    "|`XGB` (5-fold CV & Oversampling)|`{'n_estimators': 50,'subsample': 0.5}`|`_%`|`_%`|\n",
    "    \n",
    "- We can see that `Oversampling` does not help improving the model.\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6680734e-95cf-40f1-ae61-8e236916c80f",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-model\">\n",
    "    <h3> Model - Random Forest </h3>\n",
    "</a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9b56fb-8b08-4253-81a9-db922659a9e8",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c59d64-e27f-4205-be01-66633aedec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "    \n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "    \n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    model_name = \"rf_v1\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = RandomForestClassifier(n_estimators = 100,\n",
    "                                                    max_depth = 25,\n",
    "                                                    min_samples_split = 100,\n",
    "                                                    n_jobs = 3))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "rf_v1 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f748f6b0-d283-4e64-8d39-6c688ebf1b29",
   "metadata": {},
   "source": [
    "#### Random Forest + Over Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a1841a-9405-4ee8-bfcd-34293af1f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "    \n",
    "    model.init_over_sampling() # Oversampling for class imabalance\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    model_name = \"rf_v2\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = RandomForestClassifier(n_estimators = 100,\n",
    "                                                    max_depth = 25,\n",
    "                                                    min_samples_split = 100,\n",
    "                                                    n_jobs = 3))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "rf_v2 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a554a9-5ef2-4de6-98c4-1b73e0e5a907",
   "metadata": {},
   "source": [
    "#### Random Forest + SMOTENC Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8004f4c2-b995-4758-aa5e-427635a3a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "    \n",
    "    model.init_smotenc() # SMOTENC Oversampling for class imabalance\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    model_name = \"rf_v3\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = RandomForestClassifier(n_estimators = 100,\n",
    "                                                    max_depth = 25,\n",
    "                                                    min_samples_split = 100,\n",
    "                                                    n_jobs = 3))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "rf_v3 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a96f96-1049-44ff-be37-3d2d20ecc520",
   "metadata": {},
   "source": [
    "#### Random Forest + Hybrid Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d2322-86ad-412d-95b7-9d8ba882d79b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "    \n",
    "    model.init_hybrid_sampling(3) # Hybrid Sampling for class imabalance\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    model_name = \"rf_v4\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = RandomForestClassifier(n_estimators = 100,\n",
    "                                                    max_depth = 25,\n",
    "                                                    min_samples_split = 100,\n",
    "                                                    n_jobs = 3))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "rf_v4 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714ca141-aa80-437a-ba05-9d32c17980c2",
   "metadata": {},
   "source": [
    "#### Random Forest + Grid Search 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13032e9e-c2e9-47c0-bf1a-6cb8167b7ce2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "    \n",
    "#     model.init_over_sampling() # Oversampling for class imabalance\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    model_name = \"rf_v5\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = RandomForestClassifier(n_estimators = 100,\n",
    "                                                    max_depth = 25,\n",
    "                                                    min_samples_split = 100,\n",
    "                                                    n_jobs = 3))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Set K-Fold CV\n",
    "    model.set_kfold_cv(5)\n",
    "    \n",
    "    # Instantiate the grid search\n",
    "    model.init_grid_search(params={f\"{model_name}__n_estimators\":[50,75,100,150],\n",
    "                                  },\n",
    "                           n_job = 3)\n",
    "\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "rf_v5 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15022da0-2e70-4423-9179-cc75269eeb7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Random Forest Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ada072-5954-4a4d-9948-0eeecad81b7e",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-model\">\n",
    "    <h3> Model - Naive Bayes </h3>\n",
    "</a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaaa32e-4817-4ea1-8aed-41a777406264",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998b3b9d-447b-453c-9b36-ccb24016b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "    \n",
    "    # Instantiate scaler steps\n",
    "    model.init_standard_scaler(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_scaler() # Activate all scalers and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    model_name = \"nb_v1\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = GaussianNB())\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "nb_v1 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d3fb35-cb3c-486a-8bbf-51db283ce7a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Naive Bayes Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a30aaa-c27d-4489-b3a5-4d8debcad592",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-model\">\n",
    "    <h3> Model - KNN </h3>\n",
    "</a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2902ff51-f1fd-400e-8f40-355d0e7456d6",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cabfdd-2944-4953-a0aa-09b9238362af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "    \n",
    "    # Instantiate scaler steps\n",
    "    model.init_standard_scaler(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_scaler() # Activate all scalers and add to pipeline\n",
    "\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    model_name = \"knn_v1\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = KNeighborsClassifier())\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "knn_v1 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c02be-3644-441c-835d-f75f676eeb2e",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbor + Grid Search 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab83a112-47cf-472e-b8d5-00a2ca870360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=global_subsampling_pct,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "    \n",
    "    # Instantiate scaler steps\n",
    "    model.init_standard_scaler(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_scaler() # Activate all scalers and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    model_name = \"knn_v2\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = KNeighborsClassifier())\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Set K-Fold CV\n",
    "    model.set_kfold_cv(5)\n",
    "    \n",
    "    # Instantiate the grid search\n",
    "    model.init_grid_search(params={f\"{model_name}__n_neighbors\":[3,5,7,9,11,13,15],\n",
    "                                  },\n",
    "                           n_job = 3)\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "knn_v2 = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c7e11a-a359-4331-bd7d-1187926bc35c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### KNN Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd905f-2273-4ea3-aca3-e435b7ab8fe2",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-model\">\n",
    "    <h3> Model - Neural Network </h3>\n",
    "</a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b945fe7-3ac4-414e-99fd-c6268d8d4a46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b05a38-5232-48c0-82b4-12fcebcfde5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a5cccf6-db19-448c-9083-8336ee8ab94d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Neural Network Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3569ae-e5f8-4473-818f-36e39ed2c239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632d6548-0530-4691-a0d6-e21daf33cf79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36317221-5e2f-4a81-8e2c-29cc325ae0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ec629eb-764b-4ee2-99bb-e5ec7e11ef3c",
   "metadata": {},
   "source": [
    "[Back-to-top](#4-toc)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920080c6-8664-41f1-b810-046910cbf891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6d9a029-3a28-4501-b3b4-cc38d7771c18",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-pipe-full\">\n",
    "    <h2> Model Pipelines with full data </h2>\n",
    "</a>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f215f15-f724-4d00-9657-10c29713cc5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üß† Idea:</b></font>\n",
    "    \n",
    "- This section will re-train the best model from each type of model with full set of data.\n",
    "- Then compare the result between them.\n",
    "- All model will be trained with 5-fold cross validation.\n",
    "    \n",
    "\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üèÜ Previous Result with 10% of training data:</b></font>\n",
    "|Model|Best Params / Params|training F1|testing F1|\n",
    "|---|---|:---:|:---:|\n",
    "|`Logistic Regression`|`{'C': 0.1}`|`71.66%`|`71.75%`|\n",
    "|`Decision Tree` |`{'max_depth': 25, 'min_samples_split': 50}`|`82.16%`|`82.80%`|\n",
    "\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f8a621-14a9-4a1a-91a2-7b34f1118f48",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b4ab3-bcb0-4acb-aa74-ddd1f3d1da90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=None,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Drop non-important columns\n",
    "    model.init_drop_columns([\"fuel_M85\", \"fuel_Lpg\", \"fuel_Hydrogen\"])\n",
    "    \n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    model.init_standard_scaler(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_scaler() # Activate all scalers and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    model_name = \"full_logit\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = LogisticRegression(penalty = \"l2\", # Avoid Overfitting\n",
    "                                                max_iter=10000))\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Set K-Fold CV\n",
    "    model.set_kfold_cv(5)\n",
    "    \n",
    "    # Instantiate the grid search\n",
    "    model.init_grid_search(params={f\"{model_name}__C\":[10 ** i for i in [-3,-2,-1,0,1,2,3]],\n",
    "                                  },\n",
    "                           n_job = 4\n",
    "                          )\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "# full_logit = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1ac993-3fae-41ae-99ce-41b880ad99e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024895e7-3bb6-4780-ab0c-b0b70accd4fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=None,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model and add to pipeline\n",
    "    this_model_name = \"full_dtc\"\n",
    "    model.init_model(name = this_model_name,\n",
    "                     model = DecisionTreeClassifier())\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "    \n",
    "    # Set K-Fold CV\n",
    "    model.set_kfold_cv(5)\n",
    "    \n",
    "    # Instantiate the grid search\n",
    "    model.init_grid_search(params={f\"{this_model_name}__max_depth\":[5,10,25,50],\n",
    "                                   f\"{this_model_name}__min_samples_split\":[10,25,50,100]\n",
    "                                  },\n",
    "                           n_job = 4\n",
    "                          )\n",
    "\n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "# full_dtc = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaec6a63-2c09-40c1-b1a6-4a0090004707",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Adaboosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647ba1f8-d6a3-48c3-8a14-87fc09a3ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def model(test = False,save = True):\n",
    "    model = MyModel(X_train,X_test,\n",
    "                    y_train,y_test,\n",
    "                    train_subsample_size=None,\n",
    "                    test_subsample_size=None)\n",
    "\n",
    "    # Apply Subsampling for faster training in experimental stage\n",
    "    model.init_subsampling() # Subsample data for faster deployment\n",
    "\n",
    "    # Instantiate encoder steps\n",
    "    model.init_one_hot_encoding(columns=[\"vehicle_type\",\"drivetrain\",\"transmission\",\"engine_block\"])\n",
    "    model.init_target_encoding(columns=[\"make\",\"model\",\"trim\",\"body_type\"])\n",
    "    model.init_encoder() # Activate all encoders and add to pipeline\n",
    "\n",
    "    # Instantiate model\n",
    "    model_name = \"full_abc\"\n",
    "    model.init_model(name = model_name,\n",
    "                     model = AdaBoostClassifier(estimator = DecisionTreeClassifier(max_depth = 25,\n",
    "                                                                                   min_samples_split = 50),\n",
    "                                                )\n",
    "                    )\n",
    "    \n",
    "    # Instantiate the entire pipeline\n",
    "    model.init_pipeline()\n",
    "\n",
    "    # Set K-Fold CV\n",
    "    model.set_kfold_cv(5)\n",
    "    \n",
    "    # Instantiate the grid search\n",
    "    model.init_grid_search(params={f\"{model_name}__n_estimators\":[25,50,100], \n",
    "                                   f\"{model_name}__learning_rate\":[0.9,1],\n",
    "                                  },\n",
    "                           n_job = 3)\n",
    "    \n",
    "    # Fit the Pipeline\n",
    "    model.fit()\n",
    "    \n",
    "    # Make Prediction on X_test\n",
    "    model.predict()\n",
    "    \n",
    "    # Print the train score (without GridSearch) / best train score (with GridSearch)\n",
    "    model.print_best_train_score()\n",
    "    \n",
    "    # Print the test score\n",
    "    model.print_test_score()\n",
    "    \n",
    "    # Print the best params\n",
    "    model.print_best_params()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    model.print_confusion_matrix()\n",
    "\n",
    "    # Add model performance\n",
    "    if not test:\n",
    "        global perf\n",
    "        perf = model.add_perf(perf)\n",
    "        \n",
    "    if save:\n",
    "        model.save_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run this model\n",
    "# full_abc = model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9eb716-f57c-442f-9d8d-5ea6683f9613",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SGB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16f7499-b4e1-480a-9ea6-97e063aa0d36",
   "metadata": {
    "tags": []
   },
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6aa3c7-0ced-450a-a6e9-b58dadda97c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a8973d-f7cd-4b4c-94ab-36409c96fef4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613b1296-d7cb-4fe2-b52b-94c98dfaba85",
   "metadata": {
    "tags": []
   },
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3c4b29-2f5a-416b-8b71-4da500339c11",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8284bd5b-3c44-43b1-abdf-b17f31915b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f11ea57-9ed1-47d5-9e1b-b3395ac4cbc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border:#33a0ff solid; padding: 15px; background-color: #f0f1ff; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-evaluate\">\n",
    "    <h2> Model Evaluation </h2>\n",
    "</a>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995515b5-56ea-44df-8fe2-1813e75c8852",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-model\">\n",
    "    <h3> Model Comparison </h3>\n",
    "</a>\n",
    "  \n",
    "\n",
    "- By comparing the **best model** from different types of model, we obtained:\n",
    "|Model|Best Params|training F1|testing F1|\n",
    "|---|---|:---:|:---:|\n",
    "|`Logistic Regression` (with 5-fold Grid Search CV)|`{'C': 1000}`|`71.77%`|`71.70%`|\n",
    "|`Decision Tree` (with 5-fold Grid Search CV)|`{'max_depth': 25, 'min_samples_split': 50}`|`82.15%`|`82.80%`|\n",
    "|`Adaboost`|`{'max_depth': 25, 'min_samples_split': 50, 'learning_rate': 1.0, 'n_estimators': 50}`|`89.91%`|`85.36%`|\n",
    "|`Stochastic Gradient Boosting`|`{max_depth': 25, 'min_samples_split': 50, 'n_estimators': 50, 'subsample': 0.5}`|`90.98%`|`85.72%`|\n",
    "|`XG Boost`|`{'n_estimators': 100, 'max_depth': 25}`|`95.85%`|`85.29%`|\n",
    "\n",
    "- For `Adaboost`, `Stochastic Gradient Boosting` and `XG Boost`, all of them seems to have a overfitting result.\n",
    "- We may need more time to re-run cross validation for `Adaboost`, `Stochastic Gradient Boosting` and `XG Boost`.\n",
    "    \n",
    "- All of these boosting methods are tree-based model, we can conclude that tree-based model works better on our dataset.\n",
    "- But still, we may try other models later to verify this claim.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1f8848-b376-4d80-9a8f-e10bae859462",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<a class=\"anchor\" id=\"4-model\">\n",
    "    <h3> Model Training Result </h3>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8cfce0eb-78e9-47fc-919f-a8733821f53c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 14:14:15 - INFO >>> Creating new model performance file...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'test_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zh/ycxy1lsx5sx0g_2n4l8hfvph0000gn/T/ipykernel_24138/3582851051.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelPerf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model/model_performance.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mascending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/brainstation/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   6754\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6755\u001b[0m             \u001b[0;31m# len(by) == 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6757\u001b[0m             \u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6758\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6760\u001b[0m             \u001b[0;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6761\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/brainstation/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1774\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1776\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'test_score'"
     ]
    }
   ],
   "source": [
    "p = ModelPerf(\"model/model_performance.csv\")\n",
    "temp = p.get_data()\n",
    "temp.sort_values(by=[\"test_score\"],ascending = [False],inplace=True)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d4d0bc-c2c4-43f6-8faa-45ca9c028a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Getting the confusion matrix for the best model.\n",
    "print(f\"---------- Best Model: {temp.loc[temp.index[0],'model_name']} ----------\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(temp.loc[temp.index[0],\"confusion_matrix\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a658018-cd80-4d9b-81cb-18a064057e7f",
   "metadata": {},
   "source": [
    "<div style=\"border-radius:10px; border: #ffd500 solid; padding: 15px; background-color: #ffffcf; font-size:100%; text-align:left\">\n",
    "<font size=\"4px\" color=\"#ffa600\"><b>üí¨ Comment:</b></font>\n",
    "    \n",
    "- From the above results, the best model is `Stochastic Gradient Boosting`.\n",
    "- Even the there is class imbalance, the model still perform well on the task.\n",
    "\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48867037-dac2-42ad-b7c6-4a573214b7e3",
   "metadata": {},
   "source": [
    "[Back-to-top](#4-toc)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
